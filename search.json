[
  {
    "objectID": "project-work.html",
    "href": "project-work.html",
    "title": "Project work",
    "section": "",
    "text": "Here shows group project’s proposal.\n\nGroup 1:\nGroup 2:\nGroup 3:\nGroup 4:\nGroup 5:\nGroup 6:\nGroup 7:"
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "MATH/COSC 3570 - Spring 2024",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part; and\nB. produce, reproduce, and Share Adapted Material.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "weeks/week-6.html",
    "href": "weeks/week-6.html",
    "title": "Week 6",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-6.html#participate",
    "href": "weeks/week-6.html#participate",
    "title": "Week 6",
    "section": "Participate",
    "text": "Participate\n🖥️ Slides - ggplot2\n openintro::loans_full_schema"
  },
  {
    "objectID": "weeks/week-6.html#reading-and-resources",
    "href": "weeks/week-6.html#reading-and-resources",
    "title": "Week 6",
    "section": "Reading and Resources",
    "text": "Reading and Resources\n📖 More add-on 📦: ggplot2 extensions\n📖 The R Graph Gallery\n📖 R Graphics Cookbook\n📖 R CHARTS"
  },
  {
    "objectID": "weeks/week-6.html#exercise",
    "href": "weeks/week-6.html#exercise",
    "title": "Week 6",
    "section": "Exercise",
    "text": "Exercise\n📋 Lab-11 ggplot2\n penguins.csv\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-7.html",
    "href": "weeks/week-7.html",
    "title": "Week 7",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-7.html#participate",
    "href": "weeks/week-7.html#participate",
    "title": "Week 7",
    "section": "Participate",
    "text": "Participate\n🖥️ Slides -Visualizing Data\n🖥️ Slides - Interactive Visualization\n Data - loans\n Data - Murders"
  },
  {
    "objectID": "weeks/week-7.html#reading-and-resources",
    "href": "weeks/week-7.html#reading-and-resources",
    "title": "Week 7",
    "section": "Reading and Resources",
    "text": "Reading and Resources\n📖 More add-on 📦: ggplot2 extensions\n📖 The R Graph Gallery\n📖 R Graphics Cookbook\n📖 R CHARTS\n📖 R for Data Science - Data Transformation"
  },
  {
    "objectID": "weeks/week-7.html#exercise",
    "href": "weeks/week-7.html#exercise",
    "title": "Week 7",
    "section": "Exercise",
    "text": "Exercise\n📋 Lab-12 Faceting\n📋 Lab-13 Visualization\n📋 Lab-14 Interactive Visualization (Present)\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-5.html",
    "href": "weeks/week-5.html",
    "title": "Week 5",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-5.html#participate",
    "href": "weeks/week-5.html#participate",
    "title": "Week 5",
    "section": "Participate",
    "text": "Participate\n🖥️ Slides - Data Importing\n murders.csv\n df-na.csv\n cars.rds\n 2010_bigfive_regents.xls"
  },
  {
    "objectID": "weeks/week-5.html#reading-and-resources",
    "href": "weeks/week-5.html#reading-and-resources",
    "title": "Week 5",
    "section": "Reading and Resources",
    "text": "Reading and Resources\n📖 R for Data Science - Data Import\n📖 R for Data Science - Data Visualization"
  },
  {
    "objectID": "weeks/week-5.html#exercise",
    "href": "weeks/week-5.html#exercise",
    "title": "Week 5",
    "section": "Exercise",
    "text": "Exercise\n📋 Lab-10 Data Importing\n ssa_male_prob.csv\n ssa_female_prob.Rds\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-4.html",
    "href": "weeks/week-4.html",
    "title": "Week 4",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-4.html#participate",
    "href": "weeks/week-4.html#participate",
    "title": "Week 4",
    "section": "Participate",
    "text": "Participate\n🖥️ Slides - R/Python Data Frames for Data Science"
  },
  {
    "objectID": "weeks/week-4.html#reading-and-resources",
    "href": "weeks/week-4.html#reading-and-resources",
    "title": "Week 4",
    "section": "Reading and Resources",
    "text": "Reading and Resources\n📖 tibble\n📖 pipes\n📖 NumPy\n📖 pandas"
  },
  {
    "objectID": "weeks/week-4.html#perform",
    "href": "weeks/week-4.html#perform",
    "title": "Week 4",
    "section": "Perform",
    "text": "Perform\n📋 Lab-08 Tibbles and Pipes\n📋 Lab-09 NumPy and pandas\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-1.html",
    "href": "weeks/week-1.html",
    "title": "Week 1",
    "section": "",
    "text": "📖 Read the syllabus\n📖 Get your laptop and computing environment ready!"
  },
  {
    "objectID": "weeks/week-1.html#reading-and-resources",
    "href": "weeks/week-1.html#reading-and-resources",
    "title": "Week 1",
    "section": "Reading and Resources",
    "text": "Reading and Resources\n📖 R for Data Science - Introduction\n📖 R for Data Science - Whole game\n📖 RStudio IDE :: Cheatsheet\n📖 Posit Cloud Guide"
  },
  {
    "objectID": "weeks/week-1.html#participate",
    "href": "weeks/week-1.html#participate",
    "title": "Week 1",
    "section": "Participate",
    "text": "Participate\n🖥️ Slides - Welcome to MATH/COSC 3570\n🖥️ Slides - Overview of Data Science\n🖥️ Slides - Posit Cloud\n🖥️ Slides - Git/GitHub\n ggplot2::mpg\n ggplot2::diamond"
  },
  {
    "objectID": "weeks/week-1.html#perform",
    "href": "weeks/week-1.html#perform",
    "title": "Week 1",
    "section": "Perform",
    "text": "Perform\n📋 Lab-00 Posit Cloud\n📋 Lab-01 Running R Script\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-3.html",
    "href": "weeks/week-3.html",
    "title": "Week 3",
    "section": "",
    "text": "Important\n\n\n\n\nPresenting Lab 07-Plotting for extra points!"
  },
  {
    "objectID": "weeks/week-3.html#participate",
    "href": "weeks/week-3.html#participate",
    "title": "Week 3",
    "section": "Participate",
    "text": "Participate\n🖥️ Slides - R/Python Syntax"
  },
  {
    "objectID": "weeks/week-3.html#reading-and-resources",
    "href": "weeks/week-3.html#reading-and-resources",
    "title": "Week 3",
    "section": "Reading and Resources",
    "text": "Reading and Resources\n📖 The R Graph Gallery\n📖 matplotlib"
  },
  {
    "objectID": "weeks/week-3.html#exercise",
    "href": "weeks/week-3.html#exercise",
    "title": "Week 3",
    "section": "Exercise",
    "text": "Exercise\n📋 Lab-05 R Data Summary\n📋 Lab-06 Python Data Structure\n📋 Lab-07 Plotting (Present)\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-2.html",
    "href": "weeks/week-2.html",
    "title": "Week 2",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-2.html#reading-and-resources",
    "href": "weeks/week-2.html#reading-and-resources",
    "title": "Week 2",
    "section": "Reading and Resources",
    "text": "Reading and Resources\n\nGit/GitHub\n📖 Create a personal access token (PAT)\n📖 Two-factor authentication\n📖 Git hands-on session within RStudio\n📖 Happy Git and GitHub for the useR\n📖 Happier version control with Git and GitHub\n\n\nMarkdown\n📖 Markdown Tutorial\n📖 Mastering Markdown GitHub Guides\n📖 Markdown Guide\n\n\nQuarto\n📖 Quarto Website\n📖 Get Started with Quarto\n📖 R for Data Science - Quarto"
  },
  {
    "objectID": "weeks/week-2.html#participate",
    "href": "weeks/week-2.html#participate",
    "title": "Week 2",
    "section": "Participate",
    "text": "Participate\n🖥️ Slides - Git/GitHub\n🖥️ Slides - Quarto"
  },
  {
    "objectID": "weeks/week-2.html#perform",
    "href": "weeks/week-2.html#perform",
    "title": "Week 2",
    "section": "Perform",
    "text": "Perform\n📋 Lab-00 Git/GitHub\n📋 Lab-02 Quarto File\n📋 Lab-03 Markdown\n📋 Lab-04 Code Chunk\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-10.html",
    "href": "weeks/week-10.html",
    "title": "Week 10",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-10.html#participate",
    "href": "weeks/week-10.html#participate",
    "title": "Week 10",
    "section": "Participate",
    "text": "Participate\n🖥️ Slides - Tidying Data\n🖥️ Slides - Probability and Statistics"
  },
  {
    "objectID": "weeks/week-10.html#exercise",
    "href": "weeks/week-10.html#exercise",
    "title": "Week 10",
    "section": "Exercise",
    "text": "Exercise\n📋 Lab-17 tidyr\n trump.csv\n📋 Lab-18 Probability\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-11.html",
    "href": "weeks/week-11.html",
    "title": "Week 11",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-11.html#participate",
    "href": "weeks/week-11.html#participate",
    "title": "Week 11",
    "section": "Participate",
    "text": "Participate\n🖥️ Slides - Probability and Statistics"
  },
  {
    "objectID": "weeks/week-11.html#exercise",
    "href": "weeks/week-11.html#exercise",
    "title": "Week 11",
    "section": "Exercise",
    "text": "Exercise\n📋 Lab-19 Confidence Interval\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-13.html",
    "href": "weeks/week-13.html",
    "title": "Week 13",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-13.html#participate",
    "href": "weeks/week-13.html#participate",
    "title": "Week 13",
    "section": "Participate",
    "text": "Participate\n🖥️ Slides - Logistic Regression\n Data - body"
  },
  {
    "objectID": "weeks/week-13.html#reading",
    "href": "weeks/week-13.html#reading",
    "title": "Week 13",
    "section": "Reading",
    "text": "Reading\n📖"
  },
  {
    "objectID": "weeks/week-13.html#exercise",
    "href": "weeks/week-13.html#exercise",
    "title": "Week 13",
    "section": "Exercise",
    "text": "Exercise\n📋 Lab-21 Logistic Regression\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-12.html",
    "href": "weeks/week-12.html",
    "title": "Week 12",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-12.html#participate",
    "href": "weeks/week-12.html#participate",
    "title": "Week 12",
    "section": "Participate",
    "text": "Participate\n🖥️ Slides - Linear Regression\n ggplot2::mpg"
  },
  {
    "objectID": "weeks/week-12.html#reading",
    "href": "weeks/week-12.html#reading",
    "title": "Week 12",
    "section": "Reading",
    "text": "Reading\n📖"
  },
  {
    "objectID": "weeks/week-12.html#exercise",
    "href": "weeks/week-12.html#exercise",
    "title": "Week 12",
    "section": "Exercise",
    "text": "Exercise\n📋 Lab-20 Simple Linear Regression\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-9.html",
    "href": "weeks/week-9.html",
    "title": "Week 9",
    "section": "",
    "text": "Important\n\n\n\n\n\n\nHappy spring break!\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-16.html",
    "href": "weeks/week-16.html",
    "title": "Week 16",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-16.html#participate",
    "href": "weeks/week-16.html#participate",
    "title": "Week 16",
    "section": "Participate",
    "text": "Participate\n🖥️ Slides - K-Means Clustering\n Data - clus_data"
  },
  {
    "objectID": "weeks/week-16.html#reading",
    "href": "weeks/week-16.html#reading",
    "title": "Week 16",
    "section": "Reading",
    "text": "Reading\n📖"
  },
  {
    "objectID": "weeks/week-16.html#exercise",
    "href": "weeks/week-16.html#exercise",
    "title": "Week 16",
    "section": "Exercise",
    "text": "Exercise\n📋 Lab-24 K-Means Clustering\n palmerpenguins::penguins\n\nlibrary(palmerpenguins)\npeng <- penguins[complete.cases(penguins), ] |> \n    select(flipper_length_mm, bill_length_mm)\n\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-8.html",
    "href": "weeks/week-8.html",
    "title": "Week 8",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-8.html#participate",
    "href": "weeks/week-8.html#participate",
    "title": "Week 8",
    "section": "Participate",
    "text": "Participate\n🖥️ Slides - Data Wrangling (one data frame)\n🖥️ Slides - Data Wrangling - two data frames\n Data - Pop_x and elec_vote_y\n\nlibrary(tidyverse)\nlibrary(dslabs)\npop_x <- murders |> \n    slice(1:6) |>\n    select(state, population)\n\nelec_vote_y <- results_us_election_2016 |> \n    filter(state %in% c(\"Alabama\", \"Alaska\", \"Arizona\", \n                        \"California\", \"Connecticut\", \"Delaware\")) |> \n    select(state, electoral_votes) |> \n    rename(elec_vote = electoral_votes)\n\n Data - Customers"
  },
  {
    "objectID": "weeks/week-8.html#reading-and-resources",
    "href": "weeks/week-8.html#reading-and-resources",
    "title": "Week 8",
    "section": "Reading and Resources",
    "text": "Reading and Resources\n📖 R for Data Science - Joins\n📖 R for Data Science - Data tidying"
  },
  {
    "objectID": "weeks/week-8.html#exercise",
    "href": "weeks/week-8.html#exercise",
    "title": "Week 8",
    "section": "Exercise",
    "text": "Exercise\n📋 Lab-15 dplyr\n Data - Murders\n📋 Lab-16 Joining tables\n https://www.jaredlander.com/data/DiamondColors.csv\n ggplot2::diamonds\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-15.html",
    "href": "weeks/week-15.html",
    "title": "Week 15",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-15.html#participate",
    "href": "weeks/week-15.html#participate",
    "title": "Week 15",
    "section": "Participate",
    "text": "Participate\n🖥️ Slides - Principal Component Analysis\n USArrests"
  },
  {
    "objectID": "weeks/week-15.html#reading",
    "href": "weeks/week-15.html#reading",
    "title": "Week 15",
    "section": "Reading",
    "text": "Reading\n📖"
  },
  {
    "objectID": "weeks/week-15.html#exercise",
    "href": "weeks/week-15.html#exercise",
    "title": "Week 15",
    "section": "Exercise",
    "text": "Exercise\n📋 Lab-23 Principal Component Analysis\n iris\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-14.html",
    "href": "weeks/week-14.html",
    "title": "Week 14",
    "section": "",
    "text": "Important"
  },
  {
    "objectID": "weeks/week-14.html#participate",
    "href": "weeks/week-14.html#participate",
    "title": "Week 14",
    "section": "Participate",
    "text": "Participate\n🖥️ Slides - K Nearest Neighbors\n Data - body"
  },
  {
    "objectID": "weeks/week-14.html#reading",
    "href": "weeks/week-14.html#reading",
    "title": "Week 14",
    "section": "Reading",
    "text": "Reading\n📖"
  },
  {
    "objectID": "weeks/week-14.html#exercise",
    "href": "weeks/week-14.html#exercise",
    "title": "Week 14",
    "section": "Exercise",
    "text": "Exercise\n📋 Lab-22 K Nearest Neighbors\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "hw/hw1.html",
    "href": "hw/hw1.html",
    "title": "Homework 1: Quarto, Basic Syntax and Data Importing",
    "section": "",
    "text": "Please introduce yourself. You can share anything, your hometown, major, family, hobbies, working experience, honors and awards, special skills, etc, yes anything! Your autobiography should include:\n\nAt least two paragraphs (Paragraphs are separated by a blank line)\nBold text\nItalic text\nText with both bold AND italic font (Not mentioned in class, but you should be able to figure it out)\nClickable text with a hyperlink\nBlockquote\nListed items\nemoji (Add emoji to your writing by typing :EMOJICODE:, check emoji cheatsheet)\n\nYour Self-Introduction:"
  },
  {
    "objectID": "hw/hw1.html#chunk-options",
    "href": "hw/hw1.html#chunk-options",
    "title": "Homework 1: Quarto, Basic Syntax and Data Importing",
    "section": "2 Chunk Options",
    "text": "2 Chunk Options\nPlease check the references https://quarto.org/docs/reference/cells/cells-knitr.html or https://yihui.org/knitr/options/ and answer the following questions.\n\nPlease add your nice picture using knitr::include_graphics(). Please use\n\necho to not to show the code\nfig-cap to add a figure caption\nfig-cap-location to put the caption on the margin.\n\n\n\n\n\n\nUse the chunk option\n\necho to NOT to show library(tidyverse), library(ggplot2), and library(ggrepel). Note: you may need to use !expr. Check https://stackoverflow.com/questions/72217651/quarto-rmarkdown-code-block-to-only-display-certain-lines and https://quarto.org/docs/computations/r.html#chunk-options\nfig-align to have the figure right-aligned.\n\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nclass_avg <- mpg |> \n    group_by(class) |> \n    summarise(displ = median(displ), hwy = median(hwy))\nlibrary(ggrepel)\nggplot(mpg, aes(displ, hwy, colour = class)) + \n    geom_label_repel(aes(label = class), data = class_avg, size = 6, \n                     label.size = 0, segment.color = NA) + \n    geom_point() + theme(legend.position = \"none\")\n\n\n\n\nA Marquette student has a really bad code style. Please\n\nAdd the chunk option tidy in the chunk labelled style to make her code below more readable.\nAdd another option eval so that the code is NOT run.\n\n\n\nfor(k in 1:10){j=cos(sin(k)*k^2)+3;l=exp(k-7*log(k,base=2));print(j*l-5)}\n\n[1] 4.966218\n[1] -4.985713\n[1] -4.998994\n[1] -4.999823\n[1] -4.999956\n[1] -4.999988\n[1] -4.999988\n[1] -4.999991\n[1] -4.999995\n[1] -4.999996\n\n\n\nUse the chunk option results in the chunk labelled cat, so that the text output is “I love Marquette and Data Science!”.\n\n\ncat(\"I love **Marquette** and *Data Science*!\\n\")\n\nI love **Marquette** and *Data Science*!\n\n\n\nWe can re-use a code chunk by using its name! Please create a code chunk and use the option #| label: photo to run the code in the code chunk named photo. Note that the chunk options are not carried."
  },
  {
    "objectID": "hw/hw1.html#basic-r",
    "href": "hw/hw1.html#basic-r",
    "title": "Homework 1: Quarto, Basic Syntax and Data Importing",
    "section": "3 Basic R",
    "text": "3 Basic R\n\n3.1 Vector\nUse the built-in data set LakeHuron.\n\nFind the number of data values in LakeHuron.\n\n\n\n\n\nWhat is the highest and lowest level (in feet) of LakeHuron during the 1875-1972 period?\n\n\n\n\n\nReturn a logical vector that shows whether the lake level is higher than the average level or not.\n\n\n\n\n\nReturn years that have higher level than the average.\n\n\n\n\n\n\n3.2 Data Frame\n\nMake the mtcars dataset as a tibble using as_tibble(). Call it tbl.\n\n\n\n\n\nPrint the sub data of tbl that contains the 11th to 15th rows and the last three columns.\n\n\n\n\n\nGrab the second and the third columns of tbl using list method and their name. (Do NOT use matrix method, and their index)\n\n\n\n\n\nExtract the fourth column of tbl as a numerical vector.\n\n\n\n\n\nStart with tbl, use the pipe operator |> to do the followings sequentially.\n\nextract the first 10 observations (rows)\nfind the column names\nsort the columns names using sort() in a decreasing order. (alphabetically from z to a)\n\n\n\n\n\n\n\n3.3 Data Importing\n\nRead the data sales.xlsx from the data folder. Use arguments sheet, skip and col_names so that the output looks like\n# A tibble: 9 x 2\n  id      n    \n  <chr>   <chr>\n1 Brand 1 n    \n2 1234    8    \n3 8721    2    \n4 1822    3    \n5 Brand 2 n    \n6 3333    1    \n# … with 3 more rows\n\n\n\n\n\nRead in the favourite-food.xlsx file from the data folder and call the data fav_food. Use the argument na to treat “N/A” and “99999” as a missing value. Print the data out."
  },
  {
    "objectID": "hw/hw1.html#basic-python",
    "href": "hw/hw1.html#basic-python",
    "title": "Homework 1: Quarto, Basic Syntax and Data Importing",
    "section": "4 Basic Python",
    "text": "4 Basic Python\n\nimport numpy as np\nimport pandas as pd\n\n\n4.1 Data Frame\n\nImport the data set mtcars.csv using pd.read_csv(). Then print the first five rows.\n\n\n\n\n\nUse method .iloc to obtain the first and fourth rows, and the second and third columns. Name the data dfcar.\n\n\n\n\n\nSet the row names of dfcar to Mazda and Hornet.\n\n\n\n\n\nUse method .loc to obtain row Hornet and column disp."
  },
  {
    "objectID": "hw/hw2.html",
    "href": "hw/hw2.html",
    "title": "Homework 2: Data Visualization and Data Wrangling",
    "section": "",
    "text": "Import the data set murders. Use the pipe operator |> and the dplyr functions mutate(), filter(), and select() to get the following data output. Call the data set df.\n\nThe filtering conditions are\n\nregion in “Northeast” or “West”\nrate = total / population * 100000 is less than 1.\n\nThe new variable rank is based on rate. The highest rate is ranked 1st. [Hint:] Use the function rank().\n\n## code\n\n# # A tibble: 8 × 4\n#    rate  rank state         total\n#   <dbl> <dbl> <chr>         <dbl>\n# 1 0.515    49 Hawaii            7\n# 2 0.766    46 Idaho            12\n# 3 0.828    44 Maine            11\n# 4 0.380    50 New Hampshire     5\n# 5 0.940    42 Oregon           36\n# 6 0.796    45 Utah             22\n# 7 0.320    51 Vermont           2\n# 8 0.887    43 Wyoming           5\n\n\nChange the type of column rank to factor, and total to integer. (You can use the built-in as.factor() and as.integer() or the convert() function in the hablar package.\n\n\n## code\n\n\nCreate a list named df_lst of two elements. The first element is a subset of df whose rows have total less than 10, and the second element is a subset of df so that each of its row has total higher than 10. Print it out.\n\n\n## code\n\n\n# [[1]]\n# # A tibble: 4 × 4\n#    rate rank  state         total\n#   <dbl> <fct> <chr>         <dbl>\n# 1 0.515 49    Hawaii            7\n# 2 0.380 50    New Hampshire     5\n# 3 0.320 51    Vermont           2\n# 4 0.887 43    Wyoming           5\n# \n# [[2]]\n# # A tibble: 4 × 4\n#    rate rank  state  total\n#   <dbl> <fct> <chr>  <dbl>\n# 1 0.766 46    Idaho     12\n# 2 0.828 44    Maine     11\n# 3 0.940 42    Oregon    36\n# 4 0.796 45    Utah      22\n\n\nCombine the two data sets in df_lst using rbind().\n\n\n## code\n\n\nThe dplyr provides dplyr::bind_rows() and dplyr::bind_cols() that are analogs to rbind() and cbind() in the R base. Combine the two data sets in df_lst using bind_rows(). The result should be exactly the same as the previous one.\n\n\n## code\n\n\nCombine the two data frames data1 and data2 below using bind_rows() and rbind(). Describe what happened and their difference. (Note that the two data sets have different column names)\n\n\ndata1 <- tibble(x = letters[1:5])\ndata2 <- tibble(y = 1:3)\n\n\n## code (Error may happen. set eval: false if you'd like to render the document.)\n\n\nWith df, select state and total, and arrange df by total in an increasing order.\n\n\n## code\n\n\nWith df, use contains() to select column variables whose name contains the string “at”.\n\n\n## code\n\n\nBack to murders. Extract the rows whose has the largest population in its region as the shown output. The population is ranked in a decreasing order.\n\n\n## code\n\n# # A tibble: 4 × 5\n#   state      abb   region        population total\n#   <chr>      <chr> <chr>              <dbl> <dbl>\n# 1 California CA    West            37253956  1257\n# 2 Texas      TX    South           25145561   805\n# 3 New York   NY    Northeast       19378102   517\n# 4 Illinois   IL    North Central   12830632   364\n\n\n\n\n\nInstall and load the Lahman library. This database includes data related to baseball teams. It includes summary statistics about how the players performed on offense and defense for several years. It also includes personal information about the players. The Batting data frame contains the offensive statistics for all players for many years:\n\n\n\nRows: 112,184\nColumns: 22\n$ playerID <chr> \"abercda01\", \"addybo01\", \"allisar01\", \"allisdo01\", \"ansonca01…\n$ yearID   <int> 1871, 1871, 1871, 1871, 1871, 1871, 1871, 1871, 1871, 1871, 1…\n$ stint    <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ teamID   <fct> TRO, RC1, CL1, WS3, RC1, FW1, RC1, BS1, FW1, BS1, CL1, CL1, W…\n$ lgID     <fct> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ G        <int> 1, 25, 29, 27, 25, 12, 1, 31, 1, 18, 22, 1, 10, 3, 20, 29, 1,…\n$ AB       <int> 4, 118, 137, 133, 120, 49, 4, 157, 5, 86, 89, 3, 36, 15, 94, …\n$ R        <int> 0, 30, 28, 28, 29, 9, 0, 66, 1, 13, 18, 0, 6, 7, 24, 26, 0, 0…\n$ H        <int> 0, 32, 40, 44, 39, 11, 1, 63, 1, 13, 27, 0, 7, 6, 33, 32, 0, …\n$ X2B      <int> 0, 6, 4, 10, 11, 2, 0, 10, 1, 2, 1, 0, 0, 0, 9, 3, 0, 0, 1, 0…\n$ X3B      <int> 0, 0, 5, 2, 3, 1, 0, 9, 0, 1, 10, 0, 0, 0, 1, 3, 0, 0, 1, 0, …\n$ HR       <int> 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0…\n$ RBI      <int> 0, 13, 19, 27, 16, 5, 2, 34, 1, 11, 18, 0, 1, 5, 21, 23, 0, 0…\n$ SB       <int> 0, 8, 3, 1, 6, 0, 0, 11, 0, 1, 0, 0, 2, 2, 4, 4, 0, 0, 3, 0, …\n$ CS       <int> 0, 1, 1, 1, 2, 1, 0, 6, 0, 0, 1, 0, 0, 0, 0, 4, 0, 0, 1, 0, 0…\n$ BB       <int> 0, 4, 2, 0, 2, 0, 1, 13, 0, 0, 3, 1, 2, 0, 2, 9, 0, 0, 4, 1, …\n$ SO       <int> 0, 0, 5, 2, 1, 1, 0, 1, 0, 0, 4, 0, 0, 0, 2, 2, 3, 0, 2, 0, 2…\n$ IBB      <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ HBP      <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ SH       <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ SF       <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ GIDP     <int> 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 1, 2, 0, 0, 0, 0, 3…\n\n\nUse Batting data to obtain the top 10 player observations that hit the most home runs (in descending order) in 2022. Call the data set top10, make it as a tibble and print it out.\n\n## code\n\n\nBut who are these players? In the top10 data, we see an ID, but not the names. The player names are in the People data set:\n\n\n\nRows: 20,676\nColumns: 26\n$ playerID     <chr> \"aardsda01\", \"aaronha01\", \"aaronto01\", \"aasedo01\", \"abada…\n$ birthYear    <int> 1981, 1934, 1939, 1954, 1972, 1985, 1850, 1877, 1869, 186…\n$ birthMonth   <int> 12, 2, 8, 9, 8, 12, 11, 4, 11, 10, 9, 3, 10, 2, 8, 9, 6, …\n$ birthDay     <int> 27, 5, 5, 8, 25, 17, 4, 15, 11, 14, 20, 16, 22, 16, 17, 1…\n$ birthCountry <chr> \"USA\", \"USA\", \"USA\", \"USA\", \"USA\", \"D.R.\", \"USA\", \"USA\", …\n$ birthState   <chr> \"CO\", \"AL\", \"AL\", \"CA\", \"FL\", \"La Romana\", \"PA\", \"PA\", \"V…\n$ birthCity    <chr> \"Denver\", \"Mobile\", \"Mobile\", \"Orange\", \"Palm Beach\", \"La…\n$ deathYear    <int> NA, 2021, 1984, NA, NA, NA, 1905, 1957, 1962, 1926, NA, 1…\n$ deathMonth   <int> NA, 1, 8, NA, NA, NA, 5, 1, 6, 4, NA, 2, 6, NA, NA, NA, N…\n$ deathDay     <int> NA, 22, 16, NA, NA, NA, 17, 6, 11, 27, NA, 13, 11, NA, NA…\n$ deathCountry <chr> NA, \"USA\", \"USA\", NA, NA, NA, \"USA\", \"USA\", \"USA\", \"USA\",…\n$ deathState   <chr> NA, \"GA\", \"GA\", NA, NA, NA, \"NJ\", \"FL\", \"VT\", \"CA\", NA, \"…\n$ deathCity    <chr> NA, \"Atlanta\", \"Atlanta\", NA, NA, NA, \"Pemberton\", \"Fort …\n$ nameFirst    <chr> \"David\", \"Hank\", \"Tommie\", \"Don\", \"Andy\", \"Fernando\", \"Jo…\n$ nameLast     <chr> \"Aardsma\", \"Aaron\", \"Aaron\", \"Aase\", \"Abad\", \"Abad\", \"Aba…\n$ nameGiven    <chr> \"David Allan\", \"Henry Louis\", \"Tommie Lee\", \"Donald Willi…\n$ weight       <int> 215, 180, 190, 190, 184, 235, 192, 170, 175, 169, 220, 19…\n$ height       <int> 75, 72, 75, 75, 73, 74, 72, 71, 71, 68, 74, 71, 70, 78, 7…\n$ bats         <fct> R, R, R, R, L, L, R, R, R, L, R, R, R, R, R, L, R, L, L, …\n$ throws       <fct> R, R, R, R, L, L, R, R, R, L, R, R, R, R, L, L, R, L, R, …\n$ debut        <chr> \"2004-04-06\", \"1954-04-13\", \"1962-04-10\", \"1977-07-26\", \"…\n$ finalGame    <chr> \"2015-08-23\", \"1976-10-03\", \"1971-09-26\", \"1990-10-03\", \"…\n$ retroID      <chr> \"aardd001\", \"aaroh101\", \"aarot101\", \"aased001\", \"abada001…\n$ bbrefID      <chr> \"aardsda01\", \"aaronha01\", \"aaronto01\", \"aasedo01\", \"abada…\n$ deathDate    <date> NA, 2021-01-22, 1984-08-16, NA, NA, NA, 1905-05-17, 1957…\n$ birthDate    <date> 1981-12-27, 1934-02-05, 1939-08-05, 1954-09-08, 1972-08-…\n\n\nWe can see column names nameFirst and nameLast. Use the left_join() function to create a table of the top home run hitters. The data table should have variables playerID, nameFirst, nameLast, and HR. Overwrite the object top10 with this new table, and print it out.\n\n## code\n\n\nUse the Fielding data frame to add each player’s position to the table you created in (2). Make sure that you filter for the year 2022 first, then use right_join(). This time show nameFirst, nameLast, teamID, HR, and POS.\n\n\n## code\n\n\n\n\n\nThe R built-in co2 data set is not tidy. Let’s make it tidy. Run the following code to define the co2_wide object:\n\n\nco2_wide <- data.frame(matrix(co2, ncol = 12, byrow = TRUE)) |> \n    setNames(1:12) |> \n    mutate(year = as.character(1959:1997))\n\nUse the pivot_longer() function to make it tidy. Call the column with the CO2 measurements co2 and call the month column month. Call the resulting object co2_tidy. Print it out.\n\n## code\n\n\n## # A tibble: 468 x 3\n##    year  month   co2\n##    <chr> <chr> <dbl>\n##  1 1959  1      315.\n##  2 1959  2      316.\n##  3 1959  3      316.\n##  4 1959  4      318.\n##  5 1959  5      318.\n##  6 1959  6      318 \n##  7 1959  7      316.\n##  8 1959  8      315.\n##  9 1959  9      314.\n## 10 1959  10     313.\n## # … with 458 more rows\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\n\n\nUse Python to do Section 1.1 question 1.\n\n\n\n## code\n\n\nUse Python to do Section 1.1 question 7.\n\n\n\n## code\n\n\nUse Python to do Section 1.1 question 9. [Hint:] The method pandas.DataFrame.drop_duplicates() is analogous to dplyr::distinct(). Please figure out what we should use in the argument subset and keep.\n\n\n## code\n\n\nUse Python to do Section 1.2 question 1. (Import the data Batting.csv).\n\n\n## code\n\n\nUse Python to do Section 1.2 question 2. (Import the data People.csv).\n\n\n## code"
  },
  {
    "objectID": "hw/hw2.html#murders",
    "href": "hw/hw2.html#murders",
    "title": "Homework 2: Data Visualization and Data Wrangling",
    "section": "2.1 murders",
    "text": "2.1 murders\nUse murders to make plots.\n\nCreate a scatter plot of total murders (x-axis) versus population sizes (y-axis) using the pipe operator |> that the murders data set is on the left to |>.\n\n\n## code\n\n\nGenerate the plot below using label and color aesthetics in aes() and a geometry layer geom_label(). Save the ggplot object as p. Here, we add abbreviation as the label, and make the labels’ color be determined by the state’s region.\n\n\n## code\n\n\n\n\n\n\n\nUse the object p in (2) and\n\n\nChange both axes to be in the \\(\\log_{10}\\) scale using scale_x_log10() and scale_y_log10()\nAdd a title “Gun murder data”\nUse the wall street journal theme in ggthemes.\n\n\n## code"
  },
  {
    "objectID": "hw/hw2.html#mpg",
    "href": "hw/hw2.html#mpg",
    "title": "Homework 2: Data Visualization and Data Wrangling",
    "section": "2.2 mpg",
    "text": "2.2 mpg\nUse mpg to make plots.\n\nWhat’s gone wrong with this code? Why are the points not blue? Change it so that the points are colored in blue.\n\n\nmpg |> ggplot(mapping = aes(x = displ, y = hwy, colour = \"blue\")) +\n    geom_point()\n\n\n\n\n\nGenerate the bar chart below.\n\n\n## code\n\n\n\n\n\n\n\nComplete the code to generate the boxplot below. Note that x = class and y = hwy, so the coordinates need to be flipped.\n\n\n## code\n\n\n\n\n\n\n\nGenerate the histogram below with density scale. Map y to the internal variable ..density.. (after_stat(density)) to show density values. Put the legend inside the plot at c(0.9, 0.15). (check ?theme help page)\n\n\n## code\n\n\n\n\n\n\n\nGenerate the scatter plot below.\n\n\n## code"
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "MATH/COSC 3570: Introduction to Data Science",
    "section": "",
    "text": "This course introduces main aspects of doing a practical data science project, from importing data to deploying what is learned from data. We start with learning popular data science tools such as basic R and Python programming, Git and GitHub, and interactive documenting systems RMarkdown and Quarto. Then we learn data importing, data visualization and data wrangling using both R and Python. The second half of the course focuses on several basic simulation and machine learning methods, including Monte Carlo simulation, linear regression, K-nearest neighbors, logistic regression, principal component analysis, and K-means clustering. We learn R tidyverse and tidymodels packages. For Python, Pandas and Scikit-Learn libraries are introduced."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH/COSC 3570 - Introduction to Data Science",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nTo Do\nSlides\nLab Exercise\nHomework\nLab Presentation\nProject\n\n\n\n\n1\nTue, Jan 16\nGreetings + Overview of Data Science\n📖\n🖥️ 🖥️\n\n\n\n\n\n\n\nThu, Jan 18\nPosit Cloud and Git/GitHub Introduction\n\n🖥️\n📋📋\n\n\n\n\n\n2\nTue, Jan 23\nConnecting Posit Cloud with GitHub\n📖\n🖥️\n📋\n\n\n\n\n\n\nThu, Jan 25\nQuarto\n\n🖥️\n📋📋📋\n\n\n\n\n\n3\nTue, Jan 30\nQuarto\n\n\n\n\n\n\n\n\n\nThu, Feb 1\nR/Python Programming\n📖\n🖥️\n\n\n\n\n\n\n4\nTue, Feb 6\nPython Programming\n\n\n📋📋\n\n\n\n\n\n\nThu, Feb 8\nR Tidyverse\n📖\n🖥️\n\n✍️\n\n\n\n\n5\nTue, Feb 13\nPython Pandas/NumPy\n\n\n📋📋\n\n✅\n\n\n\n\nThu, Feb 15\nData Importing\n📖\n🖥️\n📋\n\n\n\n\n\n6\nTue, Feb 20\nData Importing\n\n\n\n\n\n\n\n\n\nThu, Feb 22\nData Visualization-ggplot2\n📖\n🖥️\n📋\n\n\n\n\n\n7\nTue, Feb 27\nData Visualization-categorical and numerical data\n📖\n🖥️\n📋📋\n\n\n\n\n\n\nThu, Feb 29\nInteractive Data Visualization\n\n🖥️\n\n\n\n\n\n\n8\nTue, Mar 5\nData Wrangling - one data frame\n📖\n🖥️\n📋\n\n✅\n\n\n\n\nThu, Mar 7\nData Wrangling - two data frames\n\n🖥️\n📋\n\n\n\n\n\n9\nTue, Mar 12\nNO CLASS: Spring break\n\n\n\n\n\n\n\n\n\nThu, Mar 14\nNO CLASS: Spring break\n\n\n\n\n\n\n\n\n10\nTue, Mar 19\nData Wrangling - tidyr\n📖\n🖥️\n📋\n✍️\n\n\n\n\n\nThu, Mar 21\nProbabilistic and Statistical Simulation\n\n🖥️\n📋\n\n\n\n\n\n11\nTue, Mar 26\nProbabilistic and Statistical Simulation\n\n\n📋\n\n\n\n\n\n\nThu, Mar 28\nNO CLASS: Easter break\n\n\n\n\n\n\n\n\n12\nTue, Apr 2\nLinear Regression\n📖\n🖥️\n\n\n\nGuideline 📂\n\n\n\nThu, Apr 4\nLinear Regression\n\n\n📋\n\n\n\n\n\n13\nTue, Apr 9\nLogistic Regression\n📖\n🖥️\n\n\n\n\n\n\n\nThu, Apr 11\nLogistic Regression\n\n\n📋\n✍️\n\n\n\n\n14\nTue, Apr 16\nK-Nearest Neighbors\n📖\n🖥️\n\n\n\n\n\n\n\nThu, Apr 18\nK-Nearest Neighbors\n\n\n📋\n\n\n\n\n\n15\nTue, Apr 23\nPrincipal Component Analysis\n📖\n🖥️\n\n\n\n\n\n\n\nThu, Apr 25\nPrincipal Component Analysis\n\n\n📋\n\n\nProposal 📂\n\n\n16\nTue, Apr 30\nK-Means Clustering\n📖\n🖥️\n\n\n\n\n\n\n\nThu, May 2\nQuarto Dashboard and Website\n\n\n\n\n\n\n\n\n\n\nI reserve the right to make changes to the schedule."
  },
  {
    "objectID": "course-news.html",
    "href": "course-news.html",
    "title": "News/Announcements",
    "section": "",
    "text": "The background survey form is at https://forms.office.com/r/PNwNY0F5Zk."
  },
  {
    "objectID": "course-news.html#jan-16-2024",
    "href": "course-news.html#jan-16-2024",
    "title": "News/Announcements",
    "section": "Jan 16, 2024",
    "text": "Jan 16, 2024\n\nPlease bring your laptop this Thursday. We’ll set up our computing environment and get it ready for doing data science."
  },
  {
    "objectID": "course-news.html#jan-17-2024",
    "href": "course-news.html#jan-17-2024",
    "title": "News/Announcements",
    "section": "Jan 17, 2024",
    "text": "Jan 17, 2024\nhttps://workforcenow.adp.com/mascsr/default/mdf/recruitment/recruitment.html?cid=03db93ed-1b1c-4447-805d-a674958c23e6&ccId=19000101_000001&jobId=480911&lang=en_US\nDeadline\nApplicants should submit their application by February 1, 2024."
  },
  {
    "objectID": "slides/13-dplyr-2.html#joining-data-frames",
    "href": "slides/13-dplyr-2.html#joining-data-frames",
    "title": "Data Wrangling - two data frames 🛠",
    "section": "Joining data frames",
    "text": "Joining data frames\n\nHave multiple data frames\nWant to bring them together\n\nSQL-like functions\n\nleft_join(x, y)\nright_join(x, y)\nfull_join(x, y)\ninner_join(x, y)\nsemi_join(x, y)\nanti_join(x, y)\n\n\n\n\nOK, back to dplyr. Here the idea is that we have two or more data frames, and we want to bring them together as one single combined data set.\nHow? we are gonna use dplyr functions with name something_join(x, y).\nThese functions borrow the idea of SQL for relational database management. They are similar to the join functions of SQL.\nAnd so we can do something that SQL usually does in R, and it’s probably easier because we don’t need to use R and SQL back and forth, and integrate data manipulation and analysis together.\nIn particular, we are gonna go through the following 6 join functions.\nHere x and y are data frames. OK."
  },
  {
    "objectID": "slides/13-dplyr-2.html#setup",
    "href": "slides/13-dplyr-2.html#setup",
    "title": "Data Wrangling - two data frames 🛠",
    "section": "Setup",
    "text": "Setup\nData sets x and y share the same variable id.\n\n\n\nx <- tibble(\n    id = c(1, 2, 3),\n    var_x = c(\"x1\", \"x2\", \"x3\")\n    )\n\n\nx\n\n# A tibble: 3 × 2\n     id var_x\n  <dbl> <chr>\n1     1 x1   \n2     2 x2   \n3     3 x3   \n\n\n\n\ny <- tibble(\n    id = c(1, 2, 4),\n    var_y = c(\"y1\", \"y2\", \"y4\")\n    )\n\n\ny\n\n# A tibble: 3 × 2\n     id var_y\n  <dbl> <chr>\n1     1 y1   \n2     2 y2   \n3     4 y4   \n\n\n\n\n\nI am going to use these two data frames as an illustration of join functions.\ndata frame x has variable id and var_x\ndata frame y has variable again id and but another variable var_y.\nData frame x and y have the common variable id, so it’s quite reasonable to merge the two data sets together by the common variable id. But x has id 1, 2, 3 and y has id 1, 2, 4. And so there are many different ways to combine the two.\nYou can think id is marquette ID, and var_x stores GPA and var_y stores say students’ height. And we are gonna combine the two data sets into one student personal information data set."
  },
  {
    "objectID": "slides/13-dplyr-2.html#left_joinx-y-all-rows-from-x",
    "href": "slides/13-dplyr-2.html#left_joinx-y-all-rows-from-x",
    "title": "Data Wrangling - two data frames 🛠",
    "section": "\nleft_join(x, y): all rows from x",
    "text": "left_join(x, y): all rows from x\n\n\n\n\n\n\n\n\n\n\n\n\n## by = keys\nleft_join(x, y, by = \"id\")\n\n# A tibble: 3 × 3\n     id var_x var_y\n  <dbl> <chr> <chr>\n1     1 x1    y1   \n2     2 x2    y2   \n3     3 x3    <NA> \n\n\n\n\nNA is added to the id not appearing in y.\n\n\n\n\n\n\n\n\n\n\n\n\nOK first left_join. Look at this gif.\nThe idea is that left_join(x, y) keeps all the rows or observations from x, and keep all the variables in x and y, including id, var_x and var_y.\nThe variables used to connect two data tables are called keys, and we use by argument to tell dplyr which variable is the key\nBy default, the function uses all variables that appear in both tables as keys. So here, the default key is also “id” because “id” is the only variable that appears in both data sets.\nThe resulting data frame is shown here. The left join function basically keeps the entire data set x, and attaches the data set y to x with “id” in x.\nBecause y doesn’t have id 3, its value of var_y is a missing value NA.\nWe can use the venn diagram to visualize the idea of joining tables. And basically, the main data set is A or x. We keep everything of A, and we add stuff of B or y for observations that are in A or x only."
  },
  {
    "objectID": "slides/13-dplyr-2.html#left_join-example",
    "href": "slides/13-dplyr-2.html#left_join-example",
    "title": "Data Wrangling - two data frames 🛠",
    "section": "\nleft_join() Example",
    "text": "left_join() Example\n\n\nLeft join\nCode for generating the data sets\n\n\n\n\n\n\npop_x\n\n       state population\n1    Alabama    4779736\n2     Alaska     710231\n3    Arizona    6392017\n4   Arkansas    2915918\n5 California   37253956\n6   Colorado    5029196\n\n\n\nelec_vote_y\n\n        state elec_vote\n1  California        55\n2     Arizona        11\n3     Alabama         9\n4 Connecticut         7\n5      Alaska         3\n6    Delaware         3\n\n\n\n\npop_x |> \n    left_join(elec_vote_y) #<<\n\n       state population elec_vote\n1    Alabama    4779736         9\n2     Alaska     710231         3\n3    Arizona    6392017        11\n4   Arkansas    2915918        NA\n5 California   37253956        55\n6   Colorado    5029196        NA\n\n\n\n\nConnecticut and Delaware in elec_vote_y will not be shown in the left-joined data because they are not in pop_x.\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(dslabs)\npop_x <- murders |> \n    slice(1:6) |>\n    select(state, population)\n\nelec_vote_y <- results_us_election_2016 |> \n    filter(state %in% c(\"Alabama\", \"Alaska\", \"Arizona\", \n                        \"California\", \"Connecticut\", \"Delaware\")) |> \n    select(state, electoral_votes) |> \n    rename(elec_vote = electoral_votes)\n\n\n\n\n\nLet’s see an example. Here we have two data sets, pop_x and elec_vote_y.\nThe left_join() function uses the common variable “state” to combine the two data sets.\nAnd the result of left joining will be that we keep the entire data set pop_x, and attach the variable elec_vote to the data.\nIf there is no such state or observation in y, its corresponding value of elec_vote is NA."
  },
  {
    "objectID": "slides/13-dplyr-2.html#right_joinx-y-all-rows-from-y",
    "href": "slides/13-dplyr-2.html#right_joinx-y-all-rows-from-y",
    "title": "Data Wrangling - two data frames 🛠",
    "section": "\nright_join(x, y): all rows from y",
    "text": "right_join(x, y): all rows from y\n\n\n\n\n\n\n\n\n\n\n\n\nright_join(x, y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 3 × 3\n     id var_x var_y\n  <dbl> <chr> <chr>\n1     1 x1    y1   \n2     2 x2    y2   \n3     4 <NA>  y4   \n\n\n\n\nNA is in the column coming from x.\n\n\n\n\n\n\n\n\n\n\n\n\nOK. If you get the idea of left_join, you =should be able to guess what right_join() function is doing.\nBasically right_join(x, y) keeps all the rows or observations from y, and second data set, and again keep all the variablesin both x and y, including id, var_x and var_y.\nFor any observation that is not in x, its corresponding value of var_x becomes a missing value NA.\nSo here because x does not have id 4, its value of var_x is NA.\nIn the venn diagram, the main data set is B or y. We keep everything of B, and we add stuff of A or x for observations that are in B or y only."
  },
  {
    "objectID": "slides/13-dplyr-2.html#right_join-example",
    "href": "slides/13-dplyr-2.html#right_join-example",
    "title": "Data Wrangling - two data frames 🛠",
    "section": "\nright_join() Example",
    "text": "right_join() Example\n\n\n\npop_x\n\n       state population\n1    Alabama    4779736\n2     Alaska     710231\n3    Arizona    6392017\n4   Arkansas    2915918\n5 California   37253956\n6   Colorado    5029196\n\n\n\nelec_vote_y\n\n        state elec_vote\n1  California        55\n2     Arizona        11\n3     Alabama         9\n4 Connecticut         7\n5      Alaska         3\n6    Delaware         3\n\n\n\n\npop_x |> \n    right_join(elec_vote_y) #<<\n\n        state population elec_vote\n1     Alabama    4779736         9\n2      Alaska     710231         3\n3     Arizona    6392017        11\n4  California   37253956        55\n5 Connecticut         NA         7\n6    Delaware         NA         3\n\n\n\n\nArkansas and Colorado in pop_x will not be shown in the right-joined data because they are not in elec_vote_y.\n\n\n\n\nBack to the example, if we are doing right-join, we keep the entire data set elec_vote_y, and attach the variable population of pop_x to the data.\nIf there is no such state or observation in x, its corresponding value of population is NA.\nSince x does not have Connecticut and Delaware, their population is NA.\n\nArkansas and Colorado in pop_x will not be shown in the right-joined data because they are not in elec_vote_y."
  },
  {
    "objectID": "slides/13-dplyr-2.html#full_joinx-y-all-rows-from-both-x-and-y",
    "href": "slides/13-dplyr-2.html#full_joinx-y-all-rows-from-both-x-and-y",
    "title": "Data Wrangling - two data frames 🛠",
    "section": "\nfull_join(x, y): all rows from both x and y",
    "text": "full_join(x, y): all rows from both x and y\n\n\n\n\n\n\n\n\n\n\n\n\nfull_join(x, y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 4 × 3\n     id var_x var_y\n  <dbl> <chr> <chr>\n1     1 x1    y1   \n2     2 x2    y2   \n3     3 x3    <NA> \n4     4 <NA>  y4   \n\n\n\nKeep all the rows and fill the missing parts with NAs.\n\n\n\n\n\n\n\n\n\n\n\n\nThe next is full_join(). full_join() preserves all the rows or observations either in x or in y or in both x and y.\nIn x, we have 1, 2, 3, and in y, we have 1, 2, 4. So the resulting full-joined data frame will have observations 1, 2, 3, 4, and fill the missing parts with NAs.\nx does not have id 4, so its var_x is NA. y does not have id 3, so its var_y is NA"
  },
  {
    "objectID": "slides/13-dplyr-2.html#full_join-example",
    "href": "slides/13-dplyr-2.html#full_join-example",
    "title": "Data Wrangling - two data frames 🛠",
    "section": "\nfull_join() Example",
    "text": "full_join() Example\n\n\n\npop_x\n\n       state population\n1    Alabama    4779736\n2     Alaska     710231\n3    Arizona    6392017\n4   Arkansas    2915918\n5 California   37253956\n6   Colorado    5029196\n\nelec_vote_y\n\n        state elec_vote\n1  California        55\n2     Arizona        11\n3     Alabama         9\n4 Connecticut         7\n5      Alaska         3\n6    Delaware         3\n\n\n\n\npop_x |> \n    full_join(elec_vote_y) #<<\n\n        state population elec_vote\n1     Alabama    4779736         9\n2      Alaska     710231         3\n3     Arizona    6392017        11\n4    Arkansas    2915918        NA\n5  California   37253956        55\n6    Colorado    5029196        NA\n7 Connecticut         NA         7\n8    Delaware         NA         3\n\n\n\n\nfull_join() takes the union of observations of x and y, so it produces the data set with the most rows.\n\n\n\n\nIn this example, the full-joined data set will contain observations either in x or in y or in both x and y.\nBecause Arkansas and Colorado are not in elec_vote_y, their elec_vote value is NA\nBecause Connecticut and Delaware are not in pop_x, their population value is NA\nfull_join() takes the union of x and y, so it produces the data set with the most rows."
  },
  {
    "objectID": "slides/13-dplyr-2.html#inner_joinx-y-only-rows-w-keys-in-both-x-and-y",
    "href": "slides/13-dplyr-2.html#inner_joinx-y-only-rows-w-keys-in-both-x-and-y",
    "title": "Data Wrangling - two data frames 🛠",
    "section": "\ninner_join(x, y): only rows w/ keys in both x and y",
    "text": "inner_join(x, y): only rows w/ keys in both x and y\n\n\n\n\n\n\n\n\n\n\n\n\ninner_join(x, y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 2 × 3\n     id var_x var_y\n  <dbl> <chr> <chr>\n1     1 x1    y1   \n2     2 x2    y2   \n\n\n\nKeep only the rows that have information in both tables.\n\n\n\n\n\n\n\n\n\n\n\n\nOK inner_join(). inner_join() preserves only rows with keys or id here in both x and y.\nAnd we know id 1 and id 2 are in both x and y, so these two observations are preserved.\nid 3 is in x, but not in y, id 4 is in y, but not in x, so both are not included in the data.\nBecause we are taking intersection of the rows of x and y, we will get the fewest rows when inner_join() is used."
  },
  {
    "objectID": "slides/13-dplyr-2.html#inner_join-example",
    "href": "slides/13-dplyr-2.html#inner_join-example",
    "title": "Data Wrangling - two data frames 🛠",
    "section": "\ninner_join() Example",
    "text": "inner_join() Example\n\n\n\npop_x\n\n       state population\n1    Alabama    4779736\n2     Alaska     710231\n3    Arizona    6392017\n4   Arkansas    2915918\n5 California   37253956\n6   Colorado    5029196\n\n\n\nelec_vote_y\n\n        state elec_vote\n1  California        55\n2     Arizona        11\n3     Alabama         9\n4 Connecticut         7\n5      Alaska         3\n6    Delaware         3\n\n\n\n\npop_x |> \n    inner_join(elec_vote_y) #<<\n\n       state population elec_vote\n1    Alabama    4779736         9\n2     Alaska     710231         3\n3    Arizona    6392017        11\n4 California   37253956        55\n\n\n\n\n\nIn this example, there are 4 states in both x and y, Alabama, Alaska, Arizona, and California.\nThe inner-joined data set only keep the 4 states data."
  },
  {
    "objectID": "slides/13-dplyr-2.html#section-2",
    "href": "slides/13-dplyr-2.html#section-2",
    "title": "Data Wrangling - two data frames 🛠",
    "section": "",
    "text": "16-Joining tables \n\n\nIn lab.qmd ## Lab 16 section\n\nImport the data at https://www.jaredlander.com/data/DiamondColors.csv. Call it diamond_color.\n\n\ndiamond_color <- readr::read_csv(\"the url\")\n\n\nUse left_join() to combine the data set diamonds in ggplot2 and diamond_color by the key variable color.\n\n\nSelect the variables carat, color, Description, Details.\n\n\n## Variable \"color\" in diamonds but \"Color\" in diamond_color\n\njoined_df <- diamonds |>  \n    _______(_______, by = c('color' = 'Color')) |>  ## join\n    _______(_________________________________________)  ## select\n\n\nCreate a bar chart of the variable color."
  },
  {
    "objectID": "slides/13-dplyr-2.html#section-3",
    "href": "slides/13-dplyr-2.html#section-3",
    "title": "Data Wrangling - two data frames 🛠",
    "section": "",
    "text": "# A tibble: 53,940 × 4\n  carat color Description    Details                  \n  <dbl> <chr> <chr>          <chr>                    \n1  0.23 E     Colorless      Minute traces of color   \n2  0.21 E     Colorless      Minute traces of color   \n3  0.23 E     Colorless      Minute traces of color   \n4  0.29 I     Near Colorless Slightly detectable color\n5  0.31 J     Near Colorless Slightly detectable color\n6  0.24 J     Near Colorless Slightly detectable color\n# ℹ 53,934 more rows"
  },
  {
    "objectID": "slides/13-dplyr-2.html#pd.merge",
    "href": "slides/13-dplyr-2.html#pd.merge",
    "title": "Data Wrangling - two data frames 🛠",
    "section": "pd.merge()",
    "text": "pd.merge()\n\n\nLeft join\nCode for generating the data sets\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\n\n\n\n\n\npop_x\n\n        state  population\n0     Alabama     4779736\n1      Alaska      710231\n2     Arizona     6392017\n3    Arkansas     2915918\n4  California    37253956\n5    Colorado     5029196\n\nelec_vote_y\n\n          state  electoral_votes\n21      Alabama                9\n43       Alaska                3\n13      Arizona               11\n0    California               55\n26  Connecticut                7\n44     Delaware                3\n\n\n\n\n## dplyr::left_join()\npd.merge(pop_x, elec_vote_y, how = 'left')\n\n        state  population  electoral_votes\n0     Alabama     4779736              9.0\n1      Alaska      710231              3.0\n2     Arizona     6392017             11.0\n3    Arkansas     2915918              NaN\n4  California    37253956             55.0\n5    Colorado     5029196              NaN\n\n\n\n\n\n\n\nmurders = pd.read_csv('./data/murders.csv')\npop_x = murders[0:6][['state','population']]\n\nelection = pd.read_csv('./data/results_us_election_2016.csv')\nraws1 = [\"Alabama\", \"Alaska\", \"Arizona\", \"California\", \"Connecticut\", \"Delaware\"]\ncols1 = [\"state\", \"electoral_votes\"]\ndf = election[cols1]\npop = []\nfor i in raws1:\n    mask = df[\"state\"] == i\n    pos = np.flatnonzero(mask)\n    pop.append(pos)\n\npop = np.array(pop)\npop = np.resize(pop, 6)\nelec_vote_y = df.iloc[pop]"
  },
  {
    "objectID": "slides/13-dplyr-2.html#section-5",
    "href": "slides/13-dplyr-2.html#section-5",
    "title": "Data Wrangling - two data frames 🛠",
    "section": "",
    "text": "pd.merge(pop_x, elec_vote_y, how = 'right') ## dplyr::right_join()\n\n         state  population  electoral_votes\n0      Alabama   4779736.0                9\n1       Alaska    710231.0                3\n2      Arizona   6392017.0               11\n3   California  37253956.0               55\n4  Connecticut         NaN                7\n5     Delaware         NaN                3\n\n\n\n\npd.merge(pop_x, elec_vote_y, how = 'outer') ## dplyr::full_join()\n\n         state  population  electoral_votes\n0      Alabama   4779736.0              9.0\n1       Alaska    710231.0              3.0\n2      Arizona   6392017.0             11.0\n3     Arkansas   2915918.0              NaN\n4   California  37253956.0             55.0\n5     Colorado   5029196.0              NaN\n6  Connecticut         NaN              7.0\n7     Delaware         NaN              3.0\n\n\n\n\n\npd.merge(pop_x, elec_vote_y, how = 'inner') ## dplyr::inner_join()\n\n        state  population  electoral_votes\n0     Alabama     4779736                9\n1      Alaska      710231                3\n2     Arizona     6392017               11\n3  California    37253956               55\n\n\n\n\n\n\nmath3570-s24.github.io/website"
  },
  {
    "objectID": "slides/08-import.html#readr-functions",
    "href": "slides/08-import.html#readr-functions",
    "title": "Data Importing \n",
    "section": "readr 📦 Functions",
    "text": "readr 📦 Functions\n\n\n\n\n\n\n\nFunction\nFormat\nTypical suffix\n\n\n\nread_table()\nwhite space separated values\ntxt\n\n\nread_csv()\ncomma separated values\ncsv\n\n\nread_csv2()\nsemicolon separated values\ncsv\n\n\nread_tsv()\ntab delimited separated values\ntsv\n\n\nread_fwf()\nfixed width files\ntxt\n\n\nread_delim()\ngeneral text file format, must define delimiter\ntxt\n\n\n\n\n\nBe careful: The suffix usually tells us what type of file it is, but no guarantee that these always match.\n\n\nreadr::read_lines(\"./data/murders.csv\", n_max = 3)  ## there is a header\n\n[1] \"state,abb,region,population,total\" \"Alabama,AL,South,4779736,135\"     \n[3] \"Alaska,AK,West,710231,19\"         \n\n\n\nreadr provides the following functions to read your data into R.\n\nBe careful: The suffix usually tells us what type of file it is, but no guarantee that these always match.\nIf you don’t want to open your data file, you can read several lines of your data in R using the function read_lines().\nFor example, I check the first three lines of the murders.csv file. And yes, it is a comma-separated-value or csv file.\nFixed width text files are special cases of text files where the format is specified by column widths, pad character and left/right alignment. Column widths are measured in units of characters. For example, if you have data in a text file where the first column always has exactly 10 characters, and the second column has exactly 5, the third has exactly 12 (and so on), this would be categorized as a fixed width text file."
  },
  {
    "objectID": "slides/08-import.html#reading-data",
    "href": "slides/08-import.html#reading-data",
    "title": "Data Importing \n",
    "section": "Reading Data",
    "text": "Reading Data\nread_csv() prints out a column specification giving us delimiter, name and type of each column.\n\nmurders_csv <- read_csv(file = \"./data/murders.csv\")\n# ── Column specification ─────────────\n# Delimiter: \",\"\n# chr (3): state, abb, region\n# dbl (2): population, total\nhead(murders_csv)\n\n# A tibble: 6 × 5\n  state      abb   region population total\n  <chr>      <chr> <chr>       <dbl> <dbl>\n1 Alabama    AL    South     4779736   135\n2 Alaska     AK    West       710231    19\n3 Arizona    AZ    West      6392017   232\n4 Arkansas   AR    South     2915918    93\n5 California CA    West     37253956  1257\n6 Colorado   CO    West      5029196    65\n\n\n\n## View data in RStudio\nview(murders_csv)\n\n\nOK. Let’s try to read the murders.csv file into R. We use read_csv(), and in the first argument, we tell R its file path. Your path is generally not the same as my path, so you get to change it to your path.\nWhen we run read_csv(), it prints out a column specification that gives the name and type of each column.\nYou can see the data set is actually the one in the dslabs package, and we use it in a previous lab exercise, right?"
  },
  {
    "objectID": "slides/08-import.html#missing-values",
    "href": "slides/08-import.html#missing-values",
    "title": "Data Importing \n",
    "section": "Missing Values",
    "text": "Missing Values\n\n\n\nWhich type is the column vector x? Why?\n\n\n\n\n\n\n\n\n\n\n\n\n\nType coercion1 happens and all column elements are transformed to character type.\n\n\nread_csv(\"./data/df-na.csv\")\n\n# A tibble: 9 × 3\n  x     y              z     \n  <chr> <chr>          <chr> \n1 1     a              hi    \n2 <NA>  b              hello \n3 3     Not applicable 9999  \n4 4     d              ola   \n5 5     e              hola  \n6 .     f              whatup\n7 7     g              wassup\n8 8     h              sup   \n9 9     i              <NA>  \n\n\n\n\n\n\nI have a csv file with 3 columns x, y, and z.\nIt looks like it should be some sort of numeric data. Right I have bunch of numbers here.\nCharacter string NA, but that’s just an NA. We have a period here, which is usually used as an NA as well.\nBut when we read this into R, we can see that it’s being read as a character.\nThe reason is that the period which is a character shows up, and type coercion happens and all column elements are transformed to character type.\nHow to solve this kind of problem. We want x to be double and the period is actually a missing value.\nImaging that if you have thousands of lines, you could very easily miss that period.\nA column with elements having different types, they’ll be coerced to the most flexible type. Types from least to most flexible: logical – integer – double – character."
  },
  {
    "objectID": "slides/08-import.html#solution-1-explicit-nas",
    "href": "slides/08-import.html#solution-1-explicit-nas",
    "title": "Data Importing \n",
    "section": "Solution 1: Explicit NAs",
    "text": "Solution 1: Explicit NAs\n\nBy default, read_csv() only recognizes ” “ and NA as a missing value.\nSpecify the values that are used to represent missing values by argument na.\n\n\nread_csv(\"./data/df-na.csv\", \n         na = c(\"\", \"NA\", \".\", \"9999\", \"Not applicable\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 9 × 3\n      x y     z     \n  <dbl> <chr> <chr> \n1     1 a     hi    \n2    NA b     hello \n3     3 <NA>  <NA>  \n4     4 d     ola   \n5     5 e     hola  \n6    NA f     whatup\n7     7 g     wassup\n8     8 h     sup   \n9     9 i     <NA>  \n\n\n\n\n\nThe function recognize string NA as missing value, but not period or any other character or strings.\nOne solution is to specify explicitly the value (or values) that are used to represent missing values.\nIf you know what sort of character are used to denote missing values in your data file, you can give those as part of na argument in the read_csv() function.\nFor example here, I treat ““,”NA”, “.”, “9999”, “Not applicable” all as missing values.\nAnd now the class of x is double, and all those character strings are denoted as NA in the loaded data set."
  },
  {
    "objectID": "slides/08-import.html#solution-2-specify-column-types",
    "href": "slides/08-import.html#solution-2-specify-column-types",
    "title": "Data Importing \n",
    "section": "Solution 2: Specify Column Types",
    "text": "Solution 2: Specify Column Types\n\n\n\nread_csv(\"./data/df-na.csv\", \n         col_types = \n             cols(col_double(), \n                  col_character(), \n                  col_character()))\n\n# A tibble: 9 × 3\n      x y              z     \n  <dbl> <chr>          <chr> \n1     1 a              hi    \n2    NA b              hello \n3     3 Not applicable 9999  \n4     4 d              ola   \n5     5 e              hola  \n6    NA f              whatup\n7     7 g              wassup\n8     8 h              sup   \n9     9 i              <NA>  \n\n# Warning message:\n# One or more parsing issues, \n# call `problems()` \n# on your data frame for details\n\n\n\nproblems()\n# A tibble: 1 × 5\n#     row   col expected actual file \n#   <int> <int> <chr>    <chr>  <chr>\n# 1     7     1 a double .      \"\" \n\n\n\n\nAnother solution is to specify column types when you import your data.\nThis might be handy when you know what your column types should be.\nFor example, here I can specify the column types using the col_types argument.\nAll column types are wrapped up in the cols() command, that x is double, y is character and z is character as well.\nAns now you can see x becomes double.\nAnd here we actually get a warning. In row 6 and col x, we expect a double but it is actually a dot or period, which is a character, and so R treats it as NA, which is exactly what I want to do.\nBut R send a message telling us about this, and make sure this is what we want to do.\nThis option might be preferable if you cannot scan your data file, or don;t know what convention is being used for missing values in your data file, but you happen to know your column types.\nThe downside of this is that, here, Not applicable and 9999, they are all treated as valid character values, and they are not missing values. So be careful about this. You may need to manually clean your data."
  },
  {
    "objectID": "slides/08-import.html#column-types",
    "href": "slides/08-import.html#column-types",
    "title": "Data Importing \n",
    "section": "Column Types",
    "text": "Column Types\n\n\ntype function\ndata type\n\n\n\ncol_character()\ncharacter\n\n\ncol_date()\ndate\n\n\ncol_datetime()\nPOSIXct (date-time)\n\n\ncol_double()\ndouble (numeric)\n\n\ncol_factor()\nfactor\n\n\ncol_guess()\nlet readr guess (default)\n\n\ncol_integer()\ninteger\n\n\ncol_logical()\nlogical\n\n\ncol_number()\nnumbers mixed with non-number characters\n\n\ncol_numeric()\ndouble or integer\n\n\ncol_skip()\ndo not read\n\n\ncol_time()\ntime\n\n\n\n\nHere shows all possible column types you can use when importing your data into R. No need to memorize it. But just aware of them, and use them when you need to."
  },
  {
    "objectID": "slides/08-import.html#writing-data",
    "href": "slides/08-import.html#writing-data",
    "title": "Data Importing \n",
    "section": "Writing Data",
    "text": "Writing Data\n\n## Create tibbles using a row-by-row layout\n(df <- tribble(\n  ~x, ~y,\n  1,  \"a\",\n  2,  \"b\",\n  3,  \"c\"\n))\n\n# A tibble: 3 × 2\n      x y    \n  <dbl> <chr>\n1     1 a    \n2     2 b    \n3     3 c    \n\n## same as tibble(x = 1:3, y = c(a, b, c))\n\n\n## save data to \"./data/df.csv\"\ndf |> write_csv(file = \"./data/df.csv\")\n\n\nWe can also write our data to a csv file as well.\nHere I create a tibble called df, and then I use write_csv() function to write the data set df to the file df.csv."
  },
  {
    "objectID": "slides/08-import.html#read_rds-and-write_rds",
    "href": "slides/08-import.html#read_rds-and-write_rds",
    "title": "Data Importing \n",
    "section": "\nread_rds() and write_rds()\n",
    "text": "read_rds() and write_rds()\n\n\nWe save an R object (usually a data set) in .Rds in the R binary file format. 1\n\n\n\nreadr::write_rds(cars, \n                 file = \"./data/cars.rds\") \n# fs::dir_ls(path = \"./data\") |> head(10) (Check Files using fs)\n\n\n\n\nmy_car <- readr::read_rds(file = \"./data/cars.rds\") \nhead(my_car, 3)\n\n  speed dist\n1     4    2\n2     4   10\n3     7    4\n\n\n\nWe save an R object (usually a data set) in .Rdsm the R binary file format.\nRemember we talked about RData format last time, right?\nR provides two file formats of its own for storing data, .RDS and .RData. RDS files can store a single R object, and RData files can store multiple R objects.\nUsually, if we save a data set, we use .Rds, and if we save several objects, and the objects are some variables or functions, we use .RData.\nRead the data back into R.\n\n\nCheck R built-in data sets using command data()."
  },
  {
    "objectID": "slides/08-import.html#section-4",
    "href": "slides/08-import.html#section-4",
    "title": "Data Importing \n",
    "section": "",
    "text": "10-Import Data \n\n\nIf you haven’t, install and load the tidyverse package.\n\nIn lab.qmd ## Lab 10 section,        \n\nImport ssa_male_prob.csv and ssa_female_prob.Rds in the data folder using read_csv() and call them ssa_male and ssa_female, respectively.\n\n\nssa_male <- readr::read____(____________)\nssa_female <- readr::read____(____________)\n\n\nPlot Age (x-axis) vs. LifeExp (y-axis) for Female. The type should be “line”, and the line color is red. Add x-label, y-label and title to your plot.\n\n\nplot(x = _____, y = _____, type = ______, col = ______,\n     xlab = ______, ylab = _______, main = ____________)\n\n\nUse lines() to add a line of Age (x-axis) vs. LifeExp (y-axis) for Male to the plot. The color is blue.\n\n\nlines(x = _____, y = _____, col = ______)\n\n\n\nlibrary(tidyverse)\nssa <- read_csv(file = \"./data/ssa-death-probability.csv\")\nssa_male <- ssa[ssa$Sex == \"Male\", ]\nssa_female <- ssa[ssa$Sex == \"Female\", ]\nplot(x = ssa_female$Age, y = ssa_female$LifeExp, \n     type = \"l\", col = 2, lwd = 3,\n     xlab = \"Age\", ylab = \"Life Exp\",\n     main = \"Age vs. Life Exp by Gender\")\nlines(ssa_male$Age, ssa_male$LifeExp, col = 4, lwd = 3)"
  },
  {
    "objectID": "slides/08-import.html#readxl-functions",
    "href": "slides/08-import.html#readxl-functions",
    "title": "Data Importing \n",
    "section": "readxl 📦 Functions",
    "text": "readxl 📦 Functions\n\n\n\n\n\n\n\nFunction\nFormat\nTypical suffix\n\n\n\nread_excel()\nauto detect the format\nxls, xlsx\n\n\nread_xls()\noriginal format\nxls\n\n\nread_xlsx()\nnew format\nxlsx\n\n\n\n\nThe Microsoft Excel can have more than one sheet in one file.\nThe functions above read the first sheet by default.\nThe excel_sheets() gives us the names of all the sheets in an Excel file.\n\n\nlibrary(readxl)\nexcel_sheets(\"./data/2010_bigfive_regents.xls\")\n\n[1] \"Sheet1\" \"Sheet2\" \"Sheet3\"\n\n\n\nHere shows the main read excel functions. They are pretty similar to readr functions, but this time, your data file is not a text file, but a microsoft excel file.\nThe Microsoft Excel can have more than one sheet in one file.\nThe functions listed above read the first sheet by default.\nIf you don’t want open your excel file and check its sheet names, the excel_sheets() function gives us the names of all the sheets in an Excel file."
  },
  {
    "objectID": "slides/08-import.html#sheet-names",
    "href": "slides/08-import.html#sheet-names",
    "title": "Data Importing \n",
    "section": "Sheet Names",
    "text": "Sheet Names\n\nThe sheet names can be passed to the sheet argument to read sheets other than the first.\n\n\nexcel_sheets(\"./data/2010_bigfive_regents.xls\")\n\n[1] \"Sheet1\" \"Sheet2\" \"Sheet3\"\n\n(data_xls <- read_xls(path = \"./data/2010_bigfive_regents.xls\", \n                      sheet = \"Sheet3\", skip = 1))\n\n# A tibble: 19 × 6\n  Scores `131024` `113804` `104201` `103886` `91756`\n   <dbl>    <dbl>    <dbl>    <dbl>    <dbl>   <dbl>\n1     10       NA       64        8      227      34\n2     11        6       83       11      217      58\n3     12       23       87        7       28      67\n4     13        1       54       16      230      42\n5     14        3      145       18      303      57\n6     15       58      151       50      192      98\n7     16        1      129       13      156     125\n8     17       73      214       59      163     115\n# ℹ 11 more rows\n\n\n\nThe sheet names can be passed to the sheet argument in the functions to read sheets other than the first.\nFor example, here we ask R to read the Sheet1 in the favourite-food excel file.\nWe can also specify an integer that indicates the position of the sheet.\nWe don’t have a lab exercise on this. You have data and code is right here. You can practice and see if you can read the excel file."
  },
  {
    "objectID": "slides/08-import.html#pd.read_csv",
    "href": "slides/08-import.html#pd.read_csv",
    "title": "Data Importing \n",
    "section": "pd.read_csv",
    "text": "pd.read_csv\n\nimport numpy as np\nimport pandas as pd\n\npy_df = pd.read_csv('./data/murders.csv')\npy_df.head()\n\n        state abb region  population  total\n0     Alabama  AL  South     4779736    135\n1      Alaska  AK   West      710231     19\n2     Arizona  AZ   West     6392017    232\n3    Arkansas  AR  South     2915918     93\n4  California  CA   West    37253956   1257"
  },
  {
    "objectID": "slides/08-import.html#pd.dataframe.to_csv",
    "href": "slides/08-import.html#pd.dataframe.to_csv",
    "title": "Data Importing \n",
    "section": "pd.DataFrame.to_csv",
    "text": "pd.DataFrame.to_csv\n\nw = {\"x\":[1, 2, 3], \n     \"y\":['a', 'b','c']}\nwdf = pd.DataFrame(w)\n\nwdf.to_csv(\"./data/wdf.csv\")\nmydf = pd.read_csv('./data/wdf.csv')\nmydf.head()\n\n   Unnamed: 0  x  y\n0           0  1  a\n1           1  2  b\n2           2  3  c\n\n\n\n## index = False means don't write row names\nwdf.to_csv(\"./data/wdf.csv\", index = False)\nmydf = pd.read_csv('./data/wdf.csv')\nmydf.head()\n\n   x  y\n0  1  a\n1  2  b\n2  3  c\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmath3570-s24.github.io/website"
  },
  {
    "objectID": "slides/18-knn.html#prediction",
    "href": "slides/18-knn.html#prediction",
    "title": "Data Splitting and K-Nearest Neighbors \n",
    "section": "Prediction",
    "text": "Prediction\n\nGoal: Build a good regression function or classifier in terms of prediction accuracy.\n\n\n\nThe mechanics of prediction is easy:\n\nPlug in values of predictors to the model equation.\nCalculate the predicted value of the response \\(\\hat{y}\\)\n\n\n\n\n\n\n\n\nGetting it right is hard! No guarantee that\n\nthe model estimates are close to the truth\nyour model performs as well with new data (test data) as it did with your sample data (training data)"
  },
  {
    "objectID": "slides/18-knn.html#spending-our-data",
    "href": "slides/18-knn.html#spending-our-data",
    "title": "Data Splitting and K-Nearest Neighbors \n",
    "section": "Spending Our Data",
    "text": "Spending Our Data\n\nSeveral steps to create a useful model:\n\nParameter estimation\nModel selection\nPerformance assessment, etc.\n\n\n\n\n\nDoing all of this on the entire data may lead to overfitting:\n\n\n\nThe model performs well on the training data, but awfully predicts the response on the new data we are interested."
  },
  {
    "objectID": "slides/18-knn.html#overfitting",
    "href": "slides/18-knn.html#overfitting",
    "title": "Data Splitting and K-Nearest Neighbors \n",
    "section": "Overfitting",
    "text": "Overfitting\n\nThe model performs well on the training data, but awfully predicts the response on the new data we are interested.\n\n\nLow error rate on observed data, but high prediction error rate on future unobserved data!\n\n\n\n\n\n\nSource: modified from https://i.pinimg.com/originals/72/e2/22/72e222c1542539754df1d914cb671bd7.png\n\n\n\n\n\nhttps://i.pinimg.com/originals/72/e2/22/72e222c1542539754df1d914cb671bd7.png - Look at this illustration, and let’s focus on the overfitting and classification case. - the blue and red points are our training data representing two categories, and the green points are the new data to be classified. - the black curve is the classification boundary that separates the two categories. - Based on the boundary, you can see that the classification performance on the training data set is perfect, because all blue points and red points are perfectly separated. - However, such classification rule generated by the training data may not be good for the new data. - With this boundary, … - OK so, if we wanna make sure that our model is good at predicting things, we probably want to avoid overfitting. - But how?"
  },
  {
    "objectID": "slides/18-knn.html#splitting-data",
    "href": "slides/18-knn.html#splitting-data",
    "title": "Data Splitting and K-Nearest Neighbors \n",
    "section": "Splitting Data",
    "text": "Splitting Data\n\nOften, we don’t have another unused data to assess the performance of our model.\nSolution: Pretend we have new data by splitting our data into training set and test set (validation set)!\n\n\n\n\nTraining set:\n\nSandbox for model building\nSpend most of our time using the training set to develop the model\nMajority of the original sample data (75% - 80%)\n\n\n\n\n\n\n\nTest set:\n\nHeld in reserve to determine efficacy of one or two chosen models\nCritical to look at it once only, otherwise it becomes part of the modeling process\nRemainder of the data (20% - 25%)\n\n\n\n\n\nAllocate specific subsets of data for different tasks, as opposed to allocating the largest possible amount to the model parameter estimation only (what we’ve done so far).\nWell we do this by splitting our data. So we split our data into to sets, training set and testing set, or sometimes called validation set.\nYou can think about your training set as your sandbox for model building. You can do whatever you want, like data wrangling, data transformation, data tidying, and data visualization, all of which help you build an appropriate model.\nSo you Spend most of your time using the training set to develop the model\nAnd this is the Majority of the original sample data, which is usually about 75% - 80% of your data. So you basically take a random sample from the data that is about 80% of it.\nAnd you don’t touch the remaining 20% of the data until you are ready to test your model performance.\nSo the test set is held in reserve to determine efficacy of one or two chosen models\nCritical to look at it once, otherwise it becomes part of the modeling process\nand that is the Remainder of the data, usually 20% - 25%\nSo ideally, we hope to use our entire data as training data to train our model, right? And to test the model performance, we just collect another data set as test data to be used for testing performance. But in reality, it is not the usual case. In reality, we only have one single data set, and it is hard to collect another sample data as test data.\nSo under this situation, this type of splitting data becomes a must if we want to have both training and test data."
  },
  {
    "objectID": "slides/18-knn.html#initial_split-in-rsample",
    "href": "slides/18-knn.html#initial_split-in-rsample",
    "title": "Data Splitting and K-Nearest Neighbors \n",
    "section": "initial_split() in \n",
    "text": "initial_split() in \n\n\nCodebodydata <- read_csv(\"./data/body.csv\")\nbody <- bodydata |> \n    select(GENDER, HEIGHT, WAIST, BMI) |> \n    mutate(GENDER = as.factor(GENDER))\n\n\n\n\n\n\nset.seed(2023)\ndf_split <- rsample::initial_split(\n    data = body, \n    prop = 0.8)\n\ndf_split\n\n<Training/Testing/Total>\n<240/60/300>\n\n\n\n\ndf_trn <- rsample::training(df_split)\ndf_tst <- rsample::testing(df_split)\n\ndim(df_trn)\n\n[1] 240   4\n\ndim(df_tst)\n\n[1] 60  4\n\n\n\n\nnames(df_split)\ndf_split$in_id |> head()"
  },
  {
    "objectID": "slides/18-knn.html#body-data",
    "href": "slides/18-knn.html#body-data",
    "title": "Data Splitting and K-Nearest Neighbors \n",
    "section": "\nbody Data",
    "text": "body Data\n\n\n\ndf_trn\n\n# A tibble: 240 × 4\n  GENDER HEIGHT WAIST   BMI\n  <fct>   <dbl> <dbl> <dbl>\n1 1        185. 115    30.2\n2 1        171.  78    22.2\n3 0        155. 104.   30.4\n4 0        156. 132.   47.2\n5 0        157.  90.5  24.6\n6 0        170. 134.   36.7\n# ℹ 234 more rows\n\n\n\n\ndf_tst\n\n# A tibble: 60 × 4\n  GENDER HEIGHT WAIST   BMI\n  <fct>   <dbl> <dbl> <dbl>\n1 0        172  120.   33.3\n2 1        166.  95    25.8\n3 1        181. 119.   37.4\n4 0        164   75.5  19.3\n5 1        181.  92.5  27.4\n6 0        156. 110    33.6\n# ℹ 54 more rows"
  },
  {
    "objectID": "slides/18-knn.html#what-makes-a-good-classifier-test-accuracy-rate",
    "href": "slides/18-knn.html#what-makes-a-good-classifier-test-accuracy-rate",
    "title": "Data Splitting and K-Nearest Neighbors \n",
    "section": "What Makes a Good Classifier: Test Accuracy Rate",
    "text": "What Makes a Good Classifier: Test Accuracy Rate\n\nThe test accuracy rate associated with the test data \\(\\{x_j, y_j\\}_{j=1}^J\\): \\[ \\frac{1}{J}\\sum_{j=1}^JI(y_j = \\hat{y}_j),\\] where \\(\\hat{y}_j\\) is the predicted label resulting from applying the classifier to the test response \\(y_j\\) with predictor \\(x_j\\).\n\n\nWhat is the value of \\(J\\) in our example?\n\n\n\nThe best estimated classifier \\(\\hat{C}(x)\\) trained from the training data for \\(C(x)\\) is the one producing the highest test accuracy rate or lowest test error rate."
  },
  {
    "objectID": "slides/18-knn.html#k-nearest-neighbors-knn-classifier",
    "href": "slides/18-knn.html#k-nearest-neighbors-knn-classifier",
    "title": "Data Splitting and K-Nearest Neighbors \n",
    "section": "K-Nearest Neighbors (KNN) Classifier",
    "text": "K-Nearest Neighbors (KNN) Classifier\nKNN classification uses majority voting:\n\nLook for the most popular class label among its neighbors.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen predicting at \\(x = (x_1, x_2) = (8, 6)\\),\n\\[\\begin{align}\n\\hat{\\pi}_{3Blue}(x = (8, 6)) &= \\hat{P}(Y = \\text{Blue} \\mid x = (8, 6))\\\\\n&= \\frac{2}{3}\n\\end{align}\\]\n\\[\\begin{align}\n\\hat{\\pi}_{3Orange}(x = (8, 6)) &= \\hat{P}(Y = \\text{Orange} \\mid x = (8, 6))\\\\\n&= \\frac{1}{3}\n\\end{align}\\]  \n\n\n\nHere is a graphical example. Suppose K = 3 and we have two predictors, \\(x_1\\) and \\(x_2\\). We want to do classification of \\(Y\\) when \\(x_1\\) is 8 and \\(x_2\\) is 6.\nHere how do we define neighbors, we use Euclidean distance to decide who are the point (8, 6)’s neighbors.\nShowing the idea in the figure, we just use the point (8, 6) as the center of a circle, draw a circle with a larger and larger radius until the circle captures 3 other data points that will be treated as neighbors.\nHere you can see the green circle captures the points, two are blue and one is orange.\nSo when we do classification at (8, 6), we just compute the proportion of blue neighbors and the proportion of the orange neighbors, and assign the category with the highest proportion or probability to the response variable at the value of predictors (8, 6)."
  },
  {
    "objectID": "slides/18-knn.html#knn-decision-boundary",
    "href": "slides/18-knn.html#knn-decision-boundary",
    "title": "Data Splitting and K-Nearest Neighbors \n",
    "section": "KNN Decision Boundary",
    "text": "KNN Decision Boundary\n\nBlue grid indicates the region in which a test response is assigned to the blue class.\nWe don’t know the true boundary (the true classification rule)!.\n\n\n\nOK. Now for every possible value of \\(x_1\\) and \\(x_2\\), we can classify its corresponding response, right?\nSo we can actually create a dense grid of \\(x_1\\) and \\(x_2\\), and label each point on the grid.\nAnd so we can have a whole picture of how the classification result looks like.\nBlue (Orange) grid indicates the region in which a test observation will be assigned to the blue (orange) class.\nThe curves that separates different classes are called decision boundaries.\nAgain, in reality, we don’t know the true boundary, the Bayesian decision boundary."
  },
  {
    "objectID": "slides/18-knn.html#knn-training-recipes",
    "href": "slides/18-knn.html#knn-training-recipes",
    "title": "Data Splitting and K-Nearest Neighbors \n",
    "section": "KNN Training \n",
    "text": "KNN Training \n\n\n\nStep 1: Create recipe: recipes::recipe()\n\n\nStandardize predictors before doing KNN!\n\n(knn_recipe <- recipes::recipe(GENDER ~ HEIGHT, data = df_trn) |> \n    step_normalize(all_predictors()))\n\nA recipe is a description of the steps to be applied to a data set in order to prepare it for data analysis."
  },
  {
    "objectID": "slides/18-knn.html#knn-training-parsnip",
    "href": "slides/18-knn.html#knn-training-parsnip",
    "title": "Data Splitting and K-Nearest Neighbors \n",
    "section": "KNN Training \n",
    "text": "KNN Training \n\n\n\nStep 2: Specify Model: parsnip::nearest_neighbor()\n\n\n\n(knn_mdl <- parsnip::nearest_neighbor(neighbors = 3) |> \n    set_mode(\"classification\") |> \n    set_engine(\"kknn\"))\n\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = 3\n\nComputational engine: kknn"
  },
  {
    "objectID": "slides/18-knn.html#knn-training-workflows",
    "href": "slides/18-knn.html#knn-training-workflows",
    "title": "Data Splitting and K-Nearest Neighbors \n",
    "section": "KNN Training \n",
    "text": "KNN Training \n\n\n\nStep 3: Fitting by creating workflow: workflows::workflow()\n\n\n\n(knn_fit <- workflows::workflow() |> \n    add_recipe(knn_recipe) |> \n    add_model(knn_mdl) |> \n    fit(data = df_trn))\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nkknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(3,     data, 5))\n\nType of response variable: nominal\nMinimal misclassification: 0.217\nBest kernel: optimal\nBest k: 3"
  },
  {
    "objectID": "slides/18-knn.html#prediction-on-test-data",
    "href": "slides/18-knn.html#prediction-on-test-data",
    "title": "Data Splitting and K-Nearest Neighbors \n",
    "section": "Prediction on Test Data",
    "text": "Prediction on Test Data\n\n\n\nbind_cols(\n    predict(knn_fit, df_tst),\n    predict(knn_fit, df_tst, type = \"prob\")) |> \n    dplyr::sample_n(size = 8)\n\n# A tibble: 8 × 3\n  .pred_class .pred_0 .pred_1\n  <fct>         <dbl>   <dbl>\n1 1             0       1    \n2 1             0.370   0.630\n3 0             0.519   0.481\n4 1             0.370   0.630\n5 0             0.852   0.148\n6 1             0       1    \n7 0             1       0    \n8 0             0.630   0.370\n\n\n\n\nknn_pred <- \n    pull(predict(knn_fit, df_tst))\n\n## Confusion matrix\ntable(knn_pred, df_tst$GENDER)\n\n        \nknn_pred  0  1\n       0 25  8\n       1  9 18\n\n## Test accuracy rate\nmean(knn_pred == df_tst$GENDER)\n\n[1] 0.717\n\n\n\n\nmean(knn_pred != df_tst$GENDER)"
  },
  {
    "objectID": "slides/18-knn.html#which-k-should-we-use",
    "href": "slides/18-knn.html#which-k-should-we-use",
    "title": "Data Splitting and K-Nearest Neighbors \n",
    "section": "Which K Should We Use?",
    "text": "Which K Should We Use?\n\n\\(K\\)-nearest neighbors has no model parameters, but a tuning parameter \\(K\\).\nThis is a parameter which determines how the model is trained, not a parameter that is learned through training."
  },
  {
    "objectID": "slides/18-knn.html#v-fold-cross-validation",
    "href": "slides/18-knn.html#v-fold-cross-validation",
    "title": "Data Splitting and K-Nearest Neighbors \n",
    "section": "\n\\(v\\)-fold Cross Validation",
    "text": "\\(v\\)-fold Cross Validation\n\nUse \\(v\\)-fold Cross Validation (CV) to choose tuning parameters. (MATH 4750 Computational Statistics)\nUsually use \\(v = 5\\) or \\(10\\).\n\nIDEA:\n\nPrepare \\(v\\) CV data sets\nCreate a sequence of values of \\(K\\)\n\nFor each value of \\(K\\), run CV, and obtain an accuracy rate\nChoose the \\(K\\) with the highest accuracy rate"
  },
  {
    "objectID": "slides/18-knn.html#final-model",
    "href": "slides/18-knn.html#final-model",
    "title": "Data Splitting and K-Nearest Neighbors \n",
    "section": "Final Model",
    "text": "Final Model\n\nknn_mdl_best <- parsnip::nearest_neighbor(neighbors = 19) |>\n    set_mode(\"classification\") |>\n    set_engine(\"kknn\")\n\nknn_fit_best <- workflows::workflow() |>\n    add_recipe(knn_recipe) |>\n    add_model(knn_mdl_best) |>\n    fit(data = df_trn)\n\nbest_K <- accu_cv |> slice_max(mean) |> pull(neighbors) |> as.integer()"
  },
  {
    "objectID": "slides/18-knn.html#final-model-performance",
    "href": "slides/18-knn.html#final-model-performance",
    "title": "Data Splitting and K-Nearest Neighbors \n",
    "section": "Final Model Performance",
    "text": "Final Model Performance\n\nknn_pred_best <- pull(predict(knn_fit_best, df_tst))\n\n## Confusion matrix\ntable(knn_pred_best, df_tst$GENDER)\n\n             \nknn_pred_best  0  1\n            0 25  6\n            1  9 20\n\n## Test accuracy rate\nmean(knn_pred_best == df_tst$GENDER)\n\n[1] 0.75"
  },
  {
    "objectID": "slides/18-knn.html#sklearn.model_selection.train_test_split",
    "href": "slides/18-knn.html#sklearn.model_selection.train_test_split",
    "title": "Data Splitting and K-Nearest Neighbors \n",
    "section": "sklearn.model_selection.train_test_split",
    "text": "sklearn.model_selection.train_test_split\n\nlibrary(reticulate); py_install(\"scikit-learn\")\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\n\n\nbody = pd.read_csv('./data/body.csv')\n\nX = body[['HEIGHT']]\ny = body['GENDER']\nX_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.2, random_state=2024)"
  },
  {
    "objectID": "slides/18-knn.html#sklearn.neighbors.kneighborsclassifier",
    "href": "slides/18-knn.html#sklearn.neighbors.kneighborsclassifier",
    "title": "Data Splitting and K-Nearest Neighbors \n",
    "section": "sklearn.neighbors.KNeighborsClassifier",
    "text": "sklearn.neighbors.KNeighborsClassifier\n\nneigh = KNeighborsClassifier(n_neighbors = 3)\nX_trn = np.array(X_trn)\nX_tst = np.array(X_tst)\nneigh.fit(X_trn, y_trn)\n\n\n\n\nKNeighborsClassifier(n_neighbors=3)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nKNeighborsClassifierKNeighborsClassifier(n_neighbors=3)"
  },
  {
    "objectID": "slides/18-knn.html#prediction-1",
    "href": "slides/18-knn.html#prediction-1",
    "title": "Data Splitting and K-Nearest Neighbors \n",
    "section": "Prediction",
    "text": "Prediction\n\ny_pred = neigh.predict(X_tst)\n\n\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_tst, y_pred)\n\narray([[22,  5],\n       [12, 21]])\n\n\n\n\n\nnp.mean(y_tst == y_pred)\n\n0.7166666666666667\n\n\n\nnp.mean(y_tst != y_pred)\n\n0.2833333333333333"
  },
  {
    "objectID": "slides/18-knn.html#section-3",
    "href": "slides/18-knn.html#section-3",
    "title": "Data Splitting and K-Nearest Neighbors \n",
    "section": "",
    "text": "22-K Nearest Neighbors \nIn lab.qmd ## Lab 22 section,\n\nuse HEIGHT, WAIST and BMI to predict GENDER using KNN with \\(K = 3\\).\nGenerate the (test) confusion matrix.\nCalculate (test) accuracy rate.\nDoes using more predictors predict better?\n\n\nlibrary(tidymodels)\n## load data\nbodydata <- read_csv(\"./data/body.csv\")\nbody <- bodydata |> \n    select(GENDER, HEIGHT, WAIST, BMI) |> \n    mutate(GENDER = as.factor(GENDER))\n\n## training and test data\nset.seed(2024)\ndf_split <- rsample::initial_split(data = body, prop = 0.8)\ndf_trn <- rsample::training(df_split)\ndf_tst <- rsample::testing(df_split)"
  },
  {
    "objectID": "slides/18-knn.html#r-code",
    "href": "slides/18-knn.html#r-code",
    "title": "Data Splitting and K-Nearest Neighbors \n",
    "section": "R Code",
    "text": "R Code\n\n## KNN training\nknn_recipe <- recipes::recipe(GENDER ~ HEIGHT + WAIST + BMI, data = df_trn) |> \n    step_normalize(all_predictors())\n\nknn_mdl <- parsnip::nearest_neighbor(neighbors = 3) |> \n    set_mode(\"classification\") |>\n    set_engine(\"kknn\")\n\nknn_out <- workflows::workflow() |> \n    add_recipe(knn_recipe) |> \n    add_model(knn_mdl) |> \n    fit(data = df_trn)\n\n## KNN prediction\nknn_pred <- pull(predict(knn_out, df_tst))\ntable(knn_pred, df_tst$GENDER)\nmean(knn_pred == df_tst$GENDER)"
  },
  {
    "objectID": "slides/18-knn.html#python-code",
    "href": "slides/18-knn.html#python-code",
    "title": "Data Splitting and K-Nearest Neighbors \n",
    "section": "Python Code",
    "text": "Python Code\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\n## load data\nbody = pd.read_csv('./data/body.csv')\n\nX = body[['HEIGHT', 'WAIST', 'BMI']]\ny = body['GENDER']\nX_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.2, random_state=2024)\n\n## KNN training\nneigh = KNeighborsClassifier(n_neighbors = 3)\nX_trn = np.array(X_trn)\nX_tst = np.array(X_tst)\nneigh.fit(X_trn, y_trn)\n\n## KNN prediction\ny_pred = neigh.predict(X_tst)\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_tst, y_pred)\n\n\n\n\nmath3570-s24.github.io/website"
  },
  {
    "objectID": "slides/10-viz.html#categorical-vs.-numerical-variables",
    "href": "slides/10-viz.html#categorical-vs.-numerical-variables",
    "title": "Visualizing Data 📈",
    "section": "Categorical vs. Numerical Variables",
    "text": "Categorical vs. Numerical Variables\n\nA categorical (qualitative) variable provides non-numerical information which can be placed in one (and only one) category from two or more categories.\n\n\n\nGender (Male 👨, Female 👩, Other 🏳️‍🌈) \nClass (Freshman, Sophomore, Junior, Senior, Graduate) \nCountry (USA 🇺🇸, Canada 🇨🇦, UK 🇬🇧, Germany 🇩🇪, Japan 🇯🇵, Korea 🇰🇷) \n\n\n\n\nA numerical (quantitative) variable is recorded in a numerical value representing counts or measurements.\n\n\n\n GPA \n The number of relationships you’ve had \n Height"
  },
  {
    "objectID": "slides/10-viz.html#data-lending-club",
    "href": "slides/10-viz.html#data-lending-club",
    "title": "Visualizing Data 📈",
    "section": "Data: Lending Club",
    "text": "Data: Lending Club\n\nLending Club is a platform that allows individuals to lend to other individuals.\nNot all loans are created equal – ease of getting a loan depends on ability to pay back the loan.\nData includes loans made, these are not loan applications.\n\n\n\nOK. The data set we will be working with comes from the lending club.\nThousands of loans made through the Lending Club, which is a platform that allows individuals to lend to other individuals.\nIf you are finance or econ major, or you know about loans, you know that Not all loans are created equal – ease of getting a loan depends on (apparent) ability to pay back the loan.\nData include loans made, these are not loan applications.\nIt’s important to keep this in mind when we are looking at the relationships between the variables that we are going to see, since the pattern may be different for those relationships for loans that never got approved."
  },
  {
    "objectID": "slides/10-viz.html#take-a-peek-at-data",
    "href": "slides/10-viz.html#take-a-peek-at-data",
    "title": "Visualizing Data 📈",
    "section": "Take a Peek at Data",
    "text": "Take a Peek at Data\n\nlibrary(openintro) ## for loading the data set\ndplyr::glimpse(loans_full_schema)\n\nRows: 10,000\nColumns: 55\n$ emp_title                        <chr> \"global config engineer \", \"warehouse…\n$ emp_length                       <dbl> 3, 10, 3, 1, 10, NA, 10, 10, 10, 3, 1…\n$ state                            <fct> NJ, HI, WI, PA, CA, KY, MI, AZ, NV, I…\n$ homeownership                    <fct> MORTGAGE, RENT, RENT, RENT, RENT, OWN…\n$ annual_income                    <dbl> 90000, 40000, 40000, 30000, 35000, 34…\n$ verified_income                  <fct> Verified, Not Verified, Source Verifi…\n$ debt_to_income                   <dbl> 18.01, 5.04, 21.15, 10.16, 57.96, 6.4…\n$ annual_income_joint              <dbl> NA, NA, NA, NA, 57000, NA, 155000, NA…\n$ verification_income_joint        <fct> , , , , Verified, , Not Verified, , ,…\n$ debt_to_income_joint             <dbl> NA, NA, NA, NA, 37.7, NA, 13.1, NA, N…\n$ delinq_2y                        <int> 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0…\n$ months_since_last_delinq         <int> 38, NA, 28, NA, NA, 3, NA, 19, 18, NA…\n$ earliest_credit_line             <dbl> 2001, 1996, 2006, 2007, 2008, 1990, 2…\n$ inquiries_last_12m               <int> 6, 1, 4, 0, 7, 6, 1, 1, 3, 0, 4, 4, 8…\n$ total_credit_lines               <int> 28, 30, 31, 4, 22, 32, 12, 30, 35, 9,…\n$ open_credit_lines                <int> 10, 14, 10, 4, 16, 12, 10, 15, 21, 6,…\n$ total_credit_limit               <int> 70795, 28800, 24193, 25400, 69839, 42…\n$ total_credit_utilized            <int> 38767, 4321, 16000, 4997, 52722, 3898…\n$ num_collections_last_12m         <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ num_historical_failed_to_pay     <int> 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0…\n$ months_since_90d_late            <int> 38, NA, 28, NA, NA, 60, NA, 71, 18, N…\n$ current_accounts_delinq          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ total_collection_amount_ever     <int> 1250, 0, 432, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ current_installment_accounts     <int> 2, 0, 1, 1, 1, 0, 2, 2, 6, 1, 2, 1, 2…\n$ accounts_opened_24m              <int> 5, 11, 13, 1, 6, 2, 1, 4, 10, 5, 6, 7…\n$ months_since_last_credit_inquiry <int> 5, 8, 7, 15, 4, 5, 9, 7, 4, 17, 3, 4,…\n$ num_satisfactory_accounts        <int> 10, 14, 10, 4, 16, 12, 10, 15, 21, 6,…\n$ num_accounts_120d_past_due       <int> 0, 0, 0, 0, 0, 0, 0, NA, 0, 0, 0, 0, …\n$ num_accounts_30d_past_due        <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ num_active_debit_accounts        <int> 2, 3, 3, 2, 10, 1, 3, 5, 11, 3, 2, 2,…\n$ total_debit_limit                <int> 11100, 16500, 4300, 19400, 32700, 272…\n$ num_total_cc_accounts            <int> 14, 24, 14, 3, 20, 27, 8, 16, 19, 7, …\n$ num_open_cc_accounts             <int> 8, 14, 8, 3, 15, 12, 7, 12, 14, 5, 8,…\n$ num_cc_carrying_balance          <int> 6, 4, 6, 2, 13, 5, 6, 10, 14, 3, 5, 3…\n$ num_mort_accounts                <int> 1, 0, 0, 0, 0, 3, 2, 7, 2, 0, 2, 3, 3…\n$ account_never_delinq_percent     <dbl> 92.9, 100.0, 93.5, 100.0, 100.0, 78.1…\n$ tax_liens                        <int> 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ public_record_bankrupt           <int> 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0…\n$ loan_purpose                     <fct> moving, debt_consolidation, other, de…\n$ application_type                 <fct> individual, individual, individual, i…\n$ loan_amount                      <int> 28000, 5000, 2000, 21600, 23000, 5000…\n$ term                             <dbl> 60, 36, 36, 36, 36, 36, 60, 60, 36, 3…\n$ interest_rate                    <dbl> 14.07, 12.61, 17.09, 6.72, 14.07, 6.7…\n$ installment                      <dbl> 652.5, 167.5, 71.4, 664.2, 786.9, 153…\n$ grade                            <ord> C, C, D, A, C, A, C, B, C, A, C, B, C…\n$ sub_grade                        <fct> C3, C1, D1, A3, C3, A3, C2, B5, C2, A…\n$ issue_month                      <fct> Mar-2018, Feb-2018, Feb-2018, Jan-201…\n$ loan_status                      <fct> Current, Current, Current, Current, C…\n$ initial_listing_status           <fct> whole, whole, fractional, whole, whol…\n$ disbursement_method              <fct> Cash, Cash, Cash, Cash, Cash, Cash, C…\n$ balance                          <dbl> 27016, 4651, 1825, 18853, 21430, 4257…\n$ paid_total                       <dbl> 1999, 499, 282, 3313, 2325, 873, 2731…\n$ paid_principal                   <dbl> 984, 349, 175, 2747, 1570, 743, 1440,…\n$ paid_interest                    <dbl> 1015.2, 150.5, 106.4, 566.1, 754.8, 1…\n$ paid_late_fees                   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\n\nThe data set is stored in the openintro package.\nIn addition to head, structure function, we can also use glimpse() function to see the general picture of the data.\nWe can see that we have 10000 rows, so there are 10000 observations, or loans made in the data set.\nAnd there are 55 columns.\nAnd the name of the data set is loans_full_schema."
  },
  {
    "objectID": "slides/10-viz.html#selected-variables",
    "href": "slides/10-viz.html#selected-variables",
    "title": "Visualizing Data 📈",
    "section": "Selected Variables",
    "text": "Selected Variables\n\nloans <- loans_full_schema |> \n    dplyr::select(loan_amount, \n                  interest_rate, \n                  grade, \n                  homeownership, \n                  debt_to_income)\nglimpse(loans)\n\nRows: 10,000\nColumns: 5\n$ loan_amount    <int> 28000, 5000, 2000, 21600, 23000, 5000, 24000, 20000, 20…\n$ interest_rate  <dbl> 14.07, 12.61, 17.09, 6.72, 14.07, 6.72, 13.59, 11.99, 1…\n$ grade          <ord> C, C, D, A, C, A, C, B, C, A, C, B, C, B, D, D, D, F, E…\n$ homeownership  <fct> MORTGAGE, RENT, RENT, RENT, RENT, OWN, MORTGAGE, MORTGA…\n$ debt_to_income <dbl> 18.01, 5.04, 21.15, 10.16, 57.96, 6.46, 23.66, 16.19, 3…\n\n\n\nThere are 55 variables in the data, but our visualization purpose, we are only going to focus on 8 of these columns.\nloan_amount, …, debt_to_income ratio\nI piped the data into the select function, which allows me to select variables I want by name, and called the selected data loans.\nSo the data has all 10000 observations and 8 selected variables."
  },
  {
    "objectID": "slides/10-viz.html#variable-description",
    "href": "slides/10-viz.html#variable-description",
    "title": "Visualizing Data 📈",
    "section": "Variable Description",
    "text": "Variable Description\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\nloan_amount\nAmount of the loan received, in US dollars\n\n\ninterest_rate\nInterest rate on the loan, in an annual percentage\n\n\ngrade\nLoan grade, which takes a values A through G and represents the quality of the loan\n\n\nhomeownership\nIndicates whether the person owns, owns but has a mortgage, or rents\n\n\ndebt_to_income\nDebt-to-income ratio"
  },
  {
    "objectID": "slides/10-viz.html#variable-types",
    "href": "slides/10-viz.html#variable-types",
    "title": "Visualizing Data 📈",
    "section": "Variable Types",
    "text": "Variable Types\n\n\n\nvariable\ntype\n\n\n\nloan_amount\nnumerical, continuous\n\n\ninterest_rate\nnumerical, continuous\n\n\ngrade\ncategorical, ordinal\n\n\nhomeownership\ncategorical, nominal\n\n\ndebt_to_income\nnumerical, continuous"
  },
  {
    "objectID": "slides/10-viz.html#bar-chart",
    "href": "slides/10-viz.html#bar-chart",
    "title": "Visualizing Data 📈",
    "section": "Bar Chart",
    "text": "Bar Chart\n\nA bar chart shows the frequency table of a categorical variable.\n\n\n## geom_bar(stat = \"count\")\nloans |> ggplot(aes(x = homeownership)) +\n    geom_bar()  #<<\n\n\n\nOne way representing categorical data is using bar plot.\nHere we don’t use geom_point anymore. Instead, we use the geom_bar function.\nHere we would like to see the frequency distribution of variable homeownership, so I map x to the homeownership variable.\nWhen the geom_bar() is called. ggplot2 actually counts the number of each category for us. In other words, ggplots automatically creates the frequency table of homeownership for us, telling us how many observations belong to mortgage, how many belong to own and how many belong to rent.\nSo the question is, Where does count come from? How does ggplot2 do the calculation?\nNote that on the y-axis, it displays count, but count is NOT a variable in loans!\nWhere does count come from?"
  },
  {
    "objectID": "slides/10-viz.html#stacked-bar-chart",
    "href": "slides/10-viz.html#stacked-bar-chart",
    "title": "Visualizing Data 📈",
    "section": "Stacked Bar Chart",
    "text": "Stacked Bar Chart\n\n\n\n(freq_tbl <- \n    as.data.frame(\n        table(loans$homeownership)\n        ))\n\n      Var1 Freq\n1             0\n2      ANY    0\n3 MORTGAGE 4789\n4      OWN 1353\n5     RENT 3858\n\n## remove count 0 item\nfreq_tbl <- freq_tbl[-c(1, 2), ]\n\n## column names\nnames(freq_tbl) <- c(\"type\",\"count\")\nfreq_tbl\n\n      type count\n3 MORTGAGE  4789\n4      OWN  1353\n5     RENT  3858\n\n\n\n\n\n\n\n\nbar <- freq_tbl |> \n    ggplot(aes(x = \"\", \n               y = count, \n               fill = type)) + \n    geom_bar(stat = \"identity\")\nbar\n\n\n\n\n\n\n\n\n\n\n\nCheck the proportion or percentage of each category, we may want to create a stacked bar chart.\nTo create such plot, we need to prepare a frequency table.\nx = “” because we don’t map any variable in the frequency table data.\nfill = type because we are gonna fill the bar with colors by the type of homeownership.\nstat = “identity” because here we don’t need to calculate the counts of each category, “identity” means using the values provided in the data.\ngeom_bar() uses stat_count() by default: it counts the number of cases at each x position. geom_col() uses stat_identity(): it leaves the data as is.\n\nbar + theme_minimal() + labs(x = ““) ggplot(freq_tbl, aes(x =”“, y = count, color = type)) + geom_bar(stat =”identity”)"
  },
  {
    "objectID": "slides/10-viz.html#pie-chart",
    "href": "slides/10-viz.html#pie-chart",
    "title": "Visualizing Data 📈",
    "section": "Pie Chart",
    "text": "Pie Chart\n\n\n\npie <- bar + \n    coord_polar(theta = \"y\")\npie\n\n\n\n\n\n\n\n\n\npie + theme_void()\n\n\n\n\n\n\n\n\n\ntheta = variable to map angle to (x or y) Offset of starting point from 12 o’clock in radians. pie + geom_text(aes(label = count), position = position_stack(vjust = 0.5))"
  },
  {
    "objectID": "slides/10-viz.html#segmented-bar-plot-stacked",
    "href": "slides/10-viz.html#segmented-bar-plot-stacked",
    "title": "Visualizing Data 📈",
    "section": "Segmented Bar Plot: Stacked",
    "text": "Segmented Bar Plot: Stacked\n\nloans |> ggplot(aes(x = homeownership, \n                    fill = grade)) + #<<\n    geom_bar()\n\n\n\nWe can use Segmented Bar Plot, which is helpful when you want to represent two variables.\nSo here we fill these bars based on the grade of the loan, which we know ranges from A to G.\nSo each one of theses segments tells us how many of that grade of loan was observed within a particular homeownership."
  },
  {
    "objectID": "slides/10-viz.html#segmented-bar-plot-compare-proportions",
    "href": "slides/10-viz.html#segmented-bar-plot-compare-proportions",
    "title": "Visualizing Data 📈",
    "section": "Segmented Bar Plot: Compare Proportions",
    "text": "Segmented Bar Plot: Compare Proportions\n\n\nposition = \"fill\" makes each set of stacked bars the same height.\n\n\nloans |> ggplot(aes(x = homeownership, fill = grade)) +\n    geom_bar(position = \"fill\") #<<\n\n\n\nWe may want to add another argument in the geom_bar layer, which is position = “fill”.\n\nposition = \"fill\" works like stacking, but makes each set of stacked bars the same height.\nAnd now the y-axis is not count anymore. Now the y limit is from 0 to 1, and each grade segment represents the relative frequency of a particular homeownership.\nWhich bar plot is a more useful representation for visualizing the relationship between homeownership and grade?"
  },
  {
    "objectID": "slides/10-viz.html#segmented-bar-plot-compare-individual-values",
    "href": "slides/10-viz.html#segmented-bar-plot-compare-individual-values",
    "title": "Visualizing Data 📈",
    "section": "Segmented Bar Plot: Compare Individual Values",
    "text": "Segmented Bar Plot: Compare Individual Values\n\n\nposition = \"dodge\" places overlapping objects directly beside one another.\n\n\nloans |> ggplot(aes(x = homeownership, fill = grade)) +\n  geom_bar(position = \"dodge\") #<<\n\n\n\nIf you wanna compare Individual Values, here the individual category of the grade of loan in each ownership type, position = \"dodge\" may be a goo option.\nThe option places the grade of loans directly beside one another.\nThis way, it’s easier to see the counts of grade of loan within each ownership."
  },
  {
    "objectID": "slides/10-viz.html#customizing-bar-plots",
    "href": "slides/10-viz.html#customizing-bar-plots",
    "title": "Visualizing Data 📈",
    "section": "Customizing Bar Plots",
    "text": "Customizing Bar Plots\n\n\n\nloans |> \n  ggplot(\n    aes(x = homeownership,\n        fill = homeownership)\n  ) + \n  geom_bar(\n    color = \"blue\", \n    width = 0.2, \n    alpha = 0.5\n  ) + \n  labs(\n    x = \"Homeownership\", \n    title = \"Homeownership Counts\"\n  ) +\n  geom_text(\n    aes(label = after_stat(count)), \n        stat = 'count', \n        hjust = 3, \n        color = \"red\", \n        size = 5\n  ) + \n  theme_minimal() + \n  coord_flip() ## y = homeownership\n\n\n\n\n\n\n\n\n\n\n\nggplot2::geom_bar()\nggplot2::geom_text()\n\n\n\n\n\nstat is short for statistical transformation.\n\nstat = 'count' uses the stat_count() method to get the count of each homeownership.\nOne reason why ggplot2 is powerful is you can customize your plot with great flexibility.\nWhen you use aesthetic option fill, each bar will be colored according to the variable we assign.\nBecause here x and fill map to the same variable, we have each bar has its own one color.\nWe can specify some settings in geom_bar. color is the color of the edge of the bar, and we can also control the width and transparency of bars.\nWe know labs already.\nIf you wanna add text other then labels to your plot, you can use geom_text() function.\nIf you wanna add the counts on each bar, you map label to the variable ..count..\nWhen you see the the variable or data has ..variable_name.., it means that it is an interval variable created by ggplot2.\nIn order to use this ..count.. internal variable, we have to set the stat transformation = “count.\nAnd we can adjust the text horizontally through [hjust], and we can also specify its color and size as we usually do.\nFinally I use theme_minimal() and I flip the coordinate so that the bars become horizontal. It is the same as y = homeownership"
  },
  {
    "objectID": "slides/10-viz.html#histogram",
    "href": "slides/10-viz.html#histogram",
    "title": "Visualizing Data 📈",
    "section": "Histogram",
    "text": "Histogram\n\nloans |> ggplot(aes(x = loan_amount)) +\n    geom_histogram() #<<\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nOne of the most commonly used ways of visualizing numerical data is with histograms.\nIn ggplot2, we use geom_histogram to create a histogram.\nWhen you use this function, ggplot2 send you a message saying that it uses 30 bins, or the number of classes created from the data, each representing one coloumn in the histogram.\nAnd you can pick better value with binwidth, or the class width."
  },
  {
    "objectID": "slides/10-viz.html#section-2",
    "href": "slides/10-viz.html#section-2",
    "title": "Visualizing Data 📈",
    "section": "",
    "text": "binwidth = 1000\nbinwidth = 5000\nbinwidth = 20000\n\n\n\n\nggplot(loans, aes(x = loan_amount)) +\n    geom_histogram(binwidth = 1000)\n\n\n\n\n\n\n\n\n\n\nggplot(loans, aes(x = loan_amount)) +\n    geom_histogram(binwidth = 5000)\n\n\n\n\n\n\n\n\n\n\nggplot(loans, aes(x = loan_amount)) +\n    geom_histogram(binwidth = 20000)\n\n\n\n\n\n\n\n\n\n\n\nIn this plot I use a binwidth of a thousand dollars, so each one of the bars represents a range of thousand dollars, and the heights of these bars tell me how many loans fall into that range.\nIn the second plot, the binwidth is 500 thousand dollars. So here we can have better sense of the shape of the distribution, comparing to the previous one.\nThe last plot use the binwidth 20000 dollars. And now we lose anything interesting about the shape of the distribution.\nSo picking a binwidth or the number of bins is an art and science. But try 10 to 20 bins may be a good start."
  },
  {
    "objectID": "slides/10-viz.html#customizing-histograms",
    "href": "slides/10-viz.html#customizing-histograms",
    "title": "Visualizing Data 📈",
    "section": "Customizing Histograms",
    "text": "Customizing Histograms\n\n\n\nloans |> \n  ggplot(\n    aes(x = loan_amount)\n  ) +\n  geom_histogram(\n    binwidth = 5000,\n    fill = \"#003366\",  \n    colour = \"#FFCC00\",  \n    alpha = 0.8,  \n    linetype = \"dashed\"\n  ) +  \n  labs(\n    x = \"Loan amount ($)\", \n    y = \"Frequency\", \n    title = \"Lending Club loans\"\n  ) + \n  theme_light()\n\n\n\n\n\n\n\n\n\n\n\nggplot2::geom_histogram()\n\n\n\n\nWe can also customize histograms.\nWhat I did here is that I change the color of these bars with fill argument\nAnd the color argument is for the edge of the bars.\nlinetype tells ggplot that the linetype of the edge of the bars is dashed line.\nAnd again, we can add labels using labs function."
  },
  {
    "objectID": "slides/10-viz.html#fill-with-a-categorical-variable-stack",
    "href": "slides/10-viz.html#fill-with-a-categorical-variable-stack",
    "title": "Visualizing Data 📈",
    "section": "Fill with a Categorical Variable: Stack",
    "text": "Fill with a Categorical Variable: Stack\n\n\n\nloans |> \n  ggplot(\n    aes(x = loan_amount,\n        fill = homeownership) #<<\n  ) +\n  geom_histogram(\n    binwidth = 5000\n  ) +\n  labs(\n    x = \"Loan amount ($)\",\n    y = \"Frequency\"\n  ) +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also fill a histogram with a categorical variable, so it allows us to explore the relationship between the numerical and categorical variables.\nThis is done by adding aesthetic fill. Here basically filling the bars based on home ownership.\nSo we can actually see what’s happening behind each level"
  },
  {
    "objectID": "slides/10-viz.html#fill-with-a-categorical-variable-identity-bad",
    "href": "slides/10-viz.html#fill-with-a-categorical-variable-identity-bad",
    "title": "Visualizing Data 📈",
    "section": "Fill with a Categorical Variable: Identity (bad)",
    "text": "Fill with a Categorical Variable: Identity (bad)\n\n\n\nloans |> \n  ggplot(\n    aes(x = loan_amount,\n        fill = homeownership)\n  ) +\n  geom_histogram(\n    binwidth = 5000,\n    position = \"identity\" #<<\n  ) +\n  labs(\n    x = \"Loan amount ($)\",\n    y = \"Frequency\",\n    title = \"Lending Club loans\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy such plot is bad?\n\n\nSo here, if we don’t specify alpha, there is no transparency at all.\nAnd we can see that the histogram of loan amount for house owners is totally masked by the histogram for renters, because there are much more renters than house owners, and the frequency of each bin in the histogram for the renters is higher than the frequency for the owners.\nSo use visualization carefully. Sometimes the plot does not provide more information, and sometimes it is misleading."
  },
  {
    "objectID": "slides/10-viz.html#fill-with-a-categorical-variable-identity",
    "href": "slides/10-viz.html#fill-with-a-categorical-variable-identity",
    "title": "Visualizing Data 📈",
    "section": "Fill with a Categorical Variable: Identity",
    "text": "Fill with a Categorical Variable: Identity\n\n\n\nloans |> \n  ggplot(\n    aes(x = loan_amount,\n        fill = homeownership)\n  ) +\n  geom_histogram(\n    binwidth = 5000,\n    alpha = 0.5,  #<<\n    position = \"identity\"\n  ) +\n  labs(\n    x = \"Loan amount ($)\",\n    y = \"Frequency\",\n    title = \"Lending Club loans\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also compare the histograms of loan amount of different homeownership by simply superimposing on one to another.\nWe use position = “identity” to do that.\nIn this case, we basically put three histograms together in one plot, sharing the same x-axis and y-axis\nIf you wanna use this type of plot, remember to make it transparent, so that each histogram can be seen even they are superimposed."
  },
  {
    "objectID": "slides/10-viz.html#facet-with-a-categorical-variable",
    "href": "slides/10-viz.html#facet-with-a-categorical-variable",
    "title": "Visualizing Data 📈",
    "section": "Facet with a Categorical Variable",
    "text": "Facet with a Categorical Variable\n\n\n\nloans |> \n  ggplot(\n    aes(x = loan_amount,\n        fill = homeownership)\n  ) +\n  geom_histogram(\n    binwidth = 5000\n  ) +\n  labs(\n    x = \"Loan amount ($)\",\n    y = \"Frequency\",\n    title = \"Lending Club loans\"\n  ) +\n  facet_wrap(\n    ~ homeownership, #<<\n    nrow = 3  #<<\n  )       \n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you don’t want to put all three histograms in one single plot, remember you can use faceting trick.\nfirst again I fill in with colors based on homeownership\nThen instead of changing alpha level, and put them on top of each other, I can facet the plot.\nSince I am using a single categorical variable to facet, I am going to use facet_wrap() function by homeownership variable and I want the data to be represented in 3 rows, so that the three histograms share the same x-axis and are compared easily. - Basically, there are not many house owners in the data set. And basically all three distributions are right-skewed."
  },
  {
    "objectID": "slides/10-viz.html#section-3",
    "href": "slides/10-viz.html#section-3",
    "title": "Visualizing Data 📈",
    "section": "",
    "text": "13-Visualization \n\nIn lab.qmd ## Lab 13 section,\n\nImport the data penguins.csv.\nGenerate the following"
  },
  {
    "objectID": "slides/10-viz.html#section-4",
    "href": "slides/10-viz.html#section-4",
    "title": "Visualizing Data 📈",
    "section": "",
    "text": "# library(tidyverse)\npenguins <- read_csv(__________________)\n________ |> ggplot(_______________________) +  ## mapping layer  \n    ___________________ +  ## geometry layer\n    _____________________________  ## label layer\n\n\n\n________ |> ggplot(______________________________) +  ## mapping layer  \n    _______________ +  ## geometry layer\n    _______________ +  ## label layer\n    ______________________________  +   ## facet layer\n    ______________________________      ## theme layer (set legend.position = \"none\")"
  },
  {
    "objectID": "slides/10-viz.html#density-plot",
    "href": "slides/10-viz.html#density-plot",
    "title": "Visualizing Data 📈",
    "section": "Density Plot",
    "text": "Density Plot\n\n\n\n\n\ngeom_density() uses kernel density estimation to smooth the histogram or our data. (MATH 4750 Statistical Computing)\n\n\nggplot(loans, aes(x = loan_amount)) +\n    geom_density()  #<<\n\n\n\nLet’s continue. Another way we can visualize numerical data is using a density plot.\nA density plot is basically a smoothed version of a histogram.\nRemember that a continuous random variable has a continuous probability distribution that is a density curve as a plot. Right?\nSo a normal r.v. has a normal density curve that this.\nBut given a data set of continuous variable, we can only draw a histogram that is sort of an approximation to its density curve. The histogram will only be very much like its true density plot when the data size is huge or approaching to infinity.\nA density plot here smooths the histogram, telling us that given the data set we have, what the true density curve might look like.\nIn ggplot, we have geometry geom_density() to create a density plot.\nAnd ggplot uses the so-called kernel density estimation to smooth the histogram. The kernel is not a the linux kernel in computer science. The kernel here is a probability distribution.\nIf you are interested, it will be taught in 4750 Computational Statistics which is a new course starting this fall.\nYou can see in the graph that the density curve smooths the histogram, but it;s still not very smooth, right. It’s a little bit jagged.\nThe ggplot choose a level of smoothness for us. And we can adjust the smoothness by ourselves."
  },
  {
    "objectID": "slides/10-viz.html#density-plots-and-adjusting-bandwidth",
    "href": "slides/10-viz.html#density-plots-and-adjusting-bandwidth",
    "title": "Visualizing Data 📈",
    "section": "Density Plots and Adjusting Bandwidth",
    "text": "Density Plots and Adjusting Bandwidth\n\n\nadjust = 0.5\nadjust = 1\nadjust = 2\n\n\n\n\nggplot(loans, aes(x = loan_amount)) +\n    geom_density(adjust = 0.5)\n\n\n\n\n\n\n\n\n\n\nggplot(loans, aes(x = loan_amount)) +\n    geom_density(adjust = 1) # default bandwidth\n\n\n\n\n\n\n\n\n\n\nggplot(loans, aes(x = loan_amount)) +\n    geom_density(adjust = 2)\n\n\n\n\n\n\n\n\n\n\n\nWe can adjust the smoothness of a density plot by the argument adjust in geom_density() function.\nbasically, the larger the value is, the smoother the curve will be.\nand the default is adjust = 1.\nKeep in mind that if you use a very smooth density plot, some local peaks or features may be washed out or smoothed away. If these local things contain some information, you may lose the information by just looking at the smooth density plot.\nSo a better idea is to try different level of smoothness, and check different plots, such as histogram, bar plots, and scatter plots as well."
  },
  {
    "objectID": "slides/10-viz.html#customizing-density-plots",
    "href": "slides/10-viz.html#customizing-density-plots",
    "title": "Visualizing Data 📈",
    "section": "Customizing Density Plots",
    "text": "Customizing Density Plots\n\n\n\nloans |> \n  ggplot(\n    aes(x = loan_amount)\n  ) +\n  geom_density(\n    adjust = 2,\n    fill = \"#FFCC00\", \n    color = \"#003366\", \n    alpha = 0.5, \n    linetype = \"dashed\"\n  ) + \n  labs(\n    x = \"Loan amount ($)\", \n    y = \"Density\", \n    title = \"Lending Club loans\"\n  ) \n\n\n\n\n\n\n\n\n\n\n\nggplot2::geom_density()\n\n\n\n\nWe can also customize density plots as others.\nIn addition to adjust, we can also use fill, color, alpha, line type as we have used in histogram."
  },
  {
    "objectID": "slides/10-viz.html#density-curve-on-a-histogram",
    "href": "slides/10-viz.html#density-curve-on-a-histogram",
    "title": "Visualizing Data 📈",
    "section": "Density Curve on a Histogram",
    "text": "Density Curve on a Histogram\n\n\n\nloans |> \n  ggplot(\n    aes(x = loan_amount)\n  ) + \n  ## scale down to \n  ## match the density\n  geom_histogram(\n    binwidth = 5000, \n    aes(y = ..density..) #<<\n  ) + \n  geom_density(\n    alpha = 0.1, \n    fill = \"#FF6666\"\n  ) +\n  labs(\n    x = \"Loan amount ($)\",\n    y = \"Density\",\n    title = \"Lending Club loans\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nSometimes we like to put the density on top of the histogram to have a better understanding how smooth the density curve is, comparing to the histogram.\nWell it’s very easy to do that.\nWe just need to add the geom_density layer to the existent geom_histogram layer.\nAnd don’t forget to use density scale in geom_histogram so that the height of histogram and density plot can share the same y-axis each other."
  },
  {
    "objectID": "slides/10-viz.html#adding-a-categorical-variable-to-density-plots",
    "href": "slides/10-viz.html#adding-a-categorical-variable-to-density-plots",
    "title": "Visualizing Data 📈",
    "section": "Adding a Categorical Variable to Density Plots",
    "text": "Adding a Categorical Variable to Density Plots\n\n\n\nloans |> \n  ggplot(\n    aes(x = loan_amount,\n        fill = homeownership)\n  ) +\n  geom_density(\n    adjust = 2,\n    alpha = 0.4\n  ) +\n  labs(\n    x = \"Loan amount ($)\",\n    y = \"Density\",\n    title = \"Amounts of loans\",\n    fill = \"Homeownership\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also add a categorical variable to a density plot. This is similar to what we did with the histograms, where you can see them overlaid on top of each other.\nSo once again, we can fill in the density curves based on the value of homeownership.\nand I am changing alpha, so that we can actually see what’s happening behind the curve. If I didn’t change alpha, we are not able to see the peaks of mortagage and own because they are masked by the density curve of home renters. OK."
  },
  {
    "objectID": "slides/10-viz.html#box-plot-1",
    "href": "slides/10-viz.html#box-plot-1",
    "title": "Visualizing Data 📈",
    "section": "Box Plot",
    "text": "Box Plot\n\nloans |> ggplot(aes(x = interest_rate)) +\n  geom_boxplot() +\n  labs(x = \"interest rate (%)\")\n\n\n\nRemember in base R, we can use boxplot function to create a boxplot.\nIn ggplot, we add the geometry layer, geom_boxplot().\nOne of things that boxplots are good at is uncovering our potential outliers, and here, the distribution of interest rates is right-skewed, and has several outliers.\nMost of the interest rates are between 10 to 15%, but some can be as high as 30% or more."
  },
  {
    "objectID": "slides/10-viz.html#customizing-box-plots",
    "href": "slides/10-viz.html#customizing-box-plots",
    "title": "Visualizing Data 📈",
    "section": "Customizing Box Plots",
    "text": "Customizing Box Plots\n\n\n\nloans |> \n  ggplot(aes(x = interest_rate)) +\n  geom_boxplot(\n    # custom outliers\n    outlier.colour = \"red\",\n    outlier.shape = 8,\n    outlier.size = 3,\n    # Notch?\n    notch = TRUE,\n    notchwidth = 0.1,\n    # custom boxes\n    fill = \"#FFCC00\",\n    colour = \"#003366\",\n    alpha = 0.2\n  ) +\n  labs(\n    x = \"Interest rate (%)\",\n    title = \"Interest rates of loans\"\n  ) +\n  theme(\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n\n\nggplot2::geom_boxplot()\n\n\n\n\nHow do we customize our boxplot?\nFor a boxplot, we can change outlier’s color, shape and size, using outlier.colour, outlier.shape and outlier.size.\nI remembered I show you point shape and its corresponding number when I introduced base R plotting. You can use the same point shape numbering system here.\nYou can also decide if the boxplot has a notch, although I don’t see the importance of this option.\nAnd again you can customize the box using fill, color, and alpha argument.\nAnd one interesting part is that, when we show a boxplot like this, the y-axis does not mean anything right? the width or height of the box does not meaning anything.\nSo we can actually remove the y axis tick marks and labels.\nAnd how? Because the axes labels and ticks are part of plotting theme, we should go to theme and set ticks.y and text.y = element_blank().\nThis is very ggplot2 syntax. It is a little bit harder to read and make sense of, so it does take time to get used to it."
  },
  {
    "objectID": "slides/10-viz.html#adding-a-categorical-variable-to-box-plots",
    "href": "slides/10-viz.html#adding-a-categorical-variable-to-box-plots",
    "title": "Visualizing Data 📈",
    "section": "Adding a Categorical Variable to Box Plots",
    "text": "Adding a Categorical Variable to Box Plots\n\n\n\nloans |> \n  ggplot(\n    aes(x = interest_rate,\n        y = grade) #<<\n  ) + \n  geom_boxplot() +\n  labs(\n    x = \"Interest rate (%)\",\n    y = \"Grade\",\n    title = \"Interest Rates of Loans\",\n    subtitle = \"by grade of loan\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also add categorical variable to a boxplot, so these are called side-by-side boxplots. They are very commonly used.\nThis is the interest rate distribution by the grade of loan, where A is of the highest quality and G is the lowest quality.\nAnd it looks like the higher the quality of the loan, the lower the interest rate it has.\nWe can also see that there is an outlier in grade D data, where the interest rate is super low, even lower than most of the interest rate of the grade A loans. You as a reasearcher or data analyst have to figure out if it is typo, or it is the true interest rate. If it is a correct number, why such low interest rate happened?\nAnd to create such side-by-side boxplot, we just add y mapping, setting it to grade."
  },
  {
    "objectID": "slides/10-viz.html#adding-two-categorical-variables-to-box-plots",
    "href": "slides/10-viz.html#adding-two-categorical-variables-to-box-plots",
    "title": "Visualizing Data 📈",
    "section": "Adding Two Categorical Variables to Box Plots",
    "text": "Adding Two Categorical Variables to Box Plots\n\n\n\nloans |> \n  ggplot(\n    aes(x = interest_rate,\n        y = grade,\n        fill = homeownership) #<<\n  ) +\n  geom_boxplot() +\n  labs(\n    x = \"Interest rate (%)\",\n    y = \"Grade\",\n    fill = \"Homeowership\",\n    title = \"Interest Rates of Loans\",\n    subtitle = \"by grade and ownership\"\n  ) +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf we want to add two categorical variables to box plots, that’s easy.\nWe can fill the boxplots with colors based on another variable, here the homeownership.\nANd now each grade of loans is furthered divided into three categories of the homeownership variable.\nAnd we can see that basically, the type of homeownership does not affect interest rate much given the same grade of loans."
  },
  {
    "objectID": "slides/10-viz.html#adding-points-on-box-plots",
    "href": "slides/10-viz.html#adding-points-on-box-plots",
    "title": "Visualizing Data 📈",
    "section": "Adding Points on Box Plots",
    "text": "Adding Points on Box Plots\n\nggplot(loans, aes(x = homeownership, y = interest_rate)) +\n    geom_boxplot() +\n    geom_point(alpha = 0.1, shape = 1) #<< geom_jitter(width = 0.2, alpha = 0.1, shape = 1)\n\n\n\nIf we don’t just want the boxplot, and we also want to show data points along with the boxplot, we can add the another layer geom_point(), as shown here.\nBut this plot is a little misleading. Anybody see why?\nThe boxplot tell us 50% of the data points should be inside the box, right?\nBut according to the boxplot, it is not the case. Why?\nActually, there are lots points overlapped at the same interest rate. So it looks like there is only one data point, but actually there are many data points right there. They just overlapped each other.\nSo how do we actually show all data points?"
  },
  {
    "objectID": "slides/10-viz.html#scatterplot",
    "href": "slides/10-viz.html#scatterplot",
    "title": "Visualizing Data 📈",
    "section": "Scatterplot",
    "text": "Scatterplot\n\nggplot(loans, aes(x = debt_to_income, y = interest_rate)) +\n    geom_point(shape = 23, fill = \"blue\", size = 0.8)  #<<\n\n\n\nWe already talked about scatter plot using geom_point().\nHere we have a scatter plot of interest rate and the debt to income ratio.\nOne potential problem here is that like adding points to boxplot, we have so many data points overlapped together, and their relationship cannot be truly revealed. and so it’s really difficult to make sense of this scatter plot built with geom_point."
  },
  {
    "objectID": "slides/10-viz.html#hex-plot",
    "href": "slides/10-viz.html#hex-plot",
    "title": "Visualizing Data 📈",
    "section": "Hex Plot",
    "text": "Hex Plot\n\nggplot(loans, aes(x = debt_to_income, y = interest_rate)) +\n    geom_hex()  #<< (hexbin pkg)\n\n\n\nSo one solution is using Hex Plot instead by using geom_hex() function.\nThe idea is that we first bin these data with the hex shape, grouping data points together for each hex bin.\nAnd the color of these hexes is what tells us how many data points or how dense my data are.\nSo the lighter the color, the more data points we have there.\nThis is particularly useful when we have lots of data points that have similar or the same values."
  },
  {
    "objectID": "slides/10-viz.html#hex-plot-zoom-in",
    "href": "slides/10-viz.html#hex-plot-zoom-in",
    "title": "Visualizing Data 📈",
    "section": "Hex Plot Zoom-in",
    "text": "Hex Plot Zoom-in\n\nloans |> \n    filter(debt_to_income < 100) |> \n    ggplot(aes(x = debt_to_income, y = interest_rate)) +\n    geom_hex() +\n    viridis::scale_fill_viridis()\n\n\n\nHere just show you that we can further zoom in the plot, focusing on the the data that has debt_to_income ratio less that 100%.\nAnd now we can more clearly see what’s going on.\nBasically, more data points happen around debt_to_income ratio 12 to 15% and interest rate 10%.\nAnd the two variables are a little bit positively correlated that higher debt_to_income ratio corresponds to higher interest rate. But again the correlation is very weak here."
  },
  {
    "objectID": "slides/10-viz.html#line-chart-for-time-series-ggplot2geom_line",
    "href": "slides/10-viz.html#line-chart-for-time-series-ggplot2geom_line",
    "title": "Visualizing Data 📈",
    "section": "Line Chart for Time Series ggplot2::geom_line()\n",
    "text": "Line Chart for Time Series ggplot2::geom_line()\n\n\neconomics_long |> ggplot(aes(x = date, y = value01, \n                             colour = variable, linetype = variable)) +\n    geom_line(linewidth = 1) + \n    theme_bw() +\n    theme(legend.position = \"bottom\")\n\n\n\nIf you have time series data, line plot is your good friend, which shows the trend of some variable.\nFor example here, the unemployment number."
  },
  {
    "objectID": "slides/10-viz.html#qq-plots",
    "href": "slides/10-viz.html#qq-plots",
    "title": "Visualizing Data 📈",
    "section": "QQ-plots",
    "text": "QQ-plots\n\nQuantile-Quantile plots are used to check if data are normally distributed (or follow any distribution).\n\n\nggplot(mpg, aes(sample = hwy)) + geom_qq() + geom_qq_line()\n\n\n\nTo create a qqplot, we add the geometry geom_qq(), and geom_qq_line() if you want to add a straight line that helps you determine whether the data are normally distributed."
  },
  {
    "objectID": "slides/10-viz.html#violin-plots",
    "href": "slides/10-viz.html#violin-plots",
    "title": "Visualizing Data 📈",
    "section": "Violin Plots",
    "text": "Violin Plots\n\nViolin plots are similar to box plots, but show the smooth density of the data.\n\n\nf <- ggplot(loans, aes(x = loan_amount, y = grade))\n\n\n\n\nf + geom_boxplot()\n\n\n\n\n\n\n\n\n\nf + geom_violin() \n\n\n\n\n\n\n\n\n\n\nIn ggplot, we can use geom_violin() to create one. \n\nSee if we just check the boxplot, we will miss the information that the loan amount of the grade G actually has a bimodal distribution, right?"
  },
  {
    "objectID": "slides/10-viz.html#add-on-ggridges-for-ridge-plots",
    "href": "slides/10-viz.html#add-on-ggridges-for-ridge-plots",
    "title": "Visualizing Data 📈",
    "section": "Add-on 📦: ggridges for Ridge Plots",
    "text": "Add-on 📦: ggridges for Ridge Plots\n\nlibrary(ggridges)\nggplot(loans, aes(x = loan_amount, y = grade, fill = grade, color = grade)) +\n    geom_density_ridges(alpha = 0.9)\n\n\n\nTo show better smooth density plots, we can create the so-called ridge plots provided by the add-on package called ggridges.\neverything is basically the same. You just need to install and load the package, and add the geometry called geom_density_ridges()."
  },
  {
    "objectID": "slides/10-viz.html#add-on-ggrepel",
    "href": "slides/10-viz.html#add-on-ggrepel",
    "title": "Visualizing Data 📈",
    "section": "Add-on 📦: ggrepel \n",
    "text": "Add-on 📦: ggrepel \n\n\n\nggrepel provides geoms for ggplot2 to repel overlapping text labels.\n\n\nlibrary(ggrepel)\np <- mtcars |> filter(wt > 2.75 & wt < 3.45) |> rownames_to_column(\"car\") |> \n    ggplot(aes(wt, mpg, label = car)) +\n    geom_point(color = \"red\")\n\n\n\n\np + geom_text() + \n    labs(title = \"geom_text()\")\n\n\n\n\n\n\n\n\n\np + geom_text_repel() + \n    labs(title = \"geom_text_repel()\")\n\n\n\n\n\n\n\n\n\n\nAnother useful ggplot2 extension is ggrepel that provides geoms for ggplot2 to repel overlapping text labels.\nThis makes your labels much more clear.\nFor example, here I want to label each data point by its observation’s name, I can add geom_text(). But the ggplot2 setting basically put the text right on the top of the point, which sometimes the labels are mixed together and makes it hard to read.\nAnd we can use the function geom_text_repel(), and then it will automatically find a good spot for those texts that are close to their data point, but not overlap each other."
  },
  {
    "objectID": "slides/10-viz.html#wordcloud-from-ggwordcloud-package",
    "href": "slides/10-viz.html#wordcloud-from-ggwordcloud-package",
    "title": "Visualizing Data 📈",
    "section": "Wordcloud from ggwordcloud Package \n",
    "text": "Wordcloud from ggwordcloud Package \n\n\n\nPlot\nCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggwordcloud)\nlibrary(showtext)\nwhere <- font_files()[which(str_detect(font_files()$family, \"Arial Unicode MS\")), ]\nthankyou_words_small |> ggplot(aes(label = word, size = speakers, color = name)) + \n    geom_text_wordcloud(area_corr = TRUE, rm_outside = TRUE, \n                        family = where[1, ]$family) +\n    scale_size_area(max_size = 24) + \n    theme_minimal() +\n    theme(plot.margin = margin(t = 0,  # Top margin\n                               r = 0,  # Right margin\n                               b = 0,  # Bottom margin\n                               l = 0)) # Left margin\n\n\n\n\nhttps://towardsdatascience.com/create-a-word-cloud-with-r-bde3e7422e8a http://www.sthda.com/english/wiki/text-mining-and-word-cloud-fundamentals-in-r-5-simple-steps-you-should-know http://www.sthda.com/english/wiki/word-cloud-generator-in-r-one-killer-function-to-do-everything-you-need https://semba-blog.netlify.app/10/29/2019/creating-wordcloud-in-r/     https://stackoverflow.com/questions/74415534/why-do-characters-from-foreign-alphabets-not-show-in-my-wordcloud-on-r"
  },
  {
    "objectID": "slides/10-viz.html#radar-chart-from-fmsb-and-ggradar-package",
    "href": "slides/10-viz.html#radar-chart-from-fmsb-and-ggradar-package",
    "title": "Visualizing Data 📈",
    "section": "Radar Chart from fmsb and ggradar Package",
    "text": "Radar Chart from fmsb and ggradar Package\n\n\nPlot\nCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(fmsb)\nradar_data <- readr::read_csv(\n    file = \"./data/radar_data.csv\")\n# Color vector\ncolors_border <- c(rgb(0.2,0.5,0.5,0.9), \n                   rgb(0.8,0.2,0.5,0.9), \n                   rgb(0.7,0.5,0.1,0.9))\ncolors_in <- c(rgb(0.2,0.5,0.5,0.4), \n               rgb(0.8,0.2,0.5,0.4), \n               rgb(0.7,0.5,0.1,0.4))\nradarchart(radar_data, axistype = 1, \n           #custom polygon\n           pcol = colors_border, \n           pfcol = colors_in, \n           plwd = 4, plty = 1,\n           #custom the grid\n           cglcol = \"grey\", cglty = 1, \n           axislabcol = \"grey\", \n           caxislabels = seq(0, 20, 5), \n           cglwd = 0.8,\n           #custom labels\n           vlcex = 1.2)\n# legend(\"topright\", legend = rownames(radar_data[-c(1, 2), ]), bty = \"n\", pch = 20 , \n#        col = colors_in, text.col = \"grey\", cex = 1.2, pt.cex = 3)\n\n\n\nlibrary(ggradar)\n\nggradar_data <- radar_data |>\n    as_tibble(rownames = \"group\") |>\n    mutate_at(vars(-group), rescale) |>\n    tail(3)\n\nggradar(ggradar_data,\n        base.size = 5,\n        grid.label.size = 6,\n        axis.label.size = 5,\n        group.point.size = 3,\n        fill.alpha = 0.2,\n        grid.line.width = 0.4,\n        plot.legend = FALSE,\n        fill = TRUE)\n\n\n\n\n\n\nlazy-load database ‘/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/callr/R/callr.rdb’ is corrupt Restart R"
  },
  {
    "objectID": "slides/10-viz.html#network-from-igraph-package",
    "href": "slides/10-viz.html#network-from-igraph-package",
    "title": "Visualizing Data 📈",
    "section": "Network from igraph Package",
    "text": "Network from igraph Package\n\n\nPlot\nCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(igraph)\nnetwork_data <- read_rds(file = \"./data/network_data.rds\")\n\n# build the graph object\nnetwork <- graph_from_adjacency_matrix(network_data)\n \n# plot it\nplot(network)"
  },
  {
    "objectID": "slides/10-viz.html#more-r-graphics-resources",
    "href": "slides/10-viz.html#more-r-graphics-resources",
    "title": "Visualizing Data 📈",
    "section": "More R Graphics Resources",
    "text": "More R Graphics Resources\n\nMore add-on 📦: ggplot2 extensions\n\nThe R Graph Gallery\nR Graphics Cookbook\nR CHARTS\n\n\n\n\n\n\n\n\n\n\nAs I mentioned before, there are tons of ggplot2 extensions.\nIf you are interested, check those links, and learn more about them. OK.\n\n\n\nmath3570-s24.github.io/website"
  },
  {
    "objectID": "slides/16-linear-reg.html#what-is-regression",
    "href": "slides/16-linear-reg.html#what-is-regression",
    "title": "Linear Regression \n",
    "section": "What is Regression",
    "text": "What is Regression\n\nRegression models the relationship between a numerical response variable \\((Y)\\) and one or more numerical/categorical predictors \\((X)\\), which is a supervised learning method in machine learning.\nA regression function \\(f(X)\\) describes how a response variable \\(Y\\) generally changes as an explanatory variable \\(X\\) changes.\n\n\n\nExamples:\n\n college GPA \\((Y)\\) vs. ACT/SAT score \\((X)\\)\n sales \\((Y)\\) vs. advertising expenditure \\((X)\\)\n crime rate \\((Y)\\) vs. median income level \\((X)\\)"
  },
  {
    "objectID": "slides/16-linear-reg.html#simple-linear-regression",
    "href": "slides/16-linear-reg.html#simple-linear-regression",
    "title": "Linear Regression \n",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\n\n\n\\[\\begin{align*}\ny_i &= f(x_i) +  \\epsilon_i \\\\\n    &= \\beta_0 + \\beta_1~x_{i} + \\epsilon_i, \\quad i = 1, 2, \\dots, n\n\\end{align*}\\]\n\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are unknown parameters to be learned or estimated.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat are the assumption on \\(\\epsilon_i\\)?\n\n\n\\(\\epsilon_i \\sim N(0, \\sigma^2)\\) and hence \\(y_i \\mid x_i \\sim N(\\beta_0+\\beta_1x_i, \\sigma^2)\\) or \\(\\mu_{y\\mid x_i} = \\beta_0+\\beta_1x_i\\).\n\nOK we start with the simple linear regression, the linear regression a Single Predictor \\(x\\).\nIn linear regression, our regression function \\(f\\) is a linear function \\(\\beta_0 + \\beta_1~x\\).\nEpsilon, the random error is there to capture any random measurement errors or any variations in \\(y\\) that cannot be explained by the predictor \\(x\\).\nGiven this model, we’re interested in \\(\\beta_0\\) (population parameter for the intercept) and \\(\\beta_1\\) (population parameter for the slope) because once we know \\(\\beta_0\\) and \\(\\beta_1\\), we know the exact shape of \\(f\\) and we know the relationship of \\(y\\) and \\(x\\), and given any value of \\(x\\), we can predict its corresponding value of \\(y\\) using the regression line \\(\\hat{y}_{i} = \\beta_0 + \\beta_1~x_{i}\\).\nBut unfortunately, the population parameters are typically unknown to us."
  },
  {
    "objectID": "slides/16-linear-reg.html#simple-linear-regression-assumptions",
    "href": "slides/16-linear-reg.html#simple-linear-regression-assumptions",
    "title": "Linear Regression \n",
    "section": "Simple Linear Regression Assumptions",
    "text": "Simple Linear Regression Assumptions"
  },
  {
    "objectID": "slides/16-linear-reg.html#simple-linear-regression-assumptions-1",
    "href": "slides/16-linear-reg.html#simple-linear-regression-assumptions-1",
    "title": "Linear Regression \n",
    "section": "Simple Linear Regression Assumptions",
    "text": "Simple Linear Regression Assumptions"
  },
  {
    "objectID": "slides/16-linear-reg.html#simple-linear-regression-assumptions-2",
    "href": "slides/16-linear-reg.html#simple-linear-regression-assumptions-2",
    "title": "Linear Regression \n",
    "section": "Simple Linear Regression Assumptions",
    "text": "Simple Linear Regression Assumptions"
  },
  {
    "objectID": "slides/16-linear-reg.html#ordinary-least-squares-ols",
    "href": "slides/16-linear-reg.html#ordinary-least-squares-ols",
    "title": "Linear Regression \n",
    "section": "Ordinary Least Squares (OLS)",
    "text": "Ordinary Least Squares (OLS)\nGiven the training data \\((x_1, y_1), \\dots, (x_n, y_n)\\), use sample statistics \\(b_0\\) and \\(b_1\\) computed from the training data to\n\ninference: estimate \\(\\beta_0\\) and \\(\\beta_1\\)\nfitting: estimate \\(y_i\\) or \\(f(x_i)\\) at \\(x_i\\) by its fitted value \\[\\hat{y}_{i} = \\hat{f}(x_i) = b_0 + b_1~x_{i}\\]\nprediction: predict \\(y_j\\) or \\(f(x_j)\\) at \\(x_j\\) by its predicted value \\[\\hat{y}_{j} = \\hat{f}(x_j) = b_0 + b_1~x_{j}\\] where \\((x_j, y_j)\\) is never seen and used in training before.\n\n\n\n\nOrdinary Least Squares: We find \\(b_0\\) and \\(b_1\\), or regression line \\(b_0 + b_1x\\) that minimizes the sum of squared residuals.\nThe residual \\(e_i = y_i - \\hat{y}_i\\). The sample regression line minimizes \\(\\sum_{i = 1}^n e_i^2\\).\n\n\nHow do we get \\(b_0\\) and \\(b_1\\) that sort of well estimate \\(\\beta_0\\) and \\(\\beta_1\\)?\nWe choose \\(b_0\\) and \\(b_1\\), or regression line \\(b_0 + b_1x\\) that minimizes the sum of squared residuals.\nIf we define residual as \\(e_i = y_i - \\hat{y}_i\\), then the sum of squared residuals is \\(\\sum_{i = 1}^n e_i^2\\).\nAnd this approach that estimates the population parameters \\(\\beta_0\\) and \\(\\beta_1\\) or the population regression line is called Ordinary Least Squares method."
  },
  {
    "objectID": "slides/16-linear-reg.html#visualizing-residuals",
    "href": "slides/16-linear-reg.html#visualizing-residuals",
    "title": "Linear Regression \n",
    "section": "Visualizing Residuals",
    "text": "Visualizing Residuals\n\nOK. That’s see the idea of Ordinary Least Squares visually. Here just showed the data. - Do you see why some points are darker than some others? - A darker point means that there are several identical (x, y) pairs, or replicates in the data set."
  },
  {
    "objectID": "slides/16-linear-reg.html#visualizing-residuals-cont.",
    "href": "slides/16-linear-reg.html#visualizing-residuals-cont.",
    "title": "Linear Regression \n",
    "section": "Visualizing Residuals (cont.)",
    "text": "Visualizing Residuals (cont.)\n\n\nAll right, with the data, this figure also shows the least squares regression line, and the fitted (predicted) value of \\(y\\) for each \\(x\\) in the training data, which are those red points.\nThe predicted values of y are right on the regression line.\nNow the question is, how do we find this line?\nGiven a line, we can have predicted values of y, right?\nThen what is residual on the plot? The residual will be the difference between the true observation y and the predicted value of y given any value of x.\nSo a residual in the plot will be a vertical bar at the value of x with two ends of the bar \\(y\\) and \\(\\hat{y}\\), right?\n(Show on board)\n(add \\(y_i = b_0+b_1x_i\\) and residual line)"
  },
  {
    "objectID": "slides/16-linear-reg.html#visualizing-residuals-cont.-1",
    "href": "slides/16-linear-reg.html#visualizing-residuals-cont.-1",
    "title": "Linear Regression \n",
    "section": "Visualizing Residuals (cont.)",
    "text": "Visualizing Residuals (cont.)\n\n\nHere shows all the residuals in vertical bars.\nleast squares line is the line such that the sum of all the squared residuals is minimized.\nWhy we square the residuals?\nIt’s mathematically more convenient.\nSquaring emphasizes larger differences"
  },
  {
    "objectID": "slides/16-linear-reg.html#predict-highway-mpg-hwy-from-displacement-displ",
    "href": "slides/16-linear-reg.html#predict-highway-mpg-hwy-from-displacement-displ",
    "title": "Linear Regression \n",
    "section": "Predict Highway MPG hwy from Displacement displ\n",
    "text": "Predict Highway MPG hwy from Displacement displ\n\n\\[\\widehat{hwy}_{i} = b_0 + b_1 \\times displ_{i}\\]\n\n\nThe data I just show you is the mpg data set in ggplot2.\nHere we are trying to Predict Highway MPG hwy from Engine Displacement displ"
  },
  {
    "objectID": "slides/16-linear-reg.html#step-1-specify-model-linear_reg",
    "href": "slides/16-linear-reg.html#step-1-specify-model-linear_reg",
    "title": "Linear Regression \n",
    "section": "Step 1: Specify Model: linear_reg()\n",
    "text": "Step 1: Specify Model: linear_reg()\n\n\nlibrary(tidymodels)\nparsnip::linear_reg()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nparsnip package provides a tidy, unified interface for fitting models\n\nOne challenge with different modeling functions available in R that do the same thing is that they can have different interfaces and arguments. Note that the model syntax can be very different and that the argument names (and formats) are also different. This is a pain if you switch between implementations. - So how do we use tidymodels to actually fit a model. - Step 1 is to specify the model that we are building. - We are going to build a linear regression model, so the function we start with is linear_reg() - And you can see that the output is saying OK I’m ready to use this model specification."
  },
  {
    "objectID": "slides/16-linear-reg.html#step-2-set-model-fitting-engine",
    "href": "slides/16-linear-reg.html#step-2-set-model-fitting-engine",
    "title": "Linear Regression \n",
    "section": "Step 2: Set Model Fitting Engine\n",
    "text": "Step 2: Set Model Fitting Engine\n\n\nUse lm() in the built-in stats package.\n\n\nlinear_reg() |> \n    set_engine(\"lm\")\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\n\nshow_engines(\"linear_reg\")\n\n# A tibble: 7 × 2\n  engine mode      \n  <chr>  <chr>     \n1 lm     regression\n2 glm    regression\n3 glmnet regression\n4 stan   regression\n5 spark  regression\n6 keras  regression\n7 brulee regression\n\n\n\nStep 2 is to define the model fitting engine.\nFor now we are going to use “lm” that is a function in the stats package to fit a linear model.\nBasically, we ask R or tidymodel to use the lm() function as our computational engine to fit a linear regression model.\nSo you can think tidymodel as an platform or interface with many other R packages that do the work of model fitting, and tidymodel provides a consistent interface to them.\nSo you can actually use different R packages as different computational engines to fit the same model.\nWith tidymodel, you don’t need to worry about different syntax should be used in different packages because the syntax is the same, and its outputs also remain the same even different computational engine is used."
  },
  {
    "objectID": "slides/16-linear-reg.html#step-3-fit-model-estimate-parameters",
    "href": "slides/16-linear-reg.html#step-3-fit-model-estimate-parameters",
    "title": "Linear Regression \n",
    "section": "Step 3: Fit Model & Estimate Parameters",
    "text": "Step 3: Fit Model & Estimate Parameters\n… using formula syntax\n\nlinear_reg() |> \n    set_engine(\"lm\") |> \n    fit(hwy ~ displ, data = mpg)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = hwy ~ displ, data = data)\n\nCoefficients:\n(Intercept)        displ  \n      35.70        -3.53  \n\n\n\\[\\widehat{hwy}_{i} = 35.7  -3.53 \\times displ_{i}\\]\n\n\nSlope: When the engine displacement volume of a car is increased by one litre, the highway miles per gallon is expected to be lower, on average, by 3.53 miles.\n\n\nStep 3, after selecting the computational engine, we fit the model and estimate parameters.\nwe use the fit() function, and inside the function, we provide the model fitting formula, response variable ~ predictors, followed by the data set’s name.\nHere our response is hwy, and the predictor is displ, and the data set is mpg.\nAfter fitting the model, we can the model output. We can see our estimated coefficients intercept and slope for displ, which are \\(b_0\\) and \\(b_1\\) in previous slides.\nAnd with the coefficients, we now can have the regression line, \\(\\widehat{hwy}_{i} = 35.698 -3.531 \\times displ_{i}\\), where \\(\\widehat{hwy}_{i}\\) is the predicted value of hwy MPG of the \\(i\\)-th observation."
  },
  {
    "objectID": "slides/16-linear-reg.html#tidy-look-at-model-output",
    "href": "slides/16-linear-reg.html#tidy-look-at-model-output",
    "title": "Linear Regression \n",
    "section": "Tidy Look at Model Output",
    "text": "Tidy Look at Model Output\n\nTidymodels output (tibble)\n\n\nlinear_reg() |> \n    set_engine(\"lm\") |> \n    fit(hwy ~ displ, data = mpg) |> \n    tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)    35.7      0.720      49.6 2.12e-125\n2 displ          -3.53     0.195     -18.2 2.04e- 46\n\n\n\\[\\widehat{hwy}_{i} = 35.7  -3.53 \\times displ_{i}\\]\n\nWe can print the model output as a tidy table using the tidy() function.\nHere we not only have the point estimates in the fist column, but also have the standard error that measure the uncertainty of these coefficients, the test statistics, and p-values that are used in hypothesis testing.\nWe’ll use standard error when quantifying the uncertainty about the regression line, but we won’t talk about test statistics and p-values in this course because they are covered in 4720."
  },
  {
    "objectID": "slides/16-linear-reg.html#section-3",
    "href": "slides/16-linear-reg.html#section-3",
    "title": "Linear Regression \n",
    "section": "",
    "text": "20-Simple Linear Regression \nIn lab.qmd ## Lab 20 section,\n\nUse the mpg data to fit a simple linear regression where \\(y\\) is hwy and \\(x\\) is cty.\nProduce the plot below. (add the layer geom_smooth(method = \"lm\", se = FALSE))\n\n\nlibrary(tidymodels); library(ggplot2)\n\n\n\n\n\n\n\n\n\nparsnip::linear_reg() %>% \n    set_engine(\"lm\") %>% \n    fit(hwy ~ displ, data = mpg) %>% \n    tidy()\n\n\nlibrary(tidymodels); library(ggplot2)\nggplot(data = mpg, aes(x = cty, y = hwy)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"#003366\") + \n    labs(title = \"Highway MPG vs. City MPG\",\n         x = \"City MPG\",\n         y = \"Highway MPG\")"
  },
  {
    "objectID": "slides/16-linear-reg.html#quantify-uncertainty-about-coefficients",
    "href": "slides/16-linear-reg.html#quantify-uncertainty-about-coefficients",
    "title": "Linear Regression \n",
    "section": "Quantify Uncertainty about Coefficients",
    "text": "Quantify Uncertainty about Coefficients\n\nUncertainty about regression coefficients \\(\\beta_0\\) and \\(\\beta_1\\)\n\n\n\ntidymdl <- linear_reg() |> \n    set_engine(\"lm\")\n\nreg_out <- tidymdl |> \n    fit(hwy ~ displ, data = mpg)\n\nreg_out$fit$coefficients\n\n(Intercept)       displ \n      35.70       -3.53 \n\n\n\nconfint(reg_out$fit)\n\n            2.5 % 97.5 %\n(Intercept) 34.28  37.12\ndispl       -3.91  -3.15"
  },
  {
    "objectID": "slides/16-linear-reg.html#quantify-uncertainty-about-mean-of-y",
    "href": "slides/16-linear-reg.html#quantify-uncertainty-about-mean-of-y",
    "title": "Linear Regression \n",
    "section": "Quantify Uncertainty about Mean of \\(y\\)\n",
    "text": "Quantify Uncertainty about Mean of \\(y\\)\n\n\n\n\nUncertainty about the mean value of \\(y\\) given \\(X = x\\) \\[\\mu_{Y \\mid X = x} = \\beta_0 + \\beta_1x\\]\n\n\n\nnew_input <- \n    data.frame(displ = 3:6)\n\npredict(reg_out$fit,\n        newdata = new_input,\n        interval = \"confidence\",\n        level = 0.95)\n\n   fit  lwr  upr\n1 25.1 24.6 25.6\n2 21.6 21.0 22.1\n3 18.0 17.3 18.8\n4 14.5 13.4 15.6\n\n\n\n\n\nFit with uncertainty band\nggplot object p\n\n\n\n\n(p_ci <- p + geom_smooth(method = \"lm\", \n                         color = \"#003366\", \n                         fill = \"blue\",\n                         se = TRUE))\n\n\n\n\n\n\n\n\n\n\np <- ggplot(data = reg_out$fit, \n            aes(x = displ, y = hwy)) +\n     geom_point(alpha = 0.3) + \n     labs(title = \"Highway MPG vs. Engine Displacement\",\n          x = \"Displacement (litres)\",\n          y = \"Highway miles per gallon\") +\n      coord_cartesian(ylim = c(11, 44))\n\n\n\n\n\n\n\nWe are also interested in uncertainty about the mean value of \\(y\\) given some value of \\(x\\), especially when we are predicting \\(y\\).\nTo get the CI for the mean of \\(y\\) at any value of \\(x\\), we can use the predict() function.\nThe first argument in the function is the regression fitted result.\nThen and argument new data is a data frame of predictor values.\nHere, the predictor is at 1, 2, 3, up to 8.\nAnd interval is confidence, and confidence level is 95%.\nThe output will have 3 columns. The first column shows the predicted or fitted value of \\(y\\) given \\(x\\), these are values right on the regression line.\nThe second and third columns are lower and upper bound respectively.\nHow do we show the confidence interval for the mean of y?\nAgain we use geom_smooth() layer, and set se = TRUE, so that geom_smooth() uses the standard error info to obtain the confidence interval for us."
  },
  {
    "objectID": "slides/16-linear-reg.html#quantify-uncertainty-about-individual-y",
    "href": "slides/16-linear-reg.html#quantify-uncertainty-about-individual-y",
    "title": "Linear Regression \n",
    "section": "Quantify Uncertainty about Individual \\(y\\)\n",
    "text": "Quantify Uncertainty about Individual \\(y\\)\n\n\n\n\nUncertainty about the individual value of \\(y\\) given \\(X = x\\), \\(Y \\mid X = x\\)\n\n\n\npredict(reg_out$fit, \n        newdata = new_input, \n        interval = \"prediction\", \n        level = 0.95)\n\n   fit   lwr  upr\n1 25.1 17.53 32.7\n2 21.6 14.00 29.2\n3 18.0 10.45 25.6\n4 14.5  6.88 22.1\n\n## predict at current inputs\ndf <- as.tibble(\n    predict(\n        reg_out$fit, \n        interval = \"prediction\"))\n\n\n\np_ci + \n    geom_line(aes(x = displ, y = df$lwr), \n              color = \"red\") +\n    geom_line(aes(x = displ, y = df$upr), \n              color = \"red\")\n\n\n\n\n\n\n\n\n\n\nWe can also quantify the uncertainty about the value individual \\(y\\) given \\(x\\).\nAs you can imagine, this is more difficult to predict value of individual \\(y\\) than to predict value of mean of \\(y\\), right?\nBecause individual \\(y\\) value varies more than the mean of \\(y\\) due to the random error noises added to it.\nAnd so with the same confidence level, the CI for individual \\(y\\) will be much wider than the CI for the mean of \\(y\\).\nHow do we get the CI?\nThe code is basically the same as before. But here instead of “confidence”, here we use “prediction” in the interval argument.\nYou can see that the 95% interval is much wider, trying to capture or contain most of the observed values of \\(y\\)."
  },
  {
    "objectID": "slides/16-linear-reg.html#graphical-diagnostics-residual-plot",
    "href": "slides/16-linear-reg.html#graphical-diagnostics-residual-plot",
    "title": "Linear Regression \n",
    "section": "Graphical Diagnostics: Residual Plot",
    "text": "Graphical Diagnostics: Residual Plot\n\nResiduals distributed randomly around 0.\nCheck it by plotting residuals against the fitted value of \\(y\\): \\(e_i\\) vs. \\(\\hat{y}_i\\)\n\nWith no visible pattern along the x or y axis.\n\n\nWe are looking for - Residuals distributed randomly around 0. - With no visible pattern along the \\(x\\) or \\(y\\) axes. - This can be checked by plotting residuals against the fitted value of \\(y\\). - Here shows a good residual plot. Residuals are around 0, and its variation is more or less the same across different values of \\(\\hat{y}\\). - There is no significant pattern in the plot."
  },
  {
    "objectID": "slides/16-linear-reg.html#not-looking-for",
    "href": "slides/16-linear-reg.html#not-looking-for",
    "title": "Linear Regression \n",
    "section": "Not looking for…",
    "text": "Not looking for…\n\nFan shapes\n\n\n\nWe are not looking for a plot that has a fan shape."
  },
  {
    "objectID": "slides/16-linear-reg.html#not-looking-for-1",
    "href": "slides/16-linear-reg.html#not-looking-for-1",
    "title": "Linear Regression \n",
    "section": "Not looking for…",
    "text": "Not looking for…\n\nGroups of patterns\n\n\n\nWe are not looking for a plot that has Groups of patterns"
  },
  {
    "objectID": "slides/16-linear-reg.html#not-looking-for-2",
    "href": "slides/16-linear-reg.html#not-looking-for-2",
    "title": "Linear Regression \n",
    "section": "Not looking for…",
    "text": "Not looking for…\n\nResiduals correlated with predicted values\n\n\n\nWe also don’t want Residuals to be correlated with predicted values"
  },
  {
    "objectID": "slides/16-linear-reg.html#not-looking-for-3",
    "href": "slides/16-linear-reg.html#not-looking-for-3",
    "title": "Linear Regression \n",
    "section": "Not looking for…",
    "text": "Not looking for…\n\nAny patterns!\n\n\n\nWe basically don’t want the residuals show Any patterns!\nIf \\(X\\) and \\(Y\\) are not linearly related, or the residual plot exists some pattern, some data transformation is needed. Or use another model."
  },
  {
    "objectID": "slides/16-linear-reg.html#mpg-data-residuals",
    "href": "slides/16-linear-reg.html#mpg-data-residuals",
    "title": "Linear Regression \n",
    "section": "MPG Data Residuals",
    "text": "MPG Data Residuals\n\nplot(reg_out$fit, which = 1, col = \"blue\", las = 1)"
  },
  {
    "objectID": "slides/16-linear-reg.html#categorical-predictor-with-2-categories",
    "href": "slides/16-linear-reg.html#categorical-predictor-with-2-categories",
    "title": "Linear Regression \n",
    "section": "Categorical Predictor with 2 Categories",
    "text": "Categorical Predictor with 2 Categories\n\n\n\nmpg |>  \n  select(hwy, trans) |>  \n  print(n = 4)\n\n# A tibble: 234 × 2\n    hwy trans     \n  <int> <chr>     \n1    29 auto(l5)  \n2    29 manual(m5)\n3    31 manual(m6)\n4    30 auto(av)  \n# ℹ 230 more rows\n\n\n\n\n\n\n\n\n\n\n\n\nmpg_new <- mpg\nmpg_new$trans[grepl(\"auto\", mpg_new$trans)] <- \"auto\"\nmpg_new$trans[grepl(\"manual\", mpg_new$trans)] <- \"manual\"\n\nmpg_new |> \n    select(hwy, trans) |> \n    print(n = 4)\n\n# A tibble: 234 × 2\n    hwy trans \n  <int> <chr> \n1    29 auto  \n2    29 manual\n3    31 manual\n4    30 auto  \n# ℹ 230 more rows\n\n\n\n\n\n\ntrans = auto: Automatic transmission\n\ntrans = manual: Manual transmission\n\n\nIf you look at the mpg data set, you’ll find that the variable transmission is a categorical variable. We either have Automatic transmission or Manual transmission.\nHere I clean the data a little bit, so that the trans variable has values either auto or manual.\nAnd I am going to use this trans variable as our predictor to predict the hwy MPG. OK."
  },
  {
    "objectID": "slides/16-linear-reg.html#highway-mpg-transmission-type",
    "href": "slides/16-linear-reg.html#highway-mpg-transmission-type",
    "title": "Linear Regression \n",
    "section": "Highway MPG & Transmission Type",
    "text": "Highway MPG & Transmission Type\n\nMake sure that your categorical variable is of type character or factor.\n\n\ntypeof(mpg_new$trans)\n\n[1] \"character\"\n\n\n\ntidymdl |> \n    fit(hwy ~ trans, data = mpg_new) |> \n    tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)    22.3      0.458     48.7  8.60e-124\n2 transmanual     3.49     0.798      4.37 1.89e-  5\n\n\n\nThe baseline level is chosen to be auto transmission.\n\n\nBefore fitting the regression model, Make sure that your categorical variable is of type character or factor. If not, convert the type before fitting.\nAfter fitting the model, you can see that the name in the second row for the slope is transmanual, and it means something because R actually does something for us when fitting a regression model.\nActually, it means that the baseline level is chosen to be auto transmission."
  },
  {
    "objectID": "slides/16-linear-reg.html#highway-mpg-transmission-type-1",
    "href": "slides/16-linear-reg.html#highway-mpg-transmission-type-1",
    "title": "Linear Regression \n",
    "section": "Highway MPG & Transmission Type",
    "text": "Highway MPG & Transmission Type\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)    22.3      0.458     48.7  8.60e-124\n2 transmanual     3.49     0.798      4.37 1.89e-  5\n\n\n\\[\\widehat{hwy_{i}} = 22.3 + 3.49~trans_i\\]\n\n\nSlope: Cars with manual transmission are expected, on average, to be 3.49 more miles per gallon than cars with auto transmission.\n\nCompare baseline level (trans = auto) to the other level (trans = manual)\n\n\n\nIntercept: Cars with auto transmission are expected, on average, to have 22.3 highway miles per gallon.\n\n\nHere the regression line is \\(\\widehat{hwy_{i}} = 22.3 + 3.49~trans_i\\).\nAnd we interpret the slope and intercept as follows.\nCars with manual transmission are expected, on average, to be 3.48 more miles per gallon than cars with auto transmission.\nWe Compare baseline level (trans = auto) to the other level (trans = manual).\n\nIntercept here is Cars with auto transmission are expected, on average, to have 22.3 highway miles per gallon.\nSo basically, the intercept value 22.3 here is the average hwy MPG for cars with auto transmission, and intercept + slope (22.3 + 3.49) is the average hwy MPG for cars with manual transmission."
  },
  {
    "objectID": "slides/16-linear-reg.html#sklearn.linear_model.linearregression",
    "href": "slides/16-linear-reg.html#sklearn.linear_model.linearregression",
    "title": "Linear Regression \n",
    "section": "sklearn.linear_model.LinearRegression",
    "text": "sklearn.linear_model.LinearRegression\n\nlibrary(reticulate)\npy_install(\"scikit-learn\")\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\n\n\n\nmpg1 = pd.read_csv('./data/mpg.csv')\n# x = np.array(mpg1['displ']).reshape(-1, 1)\nx = np.array(mpg1[['displ']]) ## 2d array with one column\ny = np.array(mpg1['hwy'])\nx[0:4]\n\narray([[1.8],\n       [1.8],\n       [2. ],\n       [2. ]])\n\ny[0:4]\n\narray([29, 29, 31, 30])\n\n\n-1 in reshape function is used when you dont know or want to explicitly tell the dimension of that axis. E.g, If you have an array of shape (2,4) then reshaping it with (-1, 1), then the array will get reshaped in such a way that the resulting array has only 1 column and this is only possible by having 8 rows, hence, (8,1)."
  },
  {
    "objectID": "slides/16-linear-reg.html#sklearn.linear_model.linearregression-1",
    "href": "slides/16-linear-reg.html#sklearn.linear_model.linearregression-1",
    "title": "Linear Regression \n",
    "section": "sklearn.linear_model.LinearRegression",
    "text": "sklearn.linear_model.LinearRegression\n\nreg = LinearRegression().fit(x, y)\nreg.coef_\n\narray([-3.53058881])\n\nreg.intercept_\n\n35.6976510518446\n\n\n\n\n\nnew_input = np.arange(3, 7, 1).reshape(-1, 1)\npred = reg.predict(new_input)\npred\n\narray([25.10588463, 21.57529583, 18.04470702, 14.51411821])\n\n\n\n\n\n\nmath3570-s24.github.io/website"
  },
  {
    "objectID": "slides/09-ggplot2.html#plotting-systems-base-lattice-and-ggplot2",
    "href": "slides/09-ggplot2.html#plotting-systems-base-lattice-and-ggplot2",
    "title": "Data Visualization – ggplot2 📊",
    "section": "\nPlotting Systems: base, lattice and ggplot2\n",
    "text": "Plotting Systems: base, lattice and ggplot2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2\n\nhas the most powerful functionality.\nis more beautiful?\nhas larger file size that occupies more memory space and has longer render time.\n\n\nWe already learned a little bit about R plotting, right? In fact R has three main plotting systems, the base package, lattice package, and the ggplot2 package.\nThere are lots of tools and packages that greatly extend the ggplot2 functionality, as I listed here.\nBasically, ggplot2 has the most powerful functionality than the other two. It can create lots of graphs that the other two cannot, and ggplot2 is more beautiful than the other two, I think. But one disadvantage of ggplot2 is that it’s size is larger than the other two, occupying more memory spaces than the other two, and the rendering time is longer."
  },
  {
    "objectID": "slides/09-ggplot2.html#the-ggplot2-grammar",
    "href": "slides/09-ggplot2.html#the-ggplot2-grammar",
    "title": "Data Visualization – ggplot2 📊",
    "section": "The ggplot2 Grammar",
    "text": "The ggplot2 Grammar\n\nThree main components:\n\n      \n\n\n\n\n\n\nGrammar element\nWhat it is\n\n\n\nData\nThe data frame used for plotting\n\n\nGeometry\n\nThe geometric shape that represents the data\ne.g., point, boxplot, histogram\n\n\n\n\nAesthetic mapping\n\nThe aesthetics of the geometric object\ne.g., color, size, shape\n\n\n\n\n\n\nHow we define the mapping depends on what geometry we are using.\n\n\n\nggplot(data = <DATASET>, mapping = aes(<MAPPINGS>)) + \n       <GEOM_FUNCTION>() +\n       other options/layers\n\n\n\nData: the data input of ggplot2 is always a data frame, not a vector, or matrix.\nGeometry: defines what kind of plot we are going to make.\nAesthetic mapping: basically tell ggplot2 how we decorate the plot and make it more visible or more informative. For example, use different point colors or sizes for different categories, such male and female\nStructure of the code for ggplots can be summarized as the following code."
  },
  {
    "objectID": "slides/09-ggplot2.html#mpg-data",
    "href": "slides/09-ggplot2.html#mpg-data",
    "title": "Data Visualization – ggplot2 📊",
    "section": "mpg Data",
    "text": "mpg Data\n\nggplot2::mpg\n\n# A tibble: 234 × 11\n  manufacturer model      displ  year   cyl trans  drv     cty   hwy fl    class\n  <chr>        <chr>      <dbl> <int> <int> <chr>  <chr> <int> <int> <chr> <chr>\n1 audi         a4           1.8  1999     4 auto(… f        18    29 p     comp…\n2 audi         a4           1.8  1999     4 manua… f        21    29 p     comp…\n3 audi         a4           2    2008     4 manua… f        20    31 p     comp…\n4 audi         a4           2    2008     4 auto(… f        21    30 p     comp…\n5 audi         a4           2.8  1999     6 auto(… f        16    26 p     comp…\n6 audi         a4           2.8  1999     6 manua… f        18    26 p     comp…\n7 audi         a4           3.1  2008     6 auto(… f        18    27 p     comp…\n8 audi         a4 quattro   1.8  1999     4 manua… 4        18    26 p     comp…\n# ℹ 226 more rows\n\n\n\nHere shows the mpg data set saved in ggplot2.\nIt is a tibble with 11 variables, including ……\nWe are gonna use it as an example to learn to create a ggplot."
  },
  {
    "objectID": "slides/09-ggplot2.html#section-3",
    "href": "slides/09-ggplot2.html#section-3",
    "title": "Data Visualization – ggplot2 📊",
    "section": "",
    "text": "Plot\nCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = mpg,\n       mapping = aes(x = displ, \n                     y = hwy, \n                     color = class)) + \n    geom_point() +\n    labs(title = \"Engine Size v.s. Fuel Efficiency\",\n         subtitle = \"Dimensions for class\",\n         x = \"Engine displacement (litres)\", y = \"Highway (mpg)\",\n         color = \"Type of car\",\n         caption = \"Source: http://fueleconomy.gov\")\n\n\n\n\n\nHere shows a ggplot, showing the relationship between mpg and engine displacement. We have titles, labels, and footnote.\nAnd we also use different colors to show different types of car. Right.\nThe code is right here. We are gonna create this plot step by step by step OK."
  },
  {
    "objectID": "slides/09-ggplot2.html#section-4",
    "href": "slides/09-ggplot2.html#section-4",
    "title": "Data Visualization – ggplot2 📊",
    "section": "",
    "text": "Start with the mpg data frame\n\n\n\n\nlibrary(ggplot2)\nggplot(data = mpg) #<<\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemember the data input is always a data frame.\n\nWhen you call the function ggplot(), but without any geometry, R just renders a plot background colored in gray."
  },
  {
    "objectID": "slides/09-ggplot2.html#section-5",
    "href": "slides/09-ggplot2.html#section-5",
    "title": "Data Visualization – ggplot2 📊",
    "section": "",
    "text": "Start with the mpg data frame, map engine displacement to the x-axis\n\n\n\n\nggplot(data = mpg,\n       mapping = aes(x = displ)) #<<\n\n\n\ndispl is the variable name in mpg.\n\n\n\n\n\n\n\n\n\n\n\nR will create tick marks and label of x-axis for you.\n\n\n\n\nThen after specifying the data set, we can start decorating our plot.\nRemember we are going to create a scatter plot of displacement and miles pewr gallon.\nSo we can first map engine displacement to the x-axis using mapping = aes(x = displ)\ndispl is the variable name in the mpg data set.\nR will create some default tick marks and label of x-axis for you."
  },
  {
    "objectID": "slides/09-ggplot2.html#section-6",
    "href": "slides/09-ggplot2.html#section-6",
    "title": "Data Visualization – ggplot2 📊",
    "section": "",
    "text": "Start with the mpg data frame, map engine displacement to the x-axis and map highway miles per gallon to the y-axis.\n\n\n\n\nggplot(data = mpg,\n       mapping = aes(x = displ,\n                     y = hwy)) #<<\n\n\nSpecify y = hwy in the same aes() of the mapping argument as x = displ, separated by comma.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThen we map highway miles per gallon to the y-axis by specifying y = hwy in aesthetics\n\nhwy is the variable name in the mpg data set.\nR will create some default tick marks and label of y-axis for you.\nNow we have variables in both x and y axis. It’s time to define the geometry of the plot. That is, tell ggplot, what kind of plot you want!"
  },
  {
    "objectID": "slides/09-ggplot2.html#section-7",
    "href": "slides/09-ggplot2.html#section-7",
    "title": "Data Visualization – ggplot2 📊",
    "section": "",
    "text": "Start with the mpg data frame, map engine displacement to the x-axis and map highway miles per gallon to the y-axis. Represent each observation with a point\n\n\n\n\nggplot(data = mpg,\n       mapping = aes(x = displ, \n                     y = hwy)) + \n  geom_point() #<<\n\n\nTo define a geometry, add a geom layer.\n\n\nDon’t miss + sign!\n\n\nFor scatterplots we add points, and use geom_point()\nLots of geoms\n\n\n\n\n\n\n\n\n\n\n\n\ngeom (geometric object) - What’s the next? Remember we use different colors of points to represent different car types, right? - Which part of code you think we can use to color the points? - aesthetics in the mapping! - By default, ggplot generates black solid points, each representing an observation’s hwy and displ value."
  },
  {
    "objectID": "slides/09-ggplot2.html#section-8",
    "href": "slides/09-ggplot2.html#section-8",
    "title": "Data Visualization – ggplot2 📊",
    "section": "",
    "text": "Start with the mpg data frame, map engine displacement to the x-axis and map highway miles per gallon to the y-axis. Represent each observation with a point and map type of car (class) to the color of each point.\n\n\n\n\nggplot(data = mpg,\n       mapping = \n         aes(x = displ, \n             y = hwy, \n             color = class)) + #<<\n  geom_point()\n\n\nAdd color = class in aes() of the mapping argument, where class is the variable name for type of car.\nggplot automatically generates a legend on the right.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo map type of car (class) to the color of each point, add color = class in aes() of the mapping argument\nAnd now you can see that each color represent a type of car.\nggplot automatically generates a default legend on the right, where the title of the legend is the variable name, and the names of legend are the variable values.\nOK. We are almost done. The rest are just adding titles x, y labels. Let’s see how."
  },
  {
    "objectID": "slides/09-ggplot2.html#section-9",
    "href": "slides/09-ggplot2.html#section-9",
    "title": "Data Visualization – ggplot2 📊",
    "section": "",
    "text": "Start with the mpg data frame, map engine displacement to the x-axis and map highway miles per gallon to the y-axis. Represent each observation with a point and map type of car (class) to the color of each point. Title the plot “Engine Size v.s. Fuel Efficiency”\n\n\n\n\n\nggplot(data = mpg,\n       mapping = aes(x = displ, \n                     y = hwy, \n                     color = class)) + \n  geom_point() +\n  labs(\n    title=\"Engine Size vs. Fuel Efficiency\" #<<\n    )\n\n\nAdd any labels in labs() layer.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can add any labels in labs() layer.\nHere we set title = “Engine Size v.s. Fuel Efficiency”"
  },
  {
    "objectID": "slides/09-ggplot2.html#section-10",
    "href": "slides/09-ggplot2.html#section-10",
    "title": "Data Visualization – ggplot2 📊",
    "section": "",
    "text": "Start with the mpg data frame, map engine displacement to the x-axis and map highway miles per gallon to the y-axis. Represent each observation with a point and map type of car (class) to the color of each point. Title the plot “Engine Size vs. Fuel Efficiency”, add the subtitle “Dimensions for class”\n\n\n\n\n\nggplot(data = mpg,\n       mapping = aes(x = displ, \n                     y = hwy, \n                     color = class)) + \n  geom_point() +\n  labs(\n    title=\"Engine Size vs. Fuel Efficiency\",\n    subtitle=\"Dimensions for class\" #<<\n    ) \n\n\nAdd a subtitle in labs()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also add a subtitle title “Dimensions for class”.\nBTW, we can change the font size, color and position of the title. Here is just the default setting.\nIn theme(plot.title = element_text(size = 20, color = “#1b98e0”)\nhttps://statisticsglobe.com/ggplot2-title-subtitle-with-different-size-and-color-in-r\nhttps://en.wikipedia.org/wiki/Point_(typography)\nhttps://stackoverflow.com/questions/17311917/ggplot2-the-unit-of-size\nWe will talk about that if time permitted. Or I can ask you in homework and you can learn by yourself."
  },
  {
    "objectID": "slides/09-ggplot2.html#section-11",
    "href": "slides/09-ggplot2.html#section-11",
    "title": "Data Visualization – ggplot2 📊",
    "section": "",
    "text": "Start with the mpg data frame, map engine displacement to the x-axis and map highway miles per gallon to the y-axis. Represent each observation with a point and map type of car (class) to the color of each point. Title the plot “Engine Size vs. Fuel Efficiency”, add the subtitle “Dimensions for class”, label the x and y axes as “Engine displacement (litres)” and “Highway (mpg)”, respectively\n\n\n\n\n\nggplot(data = mpg,\n       mapping = aes(x = displ, \n                     y = hwy, \n                     color = class)) + \n  geom_point() +\n  labs(\n    title = \"Engine Size vs. Fuel Efficiency\",\n    subtitle = \"Dimensions for class\",\n    x = \"Engine displacement (litres)\", #<<\n    y = \"Highway (mpg)\" #<<\n    ) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe then label the x and y axes as “Engine displacement (litres)” and “Highway (mpg)”, respectively."
  },
  {
    "objectID": "slides/09-ggplot2.html#section-12",
    "href": "slides/09-ggplot2.html#section-12",
    "title": "Data Visualization – ggplot2 📊",
    "section": "",
    "text": "Start with the mpg data frame, map engine displacement to the x-axis and map highway miles per gallon to the y-axis. Represent each observation with a point and map type of car (class) to the color of each point. Title the plot “Engine Size vs. Fuel Efficiency”, add the subtitle “Dimensions for class”, label the x and y axes as “Engine displacement (litres)” and “Highway (mpg)”, respectively, label the legend “Type of car”\n\n\n\n\n\nggplot(data = mpg,\n       mapping = aes(x = displ, \n                     y = hwy, \n                     color = class)) + \n  geom_point() +\n  labs(\n    title = \"Engine Size vs. Fuel Efficiency\",\n    subtitle = \"Dimensions for class\",\n    x = \"Engine displacement (litres)\", \n    y = \"Highway (mpg)\",\n    color = \"Type of car\" #<<\n    ) \n\n\nThe legend is generated when we map type of car (class) to color.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe label the legend “Type of car” using color = “Type of car”. Why?\nBecause the legend is generated when we map type of car to color.\nThe color title is the legend title."
  },
  {
    "objectID": "slides/09-ggplot2.html#section-13",
    "href": "slides/09-ggplot2.html#section-13",
    "title": "Data Visualization – ggplot2 📊",
    "section": "",
    "text": "Start with the mpg data frame, map engine displacement to the x-axis and map highway miles per gallon to the y-axis. Represent each observation with a point and map type of car (class) to the color of each point. Title the plot “Engine Size vs. Fuel Efficiency”, add the subtitle “Dimensions for class”, label the x and y axes as “Engine displacement (litres)” and “Highway (mpg)”, respectively, label the legend “Type of car”, and add a caption for the data source.\n\n\n\n\n\nggplot(data = mpg,\n       mapping = aes(x = displ, \n                     y = hwy, \n                     color = class)) + \n  geom_point() +\n  labs(\n    title = \"Engine Size vs. Fuel Efficiency\",\n    subtitle = \"Dimensions for class\",\n    x = \"Engine displacement (litres)\", \n    y = \"Highway (mpg)\",\n    color = \"Type of car\",\n    caption=\"Source: http://fueleconomy.gov\" #<<\n    ) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, we can add a caption for the data source using caption argument\nThen we are done! We have this beautiful scatter plot!"
  },
  {
    "objectID": "slides/09-ggplot2.html#section-14",
    "href": "slides/09-ggplot2.html#section-14",
    "title": "Data Visualization – ggplot2 📊",
    "section": "",
    "text": "Start with the mpg data frame, map engine displacement to the x-axis and map highway miles per gallon to the y-axis. Represent each observation with a point and map type of car (class) to the color of each point. Title the plot “Engine Size vs. Fuel Efficiency”, add the subtitle “Dimensions for class”, label the x and y axes as “Engine displacement (litres)” and “Highway (mpg)”, respectively, label the legend “Type of car”, and add a caption for the data source. Finally, use a discrete color scale that is designed to be perceived by viewers with common forms of color blindness.\n\n\n\n\n\nggplot(data = mpg,\n       mapping = aes(x = displ, \n                     y = hwy, \n                     color = class)) + \n  geom_point() +\n  labs(\n    title = \"Engine Size vs. Fuel Efficiency\",\n    subtitle = \"Dimensions for class\",\n    x = \"Engine displacement (litres)\", \n    y = \"Highway (mpg)\",\n    color = \"Type of car\",\n    caption = \"Source: http://fueleconomy.gov\"\n    ) +\n  scale_colour_viridis_d() #<<\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd if we are considerate enough, we can use a discrete color scale that is designed to be perceived by viewers with common forms of color blindness.\n=======================================\nLet’s continue our discussion of ggplot2. We learned there are three main components of ggplot2, the data, which is a data frame, the geometry object, and aesthetics mapping.\nAnd we are free to add more layers with the plus sign to manipulate or put more features on the plot.\nIn fact, we can actually save a ggplot as an object, and we can recreate the plot by just printing the object out. Let’s see how."
  },
  {
    "objectID": "slides/09-ggplot2.html#section-15",
    "href": "slides/09-ggplot2.html#section-15",
    "title": "Data Visualization – ggplot2 📊",
    "section": "",
    "text": "11-ggplot2  \nIn lab.qmd ## Lab 11 section,\n\nUse readr::read_csv() to import the data penguins.csv into your R workspace.\nGenerate the following ggplot:"
  },
  {
    "objectID": "slides/09-ggplot2.html#section-16",
    "href": "slides/09-ggplot2.html#section-16",
    "title": "Data Visualization – ggplot2 📊",
    "section": "",
    "text": "penguins <- read_csv(_________________)\n________ |> \n  ggplot(mapping = ____(x = ______________,\n                        y = ______________,\n                        colour = ________)) +\n  geom______() +\n  ____(title = ____________________,\n       _________ = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = _____________, y = _______________,\n       _______ = \"Species\",\n       _______ = \"Source: Palmer Station LTER / palmerpenguins package\")"
  },
  {
    "objectID": "slides/09-ggplot2.html#assign-a-plot-to-an-object",
    "href": "slides/09-ggplot2.html#assign-a-plot-to-an-object",
    "title": "Data Visualization – ggplot2 📊",
    "section": "Assign a Plot to an Object",
    "text": "Assign a Plot to an Object\n\n\n\np <- ggplot(data = mpg,\n            mapping = \n                aes(x = displ, \n                    y = hwy, \n                    color = class)) + \n    geom_point()\nclass(p)\n\n[1] \"gg\"     \"ggplot\"\n\np\n\n\n\n\n\n\n\n\n\np + labs(\n      title = \"Engine Size vs. Fuel Efficiency\",\n      subtitle = \"Dimensions for class\",\n      x = \"Engine displacement (litres)\", \n      y = \"Highway (mpg)\",\n      color = \"Type of car\",\n      caption = \"Source: http://fueleconomy.gov\"\n    )\n\n\n\n\n\n\n\n\n\n\nSo one advantage of ggplot is that you can Assign a ggplot to an Object that has its own class ggplot.\nbase R plotting system is not able to assign an object to a plot for rendering.\nhere, I assign the plot to the object called “p”. When I print it out, it shows the ggplot.\nThis object “p” stores the scatter plot structure, and can be saved for later use.\nFor example, we can add labels on the existent ggplot object. And if we want to use the same scatter plot but with a different labels or any other features, just add the new labels or features to the object “p”.\nSo this way we don’t need to type the basic ggplot structure all over again.\nIt’s quite useful if you wanna generate the same plot but with different decorations. Maybe here you want red labels, and at another place you want blue labels, for example."
  },
  {
    "objectID": "slides/09-ggplot2.html#theme-options",
    "href": "slides/09-ggplot2.html#theme-options",
    "title": "Data Visualization – ggplot2 📊",
    "section": "Theme Options",
    "text": "Theme Options\nOptions include\ntheme_grey() (default), theme_bw(), theme_dark(), theme_classic(), etc.\n\n\n\n\n\n\np + theme_bw()\n\n\n\n\n\n\n\n\n\np + theme_dark()\n\n\n\n\n\n\n\n\n\n\nOK theme. You can add a theme layer theme() to your plot to tweak the display of the theme the plot is currently using, including title, axis labels, etc. Check theme() in the help page. You will learn which components of a plot can be changed.\nSome theme options you can use include theme_grey() (default), theme_bw(), theme_dark(), theme_classic(), etc\nTo use another theme, we just add its corresponding theme function to the plot.\nHere shows how the black/white theme and dark theme look like."
  },
  {
    "objectID": "slides/09-ggplot2.html#add-on-ggthemes",
    "href": "slides/09-ggplot2.html#add-on-ggthemes",
    "title": "Data Visualization – ggplot2 📊",
    "section": "Add-on 📦: ggthemes\n",
    "text": "Add-on 📦: ggthemes\n\n\nMany other themes are added by the package ggthemes.\nCheck package website, ggplot2 extensions, and ALL YOUR FIGURE ARE BELONG TO US for more themes.\n\n\n\n\np + ggthemes::theme_economist()\n\n\n\n\n\n\n\n\n\np + ggthemes::theme_fivethirtyeight()\n\n\n\n\n\n\n\n\n\n\nAs I mentioned before, many people are really into ggplot2, and lots of add-on packages have been created for ggplot2, that greatly extend its functionalities.\nFor example, there is a package called ggthemes that provide more theme options for us.\nHere I showed you the same scatter plot but in economist and fivethirtyeight theme."
  },
  {
    "objectID": "slides/09-ggplot2.html#customize-theme",
    "href": "slides/09-ggplot2.html#customize-theme",
    "title": "Data Visualization – ggplot2 📊",
    "section": "Customize Theme",
    "text": "Customize Theme\n\nUse theme() to tweak the display of the current theme, including title, axis labels, etc. Check ?theme.\n\n\n\n\np + theme(\n    panel.background = \n        element_rect(fill = \"#FFCC00\",\n                     colour = \"blue\",\n                     size = 2.5, \n                     linetype = \"solid\"),\n    plot.background = \n        element_rect(fill = \"lightblue\"),\n    axis.line = \n        element_line(size = 0.5, \n                     linetype = \"solid\",\n                     colour = \"red\")\n    )\n\n\n\n\n\n\n\n\n\n\n\n\nIn conjunction with the theme system, the element_ functions specify the display of how non-data components of the plot are drawn.\nelement_blank(): draws nothing, and assigns no space.\nelement_rect(): borders and backgrounds.\nelement_line(): lines.\nelement_text(): text."
  },
  {
    "objectID": "slides/09-ggplot2.html#aesthetics-options",
    "href": "slides/09-ggplot2.html#aesthetics-options",
    "title": "Data Visualization – ggplot2 📊",
    "section": "Aesthetics options",
    "text": "Aesthetics options\nCommonly used characteristics of plotting characters that can be mapped to a specific variable in the data are\n\ncolour\nshape\nsize\n\nalpha (transparency)\n\n\nRemember that we map color to type of car in the scatter plot of hwy mpg and displacement.\nother options can be mapped to a specific variable in the data as well, such as shape, size and alpha that controls the transparency of your geometric objects."
  },
  {
    "objectID": "slides/09-ggplot2.html#colour",
    "href": "slides/09-ggplot2.html#colour",
    "title": "Data Visualization – ggplot2 📊",
    "section": "Colour",
    "text": "Colour\n\n\n\nggplot(\n    data = mpg,\n    mapping = aes(\n        x = displ, \n        y = hwy, \n        color = class)) + #<<\n    geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\nYes, we have seen this."
  },
  {
    "objectID": "slides/09-ggplot2.html#shape",
    "href": "slides/09-ggplot2.html#shape",
    "title": "Data Visualization – ggplot2 📊",
    "section": "Shape",
    "text": "Shape\nMapped to a different variable than colour\n\n\n\nggplot(\n    data = mpg,\n    mapping = aes(\n        x = displ, \n        y = hwy, \n        color = class,\n        shape = drv)) + #<<\n    geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can map color to type of car, the class variable, and map point shape to another variable, here the drive type.\nThis way, we have more information in one figure. For a single point or an observation, we can learn its displacement and highway mpg values, and we can also learn its class, the car type by color and drive train type by point shape."
  },
  {
    "objectID": "slides/09-ggplot2.html#shape-1",
    "href": "slides/09-ggplot2.html#shape-1",
    "title": "Data Visualization – ggplot2 📊",
    "section": "Shape",
    "text": "Shape\nMapped to same variable as colour\n\n\n\nggplot(\n    data = mpg,\n    mapping = aes(\n        x = displ, \n        y = hwy, \n        color = class,\n        shape = class)) + #<<\n    geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can of course map color and shape to the same variable.\nHere, we use both color and point shape to classify or categorize the type of car.\nBut you guys need to think if it is meaningful and helpful, or it is just redundant, and make it hard to read the plot."
  },
  {
    "objectID": "slides/09-ggplot2.html#size",
    "href": "slides/09-ggplot2.html#size",
    "title": "Data Visualization – ggplot2 📊",
    "section": "Size",
    "text": "Size\n\n\n\nggplot(\n    data = mpg,\n    mapping = aes(\n        x = displ, \n        y = hwy, \n        color = class,\n        shape = class,\n        size = cty)) + #<<\n    geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\nOK, Size. Here, we use color and shape to indicate car type, and we use point size to represent city mpg. The higher city mpg, the bigger the point is.\nYou can see that the plot is not very clear and visible. So it is not always good to include much information in one single plot.\nYes, the plot contains lots of information, but we may not be able to read those information."
  },
  {
    "objectID": "slides/09-ggplot2.html#alpha",
    "href": "slides/09-ggplot2.html#alpha",
    "title": "Data Visualization – ggplot2 📊",
    "section": "Alpha",
    "text": "Alpha\n\n\n\nggplot(\n    data = mpg,\n    mapping = aes(\n        x = displ, \n        y = hwy, \n        color = class,\n        shape = class,\n        size = cty,\n        alpha = year)) + #<<\n    geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere we further map the transparency of points, alpha to the variable year. So the older the car is, the more transparent the point is.\nWe use lots of aesthetics options at the same time in one figure. Again, in practice this may not be a good idea, and the plot may not visualize the data well.\nIf you are interested in data visualization, there are lots of studies out there talking about what makes a good plot. My suggestion is, don’t put too much information in one single plot. This will make it hard to read."
  },
  {
    "objectID": "slides/09-ggplot2.html#mapping-vs.-setting",
    "href": "slides/09-ggplot2.html#mapping-vs.-setting",
    "title": "Data Visualization – ggplot2 📊",
    "section": "Mapping vs. Setting",
    "text": "Mapping vs. Setting\n\n\nMapping\n\nDetermine the size, alpha, etc.\n\nbased on the values of a variable in the data.\n\nGoes into aes().\n\n\n\nggplot(data = mpg,\n       mapping = aes(x = displ, \n                     y = hwy, \n                     size = cty, #<<\n                     alpha = year)) + #<<\n    geom_point()\n\n\n\n\n\n\n\n\n\n\nOK. Now I want to talk about the difference between mapping and setting.\nFirst, the aesthetic options such as color, size, shape, alpha can be used in both mapping and setting.\nWhen we use these options in mapping arguments, wrapped by aes() function, we actually map these options to some variables. In other words, we use those options to represents the values of the variables. Like here, the point size is used to represent the city mpg, the different point transparency are for different car years.\nSo there is a mapping between aesthetics options and variables.\nAlright setting. When we use the aesthetics options as a geometry setting, the aesthetics are pure decoration of your plot, the aesthetics are not used to represent any other variable values.\nLook at the example here. If we want to set point size at 5 and point transparency at 0.5, we put size = 5, alpha = 0.5 in the geometry function geom_point(), not in the mapping argument.\nAnd the result is that, all the points in the scatter plot will have size 5 and alpha 0.5. And it is true for all observations and variables. It is not related to any other variables as mapping does. OK."
  },
  {
    "objectID": "slides/09-ggplot2.html#mapping-vs.-setting-1",
    "href": "slides/09-ggplot2.html#mapping-vs.-setting-1",
    "title": "Data Visualization – ggplot2 📊",
    "section": "Mapping vs. Setting",
    "text": "Mapping vs. Setting\n\n\nSetting\n\nDetermine the size, alpha, etc.\n\nnot based on the values of a variable in the data.\n\ngoes into geom_*().\n\n\n\nggplot(data = mpg,\n       mapping = aes(x = displ, \n                     y = hwy)) +\n    geom_point(size = 5, alpha = 0.5) #<<\n\n\n\n\n\n\n\n\n\n\nSo here is the summary.\n(geom_point() in the example, but we’ll learn other geoms soon!)"
  },
  {
    "objectID": "slides/09-ggplot2.html#faceting-1",
    "href": "slides/09-ggplot2.html#faceting-1",
    "title": "Data Visualization – ggplot2 📊",
    "section": "Faceting",
    "text": "Faceting\n\nOne way to add additional variables’ information is with aesthetics. But we see that putting all information in one plot may not be a good idea.\nAnother way, particularly useful for categorical variables, is to\n\nsplit your plot into facets, smaller plots that each display one subset of the data.\n\nUseful for exploring conditional relationships and large data.\n\n\nYou see that one way to add additional variables’ information is with aesthetics mapping. But we see that putting all information in one plot may not be a good idea.\nWe can actually present our data using another way, particularly useful for categorical variables, which is split your plot into the so-called facets, so each smaller plot display one subset of the data conditional on some categorical variable.\nThe idea is, we can map color to gender in a one single plot, or we can create two small plots, one for male, and the other for female. OK."
  },
  {
    "objectID": "slides/09-ggplot2.html#section-17",
    "href": "slides/09-ggplot2.html#section-17",
    "title": "Data Visualization – ggplot2 📊",
    "section": "",
    "text": "ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n    geom_point() + \n    facet_grid(drv ~ cyl)  #<<\n\n\n\nSuppose again we would like to see the relationship between hwy and displacement.\nBut now we want to see their relationship given at a different number of cylinders and type of drive trains.\nAnd we can create several scatterplots, each at a particular number of cylinders and type of drive trains.\nHow do we create facets?\nWe can use facet_grid() command to create a matrix-like plot defined by row and column faceting variables, which are usually categorical variables. Inside the parenthesis, we put the row variable ~ column variable.\nSo here, drive values, 4, f, r defines the rows, and cylinder values 4, 5, 6, 8 defines the columns.\nAnd each smaller plot is a scatter plot with some value of drive and some value of cylinder.\nFor example, the top right one is the scatter plot of hwy mpg and displacement when drive is 4 and cylinder is 8."
  },
  {
    "objectID": "slides/09-ggplot2.html#section-20",
    "href": "slides/09-ggplot2.html#section-20",
    "title": "Data Visualization – ggplot2 📊",
    "section": "",
    "text": "ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n    geom_point() + \n    facet_wrap(~ cyl, ncol = 2) #<<\n\n\n\nHow about we want to create a scatter plot conditional on just one variable, say cylinder?\nInstead of facet_grid(), we can use facet_wrap() function.\nInside the parenthesis, we use ~ followed by the variable name cyl.\nAnd the ggplot will automatically create a display, so that each smaller plot is normally rectangular.\nIf you don’t like the default display, you can specify any number of rows or columns you like.\nFor example, here the number of column is 3."
  },
  {
    "objectID": "slides/09-ggplot2.html#facet-and-color",
    "href": "slides/09-ggplot2.html#facet-and-color",
    "title": "Data Visualization – ggplot2 📊",
    "section": "Facet and Color",
    "text": "Facet and Color\n\nggplot(data = mpg, \n       mapping = aes(x = displ, y = hwy,\n                     color = drv)) +\n    geom_point() + \n    facet_grid(drv ~ cyl)\n\n\n\nWe can add color to faceting variables for sure.\nFor example here, we map color to variable drive, which is the row variable in faceting.\nAnd now each row has its own color."
  },
  {
    "objectID": "slides/09-ggplot2.html#facet-and-color-with-no-legend",
    "href": "slides/09-ggplot2.html#facet-and-color-with-no-legend",
    "title": "Data Visualization – ggplot2 📊",
    "section": "Facet and Color with no Legend",
    "text": "Facet and Color with no Legend\n\nggplot(data = mpg, \n       mapping = aes(x = displ, y = hwy, color = drv)) +\n    geom_point() + \n    facet_grid(drv ~ cyl) +\n    guides(color = \"none\") #<<\n\n\ntheme(legend.position = “none”) - Since each row is for one value of drive, it’s pretty clear to see which color is for which value of drive. - If you don’t want the legend, use guides() and set color = FALSE. - Basically guides() can set or remove the legend for a specific aesthetic option. If you have two aesthetics color and size, guides(color = FALSE) will only remove the legend for color and the legend for size will be still there. OK."
  },
  {
    "objectID": "slides/09-ggplot2.html#section-21",
    "href": "slides/09-ggplot2.html#section-21",
    "title": "Data Visualization – ggplot2 📊",
    "section": "",
    "text": "12-Faceting \nIn lab.qmd ## Lab 12 section, \n\nggplot(data = _______, \n       mapping = aes(x = ______, y = ______, ______ = drv, shape = _____)) +\n    geom______(______ = 3, ______ = 0.8) + \n    facet_grid(______ ~ _______) +\n    guides(______ = \"none\")"
  },
  {
    "objectID": "slides/09-ggplot2.html#ggplot-for-python",
    "href": "slides/09-ggplot2.html#ggplot-for-python",
    "title": "Data Visualization – ggplot2 📊",
    "section": "ggplot for Python",
    "text": "ggplot for Python\n\nplotnine package\nSyntax are the same as ggplot in R.\n\n\nfrom plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap\n\n\n\n\n\n\n\n\n\n\n\n\nmath3570-s24.github.io/website"
  },
  {
    "objectID": "slides/02-datascience.html#a-little-history-of-data-science",
    "href": "slides/02-datascience.html#a-little-history-of-data-science",
    "title": "Data Science Overview 📖",
    "section": "A Little History of Data Science",
    "text": "A Little History of Data Science\n\nIn 1985, Dr. Jeff Wu used the term Data Science for the first time as an alternative name for Statistics.\n\nIn 1997, Dr. Wu calls for statistics to be renamed data science and statisticians to be renamed data scientists.\n\nIn 2001, Dr. William S. Cleveland advocated an expansion of statistics beyond theory into six technical areas, and the altered expanded field will be called data science.\n\n\n\n\nThe first question is What is DS? Actually this is a really hard question to answer, at least at this moment.\n\nOriginally, DS was proposed to replace statistics. DS is just another name or maybe better name for statistics.\n\n\nBut still at that time, nobody cares. Statisticians are weird guys in math departments who deal with proofs and theorems and sometimes real data, and they are stubborn, not willing to change. But if you wiki the definition of statistics, statistics is a science of data, and you tell me what is the difference of science of data and data science?\n\n\nUsing this definition, DS becomes a broader term than statistics. Anything related to data can be a part of data sicence, while statistics now becomes a narrower subject focusing on inference and data analysis part.\n\nHere shows the google trend of DS. When I was in college or beginning of PHD studies, very few people used DS as a term. And all of sudden, DS become more and more popular, and everybody is talking about it."
  },
  {
    "objectID": "slides/02-datascience.html#section-1",
    "href": "slides/02-datascience.html#section-1",
    "title": "Data Science Overview 📖",
    "section": "",
    "text": "Source: https://www.reddit.com/r/meme/comments/floq3q/reality_behind_data_science/\n\n\n\n\n\n\n\n\n\n\nSource: https://br.ifunny.co/picture/we-will-work-together-statistics-computer-science-please-teach-now-h4hdtthT9?s=cl"
  },
  {
    "objectID": "slides/02-datascience.html#battle-of-the-data-science-venn-diagrams",
    "href": "slides/02-datascience.html#battle-of-the-data-science-venn-diagrams",
    "title": "Data Science Overview 📖",
    "section": "Battle of the Data Science Venn Diagrams",
    "text": "Battle of the Data Science Venn Diagrams\n\n\n\n2010 Drew Conway\n\n\n\n\n\n\n\n\n\n\n\n2012 Brendan Tierney"
  },
  {
    "objectID": "slides/02-datascience.html#battle-of-the-data-science-venn-diagrams-1",
    "href": "slides/02-datascience.html#battle-of-the-data-science-venn-diagrams-1",
    "title": "Data Science Overview 📖",
    "section": "Battle of the Data Science Venn Diagrams",
    "text": "Battle of the Data Science Venn Diagrams\n\n\n\n2013 Ulrich Matter\n\n\n\n\n\n\n\n\n\n\n\n2013 Joel Grus\n\n\n\n\n\n\n\n\n\n\n\n\nStarting 2010, people tried to use venn diagrams to define DS, what should be include, what should be not. It’s a subset of something, or superset of something.\nWe have to do DS on some subject/field.\n(left) ML use CS and STAT but doesn’t care about domain knowledge.\nIf you apply CS to substantive expertise without solid math and stats, you are doing bullshit basically.\n(right) DS is a subset of KDD - Knowledge Discovery in Data"
  },
  {
    "objectID": "slides/02-datascience.html#battle-of-the-data-science-venn-diagrams-2",
    "href": "slides/02-datascience.html#battle-of-the-data-science-venn-diagrams-2",
    "title": "Data Science Overview 📖",
    "section": "Battle of the Data Science Venn Diagrams",
    "text": "Battle of the Data Science Venn Diagrams\n\n\n\n2013 Harlan Harris\n\n\n\n\n\n\n\n\n\n\n\n2014 Steven Geringer"
  },
  {
    "objectID": "slides/02-datascience.html#battle-of-the-data-science-venn-diagrams-3",
    "href": "slides/02-datascience.html#battle-of-the-data-science-venn-diagrams-3",
    "title": "Data Science Overview 📖",
    "section": "Battle of the Data Science Venn Diagrams",
    "text": "Battle of the Data Science Venn Diagrams\n\n\n\n2014 Michael Malak\n\n\n\n\n\n\n\n\n\n\n\n2015 Stephan Kolassa"
  },
  {
    "objectID": "slides/02-datascience.html#battle-of-the-data-science-venn-diagrams-4",
    "href": "slides/02-datascience.html#battle-of-the-data-science-venn-diagrams-4",
    "title": "Data Science Overview 📖",
    "section": "Battle of the Data Science Venn Diagrams",
    "text": "Battle of the Data Science Venn Diagrams\n\n\n\n2016 Gregory Piatetsky-Shapiro\n\n\n\n\n\n\n\n\n\n\n\n2017 Longbing Cao"
  },
  {
    "objectID": "slides/02-datascience.html#shall-we-continue",
    "href": "slides/02-datascience.html#shall-we-continue",
    "title": "Data Science Overview 📖",
    "section": "Shall We Continue?",
    "text": "Shall We Continue?\n\n\n\ntotally-not-a-Venn-diagram by Calvin Andrus"
  },
  {
    "objectID": "slides/02-datascience.html#section-3",
    "href": "slides/02-datascience.html#section-3",
    "title": "Data Science Overview 📖",
    "section": "",
    "text": "You probably get the idea. There are so many ways to define data science."
  },
  {
    "objectID": "slides/02-datascience.html#nobody-knows-what-data-science-scientist-is",
    "href": "slides/02-datascience.html#nobody-knows-what-data-science-scientist-is",
    "title": "Data Science Overview 📖",
    "section": "Nobody Knows What Data Science (Scientist) is",
    "text": "Nobody Knows What Data Science (Scientist) is\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n💰 💰 Dr. Grant (Statistics Education Research Journal, 2017): Data science is the field of people who decide to print “Data Scientist” on their business cards to get a salary bump! 💵 💵"
  },
  {
    "objectID": "slides/02-datascience.html#what-wiki-defines",
    "href": "slides/02-datascience.html#what-wiki-defines",
    "title": "Data Science Overview 📖",
    "section": "What Wiki Defines",
    "text": "What Wiki Defines"
  },
  {
    "objectID": "slides/02-datascience.html#data-science-in-this-course",
    "href": "slides/02-datascience.html#data-science-in-this-course",
    "title": "Data Science Overview 📖",
    "section": "Data Science in This Course",
    "text": "Data Science in This Course\n\nData science is an discipline that allows us to turn raw data into understanding, insight, and knowledge.\nWe’re going to learn to do this in a tidy way – more on that later!\nThis is a introductory data science course with an emphasis on important tools in R/Python that help us do data science."
  },
  {
    "objectID": "slides/02-datascience.html#data-science-workflow",
    "href": "slides/02-datascience.html#data-science-workflow",
    "title": "Data Science Overview 📖",
    "section": "Data Science Workflow",
    "text": "Data Science Workflow"
  },
  {
    "objectID": "slides/02-datascience.html#section-4",
    "href": "slides/02-datascience.html#section-4",
    "title": "Data Science Overview 📖",
    "section": "",
    "text": "Import: Take data stored somewhere and load it into your workspace.\n\nTidy: Storing data in a consistent rectangular form, i.e., a data matrix.\n\nTransform: Narrowing in on observations of interest, creating new variables, calculating statistics.\n\n\nTo do DS, we need data, so the first step is to import data (——).\nBut usually the raw data are very messy, so we need tidy the data by storing data in a consistent rectangular form (data frame in R), which is easy for modeling and analysis.\nDepending on our research goal, we may just want to focus on some variables , or observations in the data, or we may want to create some variables for example, the data use feet, but we want to use meter instead. This data transformation further cleans the data, so that we can extract any information we need from data more easily and efficiently. In other words, we improve the quality of data."
  },
  {
    "objectID": "slides/02-datascience.html#data-matrix",
    "href": "slides/02-datascience.html#data-matrix",
    "title": "Data Science Overview 📖",
    "section": "Data Matrix",
    "text": "Data Matrix\n\nEach row corresponds to a unique case or observational unit.\nEach column represents a characteristic or variable.\nThis structure allows new cases to be added as rows or new variables as new columns.\n\n\n\nggplot2::mpg |> print(n = 10)\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   <chr>        <chr>      <dbl> <int> <int> <chr> <chr> <int> <int> <chr> <chr>\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# ℹ 224 more rows"
  },
  {
    "objectID": "slides/02-datascience.html#section-5",
    "href": "slides/02-datascience.html#section-5",
    "title": "Data Science Overview 📖",
    "section": "",
    "text": "The process of importing, tidying and transforming data is called data wrangling or data munging."
  },
  {
    "objectID": "slides/02-datascience.html#section-6",
    "href": "slides/02-datascience.html#section-6",
    "title": "Data Science Overview 📖",
    "section": "",
    "text": "Visualisation: A good visualisation shows you things that you did not expect or raise new questions about the data.\n\n\nOnce the data are ready, it’s time to explore our data.\nMy recommendation is that always plot your data before building any model or algorithm or doing analysis because good visualization let you understand more about your data, and may provide some hints about how to build your statistical model, machine learning algorithms or how to do analysis.\nAnd good visualization shows you things that you did not expect or raise new questions about the data. (outliers, missing data)\nSo after visualizing your data, you may have better idea of how to transforming your data so that you can be better prepared for modeling and answer your research questions. (nonlinear relationship <-> linear relationship)"
  },
  {
    "objectID": "slides/02-datascience.html#section-7",
    "href": "slides/02-datascience.html#section-7",
    "title": "Data Science Overview 📖",
    "section": "",
    "text": "mpg |> ggplot(aes(x = displ, y = hwy)) +\n    geom_point(aes(color = class)) + \n    geom_smooth() + \n    theme_bw()"
  },
  {
    "objectID": "slides/02-datascience.html#section-8",
    "href": "slides/02-datascience.html#section-8",
    "title": "Data Science Overview 📖",
    "section": "",
    "text": "Model: Models are complementary tools to visualisation. Once you have made your questions sufficiently precise, you can use a model to answer them.\n\n\nThe next step is to use or build a rigorous statistical or machine learning model to answer our research questions. For example, forecasting stock prices, classifying images.\nAnd transform-visualize-model becomes a loop of understanding or learning from data. We keep transforming, visualizing and modeling our data until our questions are properly answered by our data and model."
  },
  {
    "objectID": "slides/02-datascience.html#section-9",
    "href": "slides/02-datascience.html#section-9",
    "title": "Data Science Overview 📖",
    "section": "",
    "text": "library(tidymodels)\nlinear_reg() |>  \n    set_engine(\"lm\") |> \n    fit(hwy ~ displ, data = mpg)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = hwy ~ displ, data = data)\n\nCoefficients:\n(Intercept)        displ  \n      35.70        -3.53"
  },
  {
    "objectID": "slides/02-datascience.html#section-10",
    "href": "slides/02-datascience.html#section-10",
    "title": "Data Science Overview 📖",
    "section": "",
    "text": "Communication: It doesn’t matter how well your models and visualization have led you to understand the data unless you can also communicate your results to others.\n\n\nFinally we have to share our analysis results with others.\nAt this stage, visualization plays an important role as well.\nFigures/tables speak louder than words.\nWe can have different types of deployment. In addition to papers or reports, we can create a website, a software package, a mobile app, dashboard and so on."
  },
  {
    "objectID": "slides/02-datascience.html#section-11",
    "href": "slides/02-datascience.html#section-11",
    "title": "Data Science Overview 📖",
    "section": "",
    "text": "Programming: Surrounding all these tools is programming.\n\n\nOf course, every aspect of DS is done by programming. DS cannot be done by pieces of paper and pens. It has to be done using computers."
  },
  {
    "objectID": "slides/02-datascience.html#r-for-data-science",
    "href": "slides/02-datascience.html#r-for-data-science",
    "title": "Data Science Overview 📖",
    "section": "R for Data Science",
    "text": "R for Data Science\n\n\n\n\nSource: https://teachdatascience.com/tidyverse/\n\n\n\n\n\n\n\nIf you are ready, let’s dive in and learn all these tools. Well not all, but some of these tools.\nBy the way, you see these hex stickers? It kind of becomes a trend that when we create an R package, or software, we also create a hex logo associated with it, so it works as product or brand logo. It may be easier for you to promote your package if you have a cute or beautiful hex logo."
  },
  {
    "objectID": "slides/02-datascience.html#python-for-data-science",
    "href": "slides/02-datascience.html#python-for-data-science",
    "title": "Data Science Overview 📖",
    "section": "Python for Data Science",
    "text": "Python for Data Science\n\n\n\n\n\nSource: https://www.e2enetworks.com/blog/9-python-libraries-for-data-science-and-artificial-intelligence\n\n\n\n\n\n\n\n\nmath3570-s24.github.io/website"
  },
  {
    "objectID": "slides/17-logistic-reg.html#regression-vs.-classification",
    "href": "slides/17-logistic-reg.html#regression-vs.-classification",
    "title": "Logistic Regression \n",
    "section": "Regression vs. Classification",
    "text": "Regression vs. Classification\n\nLinear regression assumes that the response \\(Y\\) is numerical.\nIn many situations, \\(Y\\) is categorical!\n\n\n\n\nnormal email vs. spam email\n\n\n\n\n\n\n\n\n\n\n\nfake news vs. true news\n\n\n\n\n\n\n\n\n\n\n\n\nA process of predicting categorical response is known as classification.\n\n\neye color\ncar brand\ntrue vs. fake news"
  },
  {
    "objectID": "slides/17-logistic-reg.html#regression-function-fx-vs.-classifier-cx",
    "href": "slides/17-logistic-reg.html#regression-function-fx-vs.-classifier-cx",
    "title": "Logistic Regression \n",
    "section": "Regression Function \\(f(x)\\) vs. Classifier \\(C(x)\\)\n",
    "text": "Regression Function \\(f(x)\\) vs. Classifier \\(C(x)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: https://daviddalpiaz.github.io/r4sl/classification-overview.html\n\n\n\n\n\n\nThere are many classification tools, or classifiers used to predict a categorical response."
  },
  {
    "objectID": "slides/17-logistic-reg.html#classification-example",
    "href": "slides/17-logistic-reg.html#classification-example",
    "title": "Logistic Regression \n",
    "section": "Classification Example",
    "text": "Classification Example\n\nPredict whether people will default on their credit card payment \\((Y)\\) yes or no, based on monthly credit card balance \\((X)\\).\nWe use the training sample \\(\\{(x_1, y_1), \\dots, (x_n, y_n)\\}\\) to build a classifier."
  },
  {
    "objectID": "slides/17-logistic-reg.html#why-not-linear-regression",
    "href": "slides/17-logistic-reg.html#why-not-linear-regression",
    "title": "Logistic Regression \n",
    "section": "Why Not Linear Regression?",
    "text": "Why Not Linear Regression?\n\nMost of the time, we code categories using numbers!\n\n\\[Y =\\begin{cases}\n    0  & \\quad \\text{if not default}\\\\\n    1  & \\quad \\text{if default}\n     \\end{cases}\\]\n\n\n\\(Y = \\beta_0 + \\beta_1X + \\epsilon\\), \\(\\, X =\\) credit card balance\n\n\nAny potential issue of this dummy variable approach?\n\n\n\n\n\n\n\n\\(Y\\) is categorical but coded as dummy variable or indicator variable\nFit linear regression and treat it as a numerical variable."
  },
  {
    "objectID": "slides/17-logistic-reg.html#why-not-linear-regression-2",
    "href": "slides/17-logistic-reg.html#why-not-linear-regression-2",
    "title": "Logistic Regression \n",
    "section": "Why Not Linear Regression?",
    "text": "Why Not Linear Regression?\n\n\\(\\hat{Y} = b_0 + b_1X\\)\nSome estimates might be outside \\([0, 1]\\)."
  },
  {
    "objectID": "slides/17-logistic-reg.html#why-not-linear-regression-3",
    "href": "slides/17-logistic-reg.html#why-not-linear-regression-3",
    "title": "Logistic Regression \n",
    "section": "Why Not Linear Regression?",
    "text": "Why Not Linear Regression?\n\nFirst predict the probability of each category of \\(Y\\).\nPredict probability of default using a S-shaped curve."
  },
  {
    "objectID": "slides/17-logistic-reg.html#binary-responses-with-nonconstant-probability",
    "href": "slides/17-logistic-reg.html#binary-responses-with-nonconstant-probability",
    "title": "Logistic Regression \n",
    "section": "Binary Responses with Nonconstant Probability",
    "text": "Binary Responses with Nonconstant Probability\n\n\n\n\nTraining data \\((x_1, y_1), \\dots, (x_n, y_n)\\) where \\(y_i = 1\\) (default) or \\(0\\) (not default).\nWe first predict \\(P(y_i = 1 \\mid x_i) = \\pi(x_i) = \\pi_i\\)\nThe probability \\(\\pi\\) changes with the value of predictor \\(x\\)!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(X =\\) balance. \\(x_1 = 2000\\) has a larger \\(\\pi_1 = \\pi(2000)\\) than \\(\\pi_2 = \\pi(500)\\) with \\(x_2 = 500\\).\nCredit cards with a higher balance is more likely to be default."
  },
  {
    "objectID": "slides/17-logistic-reg.html#logistic-regression-1",
    "href": "slides/17-logistic-reg.html#logistic-regression-1",
    "title": "Logistic Regression \n",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nInstead of predicting \\(y_i\\) directly, we use predictors to model its probability of success, \\(\\pi_i\\).\n\n\nBut how?\n\n\n\n\nTransform \\(\\pi \\in (0, 1)\\) into another variable \\(\\eta \\in (-\\infty, \\infty)\\). Then construct a linear predictor on on \\(\\eta\\): \\[\\eta_i = \\beta_0 + \\beta_1 x_i\\]\n\n\n\n\n\n\nLogit function: For \\(0 < \\pi < 1\\)\n\n\n\\[\\eta = \\text{logit}(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\]\n\nThe logit function takes a value \\(\\pi \\in (0, 1)\\) and maps it to a value \\(\\eta \\in (-\\infty, \\infty)\\)."
  },
  {
    "objectID": "slides/17-logistic-reg.html#logit-function-eta-textlogitpi-lnleftfracpi1-piright",
    "href": "slides/17-logistic-reg.html#logit-function-eta-textlogitpi-lnleftfracpi1-piright",
    "title": "Logistic Regression \n",
    "section": "Logit function \\(\\eta = \\text{logit}(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\)\n",
    "text": "Logit function \\(\\eta = \\text{logit}(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\)"
  },
  {
    "objectID": "slides/17-logistic-reg.html#logistic-function",
    "href": "slides/17-logistic-reg.html#logistic-function",
    "title": "Logistic Regression \n",
    "section": "Logistic Function",
    "text": "Logistic Function\n\nThe logistic function takes a value \\(\\eta \\in (-\\infty, \\infty)\\) and maps it to a value \\(\\pi \\in (0, 1)\\).\nLogistic function: \\[\\pi = \\text{logistic}(\\eta) = \\frac{1}{1+\\exp(-\\eta)} \\in (0, 1)\\]\n\n\n\nSo once \\(\\eta\\) is estimated by the linear predictor, we use the logistic function to transform \\(\\eta\\) back to the probability."
  },
  {
    "objectID": "slides/17-logistic-reg.html#logistic-function-pi-textlogisticeta-fracexpeta1expeta",
    "href": "slides/17-logistic-reg.html#logistic-function-pi-textlogisticeta-fracexpeta1expeta",
    "title": "Logistic Regression \n",
    "section": "Logistic Function \\(\\pi = \\text{logistic}(\\eta) = \\frac{\\exp(\\eta)}{1+\\exp(\\eta)}\\)\n",
    "text": "Logistic Function \\(\\pi = \\text{logistic}(\\eta) = \\frac{\\exp(\\eta)}{1+\\exp(\\eta)}\\)"
  },
  {
    "objectID": "slides/17-logistic-reg.html#simple-logistic-regression-model",
    "href": "slides/17-logistic-reg.html#simple-logistic-regression-model",
    "title": "Logistic Regression \n",
    "section": "Simple Logistic Regression Model",
    "text": "Simple Logistic Regression Model\nFor \\(i = 1, \\dots, n\\), and with one predictor \\(X\\): \\[(Y_i \\mid X = x_i) = \\begin{cases}   1       & \\quad \\text{w/ prob } \\pi(x_i)\\\\\n    0  & \\quad \\text{w/ prob } 1 - \\pi(x_i) \\end{cases}\\] \\[\\text{logit}(\\pi_i) = \\ln \\left( \\frac{\\pi(x_i)}{1 - \\pi(x_i)} \\right) = \\eta_i = \\beta_0+\\beta_1 x_{i}\\] \n\nGoal: Get estimates \\(\\hat{\\beta}\\) and \\(\\hat{\\beta}_1\\), and therefore \\(\\hat{\\pi}_i\\)!\n\n\\[\\small \\hat{\\pi}_i = \\frac{1}{1+\\exp(-\\hat{\\beta}_0-\\hat{\\beta}_1 x_{i})}\\]\n\nwith sample size \\(n\\) and with \\(k\\) predictors, we have the logistic regression model like this\nFirst, we have a probability distribution Bernoulli describing how the outcome or response data are generated.\n\n\n\\(Y_i \\mid {\\bf x}_i; \\pi_i \\sim \\text{Bern}(p_i)\\), \\({\\bf x}_i = (x_{1,i}, \\cdots, x_{k,i})\\), \\(i = 1, \\dots, n\\)\n\n\n\nThen we have a link function, logit function, that relates the linear regression to the parameter of the outcome distribution, which is the parameter \\(p\\), the probability of success in the Bernoulli distribution.\n\n\\(\\text{logit}(\\pi_i) = \\eta_i = \\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i}\\)"
  },
  {
    "objectID": "slides/17-logistic-reg.html#probability-curve",
    "href": "slides/17-logistic-reg.html#probability-curve",
    "title": "Logistic Regression \n",
    "section": "Probability Curve",
    "text": "Probability Curve\n\n\n\nThe relationship between \\(\\pi(x)\\) and \\(x\\) is not linear! \\[\\pi(x) = \\frac{1}{1+\\exp(-\\beta_0-\\beta_1 x)}\\]\n\nThe amount that \\(\\pi(x)\\) changes due to a one-unit change in \\(x\\) depends on the current value of \\(x\\).\nRegardless of the value of \\(x\\), if \\(\\beta_1 > 0\\), increasing \\(x\\) will be increasing \\(\\pi(x)\\)."
  },
  {
    "objectID": "slides/17-logistic-reg.html#fit-logistic-regression",
    "href": "slides/17-logistic-reg.html#fit-logistic-regression",
    "title": "Logistic Regression \n",
    "section": "Fit Logistic Regression",
    "text": "Fit Logistic Regression\n\n\n\nbodydata <- read_csv(\"./data/body.csv\")\nbody <- bodydata |> \n    select(GENDER, HEIGHT) |> \n    mutate(GENDER = as.factor(GENDER))\nbody |> slice(1:4) \n\n# A tibble: 4 × 2\n  GENDER HEIGHT\n  <fct>   <dbl>\n1 0        172 \n2 1        186 \n3 0        154.\n4 1        160.\n\n\n\n\nGENDER = 1 if male\n\nGENDER = 0 if female\nUse HEIGHT (centimeter, 1 cm = 0.39 in) to predict/classify GENDER: whether one is male or female.\n\n\n\n\n\n\n\nSource: https://www.thetealmango.com/featured/average-male-and-female-height-worldwide/"
  },
  {
    "objectID": "slides/17-logistic-reg.html#logistic-regression---data-summary",
    "href": "slides/17-logistic-reg.html#logistic-regression---data-summary",
    "title": "Logistic Regression \n",
    "section": "Logistic Regression - Data Summary",
    "text": "Logistic Regression - Data Summary\n\ntable(body$GENDER)\n\n\n  0   1 \n147 153 \n\n\n\nbody |> ggplot(aes(x = GENDER, y = HEIGHT)) + geom_boxplot()"
  },
  {
    "objectID": "slides/17-logistic-reg.html#logistic-regression---model-fitting",
    "href": "slides/17-logistic-reg.html#logistic-regression---model-fitting",
    "title": "Logistic Regression \n",
    "section": "Logistic Regression - Model Fitting",
    "text": "Logistic Regression - Model Fitting\n\nSpecify the model with logistic_reg() \nUse \"glm\" instead of \"lm\" as the engine\n\n\nlibrary(tidymodels)\nlogis_mdl <- parsnip::logistic_reg() |> \n    set_engine(\"glm\") \n\n\n\nDefine family = \"binomial\"\n\n\n\nlogis_out <- logis_mdl |> \n    fit(GENDER ~ HEIGHT, \n        data = body, \n        family = \"binomial\")\n\n\n\n\nlogis_out$fit$coefficients\n\n(Intercept)      HEIGHT \n    -40.548       0.242"
  },
  {
    "objectID": "slides/17-logistic-reg.html#prgender-1-when-height-is-170-cm",
    "href": "slides/17-logistic-reg.html#prgender-1-when-height-is-170-cm",
    "title": "Logistic Regression \n",
    "section": "Pr(GENDER = 1) When HEIGHT is 170 cm",
    "text": "Pr(GENDER = 1) When HEIGHT is 170 cm\n\n\\(\\hat{\\eta} = \\ln \\left( \\frac{\\hat{\\pi}}{1 - \\hat{\\pi}}\\right) = -40.55 + 0.24 \\times \\text{HEIGHT}\\)\n\n\\[ \\hat{\\pi}(x = 170) = \\frac{1}{1+\\exp(-\\hat{\\beta}_0-\\hat{\\beta}_1 x)} = \\frac{1}{1+\\exp(-(-40.55) - 0.24 \\times 170)} = 63.3\\%\\]\n\npredict(logis_out$fit, newdata = data.frame(HEIGHT = 170), type = \"response\")\n\n    1 \n0.633"
  },
  {
    "objectID": "slides/17-logistic-reg.html#probability-curve-1",
    "href": "slides/17-logistic-reg.html#probability-curve-1",
    "title": "Logistic Regression \n",
    "section": "Probability Curve",
    "text": "Probability Curve\n\npi_hat <- predict(logis_out$fit, type = \"response\")\npi_hat |> head()\n\n     1      2      3      4      5      6 \n0.7369 0.9880 0.0383 0.1480 0.9383 0.4375 \n\nbody$HEIGHT |> head()\n\n[1] 172 186 154 160 179 167\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n160 cm, Pr(male) = 0.13\n170 cm, Pr(male) = 0.63\n180 cm, Pr(male) = 0.95"
  },
  {
    "objectID": "slides/17-logistic-reg.html#sklearn.linear_model.logisticregression",
    "href": "slides/17-logistic-reg.html#sklearn.linear_model.logisticregression",
    "title": "Logistic Regression \n",
    "section": "sklearn.linear_model.LogisticRegression",
    "text": "sklearn.linear_model.LogisticRegression\n\nlibrary(reticulate); py_install(\"scikit-learn\")\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\n\n\n\n\nbody = pd.read_csv('./data/body.csv')\nx = np.array(body[['HEIGHT']]) ## 2d array with one column\ny = np.array(body['GENDER']) ## 1d array\n\n\nx[0:4]\n\narray([[172. ],\n       [186. ],\n       [154.4],\n       [160.5]])\n\ny[0:4]\n\narray([0, 1, 0, 1])"
  },
  {
    "objectID": "slides/17-logistic-reg.html#sklearn.linear_model.logisticregression-1",
    "href": "slides/17-logistic-reg.html#sklearn.linear_model.logisticregression-1",
    "title": "Logistic Regression \n",
    "section": "sklearn.linear_model.LogisticRegression",
    "text": "sklearn.linear_model.LogisticRegression\n\nclf = LogisticRegression().fit(x, y)\nclf.coef_\n\narray([[0.24154889]])\n\nclf.intercept_\n\narray([-40.51727266])\n\n\n\n\n\nnew_height = np.array([160, 170, 180]).reshape(-1, 1)\n\n\nclf.predict(new_height)\n\narray([0, 1, 1])\n\n\n\n\n\n\nclf.predict_proba(new_height)\n\narray([[0.86639467, 0.13360533],\n       [0.36678399, 0.63321601],\n       [0.04919451, 0.95080549]])"
  },
  {
    "objectID": "slides/17-logistic-reg.html#evaluation-metrics",
    "href": "slides/17-logistic-reg.html#evaluation-metrics",
    "title": "Logistic Regression \n",
    "section": "Evaluation Metrics1\n",
    "text": "Evaluation Metrics1\n\n\nConfusion Matrix\n\n\n\n\n\n\n\n\n\nTrue 0\nTrue 1\n\n\n\nPredict 0\nTrue Negative (TN)\nFalse Negative (FN)\n\n\nPredict 1\nFalse Positive (FP)\nTrue Positive (TP)\n\n\n\n\nSensitivity (True Positive Rate) \\(= P( \\text{predict 1} \\mid \\text{true 1}) = \\frac{TP}{TP+FN}\\)\nSpecificity (True Negative Rate) \\(= P( \\text{predict 0} \\mid \\text{true 0}) = \\frac{TN}{FP+TN}\\)\nAccuracy \\(= \\frac{TP + TN}{TP+FN+FP+TN}\\)\n\n\nA good classifier is one which the test accuracy rate is highest.\n\nMore on Wiki page."
  },
  {
    "objectID": "slides/17-logistic-reg.html#confusion-matrix",
    "href": "slides/17-logistic-reg.html#confusion-matrix",
    "title": "Logistic Regression \n",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\npred_prob <- predict(logis_out$fit, type = \"response\")\n\n## true observations\ngender_true <- body$GENDER\n\n## predicted labels\ngender_predict <- (pred_prob > 0.5) * 1\n\n## confusion matrix\ntable(gender_predict, gender_true)\n\n              gender_true\ngender_predict   0   1\n             0 118  29\n             1  29 124"
  },
  {
    "objectID": "slides/17-logistic-reg.html#receiver-operating-characteristic-roc-curve",
    "href": "slides/17-logistic-reg.html#receiver-operating-characteristic-roc-curve",
    "title": "Logistic Regression \n",
    "section": "Receiver Operating Characteristic (ROC) Curve",
    "text": "Receiver Operating Characteristic (ROC) Curve\n\n\n\n\n\n\n\n\nTrue 0\nTrue 1\n\n\n\nPredict 0\nTrue Negative (TN)\nFalse Negative (FN)\n\n\nPredict 1\nFalse Positive (FP)\nTrue Positive (TP)\n\n\n\n\n\nReceiver operating characteristic (ROC) curve plots True Positive Rate (Sensitivity) vs. False Positive Rate (1 - Specificity)"
  },
  {
    "objectID": "slides/17-logistic-reg.html#comparing-models",
    "href": "slides/17-logistic-reg.html#comparing-models",
    "title": "Logistic Regression \n",
    "section": "Comparing Models",
    "text": "Comparing Models\n\nWhich model performs better?"
  },
  {
    "objectID": "slides/17-logistic-reg.html#section-2",
    "href": "slides/17-logistic-reg.html#section-2",
    "title": "Logistic Regression \n",
    "section": "",
    "text": "21-Logistic Regression \nIn lab.qmd ## Lab 21 section,\n\nUse our fitted logistic regression model to predict whether you are male or female! Change 175 to your height (cm).\nUse the converter to get your height in cm!\n\n\n# Fit the logistic regression\n\npredict(logis_out$fit, newdata = data.frame(HEIGHT = 175), \n        type = \"response\")\n\n    1 \n0.853 \n\n\n\n\n\n\n\n\n\nmath3570-s24.github.io/website"
  },
  {
    "objectID": "slides/11-interactive-viz.html#boring-static-ggplot",
    "href": "slides/11-interactive-viz.html#boring-static-ggplot",
    "title": "Interactive Visualization 📈",
    "section": "Boring Static ggplot",
    "text": "Boring Static ggplot\n\n\nJust a scatter plot showing the relationship between hwy and displ. And that’s it.\nIt cannot provide any other information or functionality."
  },
  {
    "objectID": "slides/11-interactive-viz.html#informative-inteactive-ggplot",
    "href": "slides/11-interactive-viz.html#informative-inteactive-ggplot",
    "title": "Interactive Visualization 📈",
    "section": "Informative Inteactive ggplot",
    "text": "Informative Inteactive ggplot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow about this!\nWe can actually create a more informative Inteactive ggplot with more functionality!\nYou see here, if we put our cursor on any data point, the exact value of hwy and displ will be shown ot us.\nAlso, several useful funcitons are shown on the top of the plot.\nWe can download the plot.\nWe can zoom the plot.\nWe can drag the plot.\nWe can also select any data points we are interested and like to further investigate.\nNotice that the interactive data table are automatically changed to show the selected data point information.\nHow cool is that!\nhttps://www.rebeccabarter.com/blog/2017-04-20-interactive/"
  },
  {
    "objectID": "slides/11-interactive-viz.html#plotly",
    "href": "slides/11-interactive-viz.html#plotly",
    "title": "Interactive Visualization 📈",
    "section": "\nPlotly 📦",
    "text": "Plotly 📦\n\nTo create a plotly object\n\ndirectly initializing a plotly object with plot_ly(), plot_geo(), etc.\ntransforming a ggplot2 object via ggplotly() into a plotly object\n\n\nBoth are powered by the JavaScript graphing library plotly.js, so many of the same concepts and tools for one interface can be reused in the other.\n\n\nBoth approaches have somewhat complementary strengths and weaknesses, so it can pay off to learn both approaches. Moreover, both approaches are an implementation of the Grammar of Graphics and both are powered by the JavaScript graphing library plotly.js"
  },
  {
    "objectID": "slides/11-interactive-viz.html#plot_ly",
    "href": "slides/11-interactive-viz.html#plot_ly",
    "title": "Interactive Visualization 📈",
    "section": "plot_ly()",
    "text": "plot_ly()\n\nMapping homeownership to x yields a bar chart\n\n\n\nlibrary(plotly)\nloans <- readr::read_csv(\"./data/loans.csv\")\nloans |> plot_ly(x = ~homeownership) \n\n\n\n\n# plot_ly(loans, x = loans$homeownership)\n# plot_ly(loans, x = ~homeownership, type = \"histogram\")\n\nplotly tries to find a sensible geometric representation of that information for us plot_ly(loans, x = ~homeownership, type = “histogram”) ‘bar’, ‘barpolar’, ‘box’, ‘candlestick’, ‘carpet’, ‘choropleth’, ‘choroplethmapbox’, ‘cone’, ‘contour’, ‘contourcarpet’, ‘densitymapbox’, ‘funnel’, ‘funnelarea’, ‘heatmap’, ‘heatmapgl’, ‘histogram’, ‘histogram2d’, ‘histogram2dcontour’, ‘icicle’, ‘image’, ‘indicator’, ‘isosurface’, ‘mesh3d’, ‘ohlc’, ‘parcats’, ‘parcoords’, ‘pie’, ‘pointcloud’, ‘sankey’, ‘scatter’, ‘scatter3d’, ‘scattercarpet’, ‘scattergeo’, ‘scattergl’, ‘scattermapbox’, ‘scatterpolar’, ‘scatterpolargl’, ‘scatterternary’, ‘splom’, ‘streamtube’, ‘sunburst’, ‘surface’, ‘table’, ‘treemap’, ‘violin’, ‘volume’, ‘waterfall’"
  },
  {
    "objectID": "slides/11-interactive-viz.html#plot_ly-1",
    "href": "slides/11-interactive-viz.html#plot_ly-1",
    "title": "Interactive Visualization 📈",
    "section": "plot_ly()",
    "text": "plot_ly()\n\nloans |> plot_ly(x = ~homeownership, color = I(\"pink\"), stroke = I(\"green3\"), span = I(5))\n\n\n\n\n\n\nWe can specify the bar color we like using color argument.\nNote that we cannot simply use the name of color as we do in R plotting, but have to add the function I().\nIt is because when we call the plot_ly(), it actually translate the R code into Javascript, and for some reason, we need to add I() to make the code work.\nIf you like to change the border color of the bar, you can specify a color in the stroke argument.\nSpan controls the width of the border. # doesn’t produce black bars plot_ly(loans, x = ~homeownership, color = “black”)"
  },
  {
    "objectID": "slides/11-interactive-viz.html#plot_ly-2",
    "href": "slides/11-interactive-viz.html#plot_ly-2",
    "title": "Interactive Visualization 📈",
    "section": "plot_ly()",
    "text": "plot_ly()\n\nMapping homeownership & grade to x & y yields a heatmap\n\n\n\nloans |> plot_ly(x = ~homeownership, y = ~grade)\n\n\n\n\n# plot_ly(loans, x = ~homeownership, y = ~grade, type = \"histogram2d\")\n\n\nColor in cells represents the the count of some type of homeownership and the grade of loans.\nFor example, we know there are 1285 loans that is classified as grade A, and the loan applicant has a mortgage.\nThe most frequency is grade B and mortgage."
  },
  {
    "objectID": "slides/11-interactive-viz.html#plot_ly-3",
    "href": "slides/11-interactive-viz.html#plot_ly-3",
    "title": "Interactive Visualization 📈",
    "section": "plot_ly()",
    "text": "plot_ly()\n\nMapping homeownership & grade to x & color yields a dodged bar chart \n\n\n\nloans |> plot_ly(x = ~homeownership, color = ~grade)\n\n\n\n\n# plot_ly(loans, x = ~homeownership, color = ~grade, type = \"histogram\")\n\n\nWe actually create a bar chart of grade of loan separated by homeownership, and each bar has a color corresponding to a grade of loan.\n\n\n\n\nloans |> plot_ly(x = ~homeownership, \n        color = ~grade, colors = \"Accent\")"
  },
  {
    "objectID": "slides/11-interactive-viz.html#layout",
    "href": "slides/11-interactive-viz.html#layout",
    "title": "Interactive Visualization 📈",
    "section": "layout()",
    "text": "layout()\n\nThe 1st argument is a plotly object.\nOther arguments include legend, margins, size, etc.\n\n\nplotly::layout(p = plot_ly(loans, x = ~homeownership),\n               title = \"My beatiful bar chart\")\n\n\n\n\n\n\nIf we want to decorate our plot, in plotly, we use the layout function.\nYou can also change the X-label and Y-label in the layout function.\n\nlayout( # all of layout’s properties: /r/reference/#layout title = “Unemployment”, # layout’s title: /r/reference/#layout-title xaxis = list( # layout’s xaxis is a named list. List of valid keys: /r/reference/#layout-xaxis title = “Time”, # xaxis’s title: /r/reference/#layout-xaxis-title showgrid = F), # xaxis’s showgrid: /r/reference/#layout-xaxis-showgrid yaxis = list( # layout’s yaxis is a named list. List of valid keys: /r/reference/#layout-yaxis title = “uidx”) # yaxis’s title: /r/reference/#layout-yaxis-title )"
  },
  {
    "objectID": "slides/11-interactive-viz.html#add_-functions",
    "href": "slides/11-interactive-viz.html#add_-functions",
    "title": "Interactive Visualization 📈",
    "section": "add_*() Functions",
    "text": "add_*() Functions\n\nDefine how to render data into geometric objects, add_contour(), add_boxplot(), etc.\n\nadd_markers() for scatterplots.\n\n\n# plot_ly(mpg, x = ~cty, y = ~hwy, type = \"scatter\", mode = \"markers\", alpha = 0.5)\nbase_plot <- mpg |> plot_ly(x = ~cty, y = ~hwy)\nbase_plot |> add_markers(alpha = 0.7, size = 2)\n\n\n\n\n\n\nOne advantage of using add_* family functions is that, if we haven’t decided what kind of plot we’d like to generate, we can first create a base plotly object, and then add any geometric object we like later.\nWe can try several geometric objects, and choose our favorite one. plot_ly(mpg, x = ~cty, y = ~hwy, type = “scatter”, mode = “markers”, alpha = 0.5)"
  },
  {
    "objectID": "slides/11-interactive-viz.html#add_paths",
    "href": "slides/11-interactive-viz.html#add_paths",
    "title": "Interactive Visualization 📈",
    "section": "add_paths()",
    "text": "add_paths()\n\nbase_plot |> add_paths()\n\n\n\n\n\n\nHere we add another geometry path. You can see that using point markers makes more sense in this case.\nBut the point is, we can repeatedly using the same base"
  },
  {
    "objectID": "slides/11-interactive-viz.html#color",
    "href": "slides/11-interactive-viz.html#color",
    "title": "Interactive Visualization 📈",
    "section": "Color",
    "text": "Color\n\n## alpha here is \"setting\", not \"mapping\"\np <- mpg |> plot_ly(x = ~cty, y = ~hwy, alpha = 0.7) \np |> add_markers(color = ~factor(cyl), size = 3)\n\n\n\n\n\n\nmap the number of cylinders to point color.\nBecause the variable is actually numerical, I make it categorical as a factor before doing the mapping."
  },
  {
    "objectID": "slides/11-interactive-viz.html#symbols",
    "href": "slides/11-interactive-viz.html#symbols",
    "title": "Interactive Visualization 📈",
    "section": "Symbols",
    "text": "Symbols\n\np |> add_markers(symbol = ~factor(cyl), color = I(\"red\"))\n\n\n\n\n\n\nWe can also use different symbols or point shapes to represent different number of cylinders. Very simple, we just use the symbol argument instead.\nIn ggplot2, we use “shape”"
  },
  {
    "objectID": "slides/11-interactive-viz.html#add_lines",
    "href": "slides/11-interactive-viz.html#add_lines",
    "title": "Interactive Visualization 📈",
    "section": "add_lines()",
    "text": "add_lines()\n\n\nPlot\nCode\n\n\n\n\np <- tx5 |> plot_ly(x = ~date, y = ~median)\np |> add_lines(linetype = ~city)\n\n\n\n\n\n\n\n\n\n\ntop5 <- txhousing |> \n    group_by(city) |> \n    summarise(m = mean(sales, na.rm = TRUE)) |> \n    arrange(desc(m)) |> \n    top_n(5)\ntx5 <- semi_join(txhousing, top5, by = \"city\")\np <- tx5 |> plot_ly(x = ~date, y = ~median)\np |> add_lines(linetype = ~city)\n\n\n\n\n\nWe can also create a line plot. It is useful if you have some time series data.\nHere I create a data called tx5, and I am trying see the trend of the median home price of the 5 cities in Texas.\nFirst, we can create plotly object, assigning the x and y variables, which are date and median.\nThen we can add the line plot, one linetype for one city."
  },
  {
    "objectID": "slides/11-interactive-viz.html#ggplotly",
    "href": "slides/11-interactive-viz.html#ggplotly",
    "title": "Interactive Visualization 📈",
    "section": "ggplotly()",
    "text": "ggplotly()\n\n\nplotly::ggplotly() translate ggplot2 to plotly.\n\n\np <- loans |> \n    ggplot(aes(x = debt_to_income, y = interest_rate)) +\n    geom_point(alpha = 0.5) +\n    theme_bw()\nggplotly(p)\n\n\n\n\n\n\nIf you think the syntax is hard and not intuitive, and you love using ggplot, well here is good news.\nThe function ggplotly() translate ggplot2 to plotly.\nYou just create your ggplot, save it as an object, and then plug the object into the ggplotly function.\nBang! your ggplot becomes interactive!\nHow cool is that! Very cool and convenient!\nThat’s one of the reasons why we learn ggplot!\nBut so far the translation is not 100 percent working."
  },
  {
    "objectID": "slides/11-interactive-viz.html#ggplotly-1",
    "href": "slides/11-interactive-viz.html#ggplotly-1",
    "title": "Interactive Visualization 📈",
    "section": "ggplotly()",
    "text": "ggplotly()\n\nEasily compare levels of grade (loan quality) by leveraging the legend filtering capabilities.\n\n\np <- loans |> ggplot(aes(x = log(debt_to_income), color = grade)) + \n    geom_freqpoly(stat = \"density\") + xlim(0, 5) + theme_bw()\nggplotly(p)"
  },
  {
    "objectID": "slides/11-interactive-viz.html#section-3",
    "href": "slides/11-interactive-viz.html#section-3",
    "title": "Interactive Visualization 📈",
    "section": "",
    "text": "14-plotly (Present your work!) \nIn lab.qmd ## Lab 14 section,\n\nLoad tidyverse and plotly and the loans.csv data.\nGenerate a plot using plotly. An example is shown below. Welcome to create a more fancy one!\n\n\n\n\n\n\n\n\n\n# x = interest_rate, y = grade won't work\ngg <- loans %>% ggplot(aes(x = grade, y = interest_rate, color = grade)) + \n    geom_boxplot() + theme_minimal() + coord_flip()\nggplotly(gg)"
  },
  {
    "objectID": "slides/11-interactive-viz.html#dumbell-chart",
    "href": "slides/11-interactive-viz.html#dumbell-chart",
    "title": "Interactive Visualization 📈",
    "section": "Dumbell Chart",
    "text": "Dumbell Chart\n\n\nPlot\nCode\n\n\n\n\n\n\n\n\n\n\n\n\nmpg |> \n    group_by(model) |> \n    summarise(c = mean(cty), h = mean(hwy)) |> \n    mutate(model = forcats::fct_reorder(model, c)) |> \n    plot_ly() |> \n    add_segments(x = ~c, y = ~model,\n                 xend = ~h, yend = ~model, \n                 color = I(\"gray\"), showlegend = FALSE) |> \n    add_markers(x = ~c, y = ~model, \n                color = I(\"blue\"), \n                name = \"mpg city\") |> \n    add_markers(x = ~h, y = ~model, \n                color = I(\"red\"),\n                name  = \"mpg highway\") |> \n    plotly::layout(xaxis = list(title = \"Miles per gallon\"))"
  },
  {
    "objectID": "slides/11-interactive-viz.html#maps",
    "href": "slides/11-interactive-viz.html#maps",
    "title": "Interactive Visualization 📈",
    "section": "Maps",
    "text": "Maps\n\n\nPlot\nCode\n\n\n\n\n\n\n\n\n\n\n\n\npop_den <- datasets::state.x77[, \"Population\"] / state.x77[, \"Area\"]\n\ng <- list(scope = 'usa',\n          projection = list(type = 'albers usa'),\n          lakecolor = toRGB('white'))\n\nplot_geo() |> \n    add_trace(z = ~pop_den, text = state.name, span = I(0),\n              locations = state.abb, locationmode = 'USA-states') |> \n    plotly::layout(geo = g)"
  },
  {
    "objectID": "slides/11-interactive-viz.html#d-scatterplots",
    "href": "slides/11-interactive-viz.html#d-scatterplots",
    "title": "Interactive Visualization 📈",
    "section": "3D Scatterplots",
    "text": "3D Scatterplots\n\nmpg |> plot_ly(x = ~cty, y = ~hwy, z = ~cyl) |> \n    add_markers(color = ~factor(cyl))"
  },
  {
    "objectID": "slides/11-interactive-viz.html#d-surfaces",
    "href": "slides/11-interactive-viz.html#d-surfaces",
    "title": "Interactive Visualization 📈",
    "section": "3D Surfaces",
    "text": "3D Surfaces\n\nx <- 1:nrow(datasets::volcano); y <- 1:ncol(datasets::volcano)\nplot_ly() |> add_surface(x = ~x, y = ~y, z = ~volcano)"
  },
  {
    "objectID": "slides/11-interactive-viz.html#animations-using-frames",
    "href": "slides/11-interactive-viz.html#animations-using-frames",
    "title": "Interactive Visualization 📈",
    "section": "Animations using Frames",
    "text": "Animations using Frames\n\nlibrary(gapminder)\np <- gapminder |> ggplot(aes(gdpPercap, lifeExp, color = continent)) +\n  geom_point(aes(size = pop, frame = year, ids = country)) +\n  scale_x_log10()\n\nggplotly(p)\n\n\n\n\n\nNow, along with data and layout, frames is added to the keys that figure allows. Your frames key points to a list of figures, each of which will be cycled through upon instantiation of the plot."
  },
  {
    "objectID": "slides/11-interactive-viz.html#dynamic-bar-chart-using-gganimate",
    "href": "slides/11-interactive-viz.html#dynamic-bar-chart-using-gganimate",
    "title": "Interactive Visualization 📈",
    "section": "Dynamic Bar Chart using gganimate\n",
    "text": "Dynamic Bar Chart using gganimate\n\n\n\n\n\n\nSource: https://github.com/amrrs/animated_bar_charts_in_R\n\n\n\n\n\n\n\n\nmath3570-s24.github.io/website"
  },
  {
    "objectID": "slides/01-syllabus.html#my-journey",
    "href": "slides/01-syllabus.html#my-journey",
    "title": "Welcome Aboard! 🙌",
    "section": "My Journey",
    "text": "My Journey\n\nAssistant Professor (2020/08 - )\n\n\n\n\n\n\n\n\n\n\nPostdoctoral Fellow\n\n\n\n\n\n\n\n\n\n\nPhD in Statistics\n\n\n\n\n\n\n\n\n\n\nMA in Economics/PhD program in Statistics\n\n\n\n\n\n\n\n\n\nAfter college, working and doing military service for several years, I came to the US for my PhD degree. Originally I would like to study economics, but then I switched my major to statistics.\n\nI got my master degree in economics from Indiana University Bloomington, then I transferred to UC Santa Cruz to finish my PhD studies.\nThen I spent two years doing my postdoctoral research at Rice University in Houston, Texas.\nFinally, in fall 2020, I came to Marquette as an assistant professor.\nMidwest/Indiana-West/California-South/Texas-Midwest/Wisconsin\nBeen to any one of these universities/cities?\nThe most beautiful campus.\nWho are international students? I can totally understand how hard studying and living in another country. Feel free to share your stories or difficulties, and I am more than happy to help you if you have any questions.\nPoor listening and speaking skills. I was shy.\nOK so, this is my background. How about you introducing yourself as well. You can share anything, your major, hobbies, your favorite food, what do you want to do after graduation, anything,\nI have the class list. I’d like to learn your face and remember your name. You know, you all wear a mask. It’s hard to recognize you and connect your name and your face.\nWhen I call your name, you can say something about yourself. No need to be long, couple of seconds are good."
  },
  {
    "objectID": "slides/01-syllabus.html#how-to-reach-me",
    "href": "slides/01-syllabus.html#how-to-reach-me",
    "title": "Welcome Aboard! 🙌",
    "section": "How to Reach Me",
    "text": "How to Reach Me\n\nOffice hours TuTh 4:50 - 5:50 PM and Wed 12 - 1 PM in Cudahy Hall 353.\n📧 cheng-han.yu@marquette.edu\n\nAnswer your question within 24 hours.\nExpect a reply on Monday if shoot me a message on weekends.\nStart your subject line with [math3570] or [cosc3570] followed by a clear description of your question.\n\n\n\n\n\nI will NOT reply your e-mail if … Check the email policy in the syllabus!"
  },
  {
    "objectID": "slides/01-syllabus.html#when-you-have-two-dr.-yu-in-one-semester",
    "href": "slides/01-syllabus.html#when-you-have-two-dr.-yu-in-one-semester",
    "title": "Welcome Aboard! 🙌",
    "section": "When You Have Two Dr. Yu in One Semester 😂",
    "text": "When You Have Two Dr. Yu in One Semester 😂\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: - This is a real case happened last semester. A student of my class sent her homework of another class linear systems to me because the instructor of that course also has last name Yu, and she messed up. So if you remember to add course number in the subject line, it will greatly reduce the possibility of sending a wrong message. And it will save your time and my time, and so we all can work more efficiently. Right? :::"
  },
  {
    "objectID": "slides/01-syllabus.html#what-is-this-course",
    "href": "slides/01-syllabus.html#what-is-this-course",
    "title": "Welcome Aboard! 🙌",
    "section": "What is This Course?",
    "text": "What is This Course?\n\n\nEvery aspect of doing a practical data science project, from importing data to deploying what we learn from data.\n\n\n❓ What are prerequisites?\n👉 COSC 1010 (Intro Programming) and MATH 4720 (Intro Stats) or MATH 2780 (Intro Regression)\n\n\n\n❓ Is this like another intro stats course?\n👉 No. Statistics and data science are closely related.\n\n\nNowadays\n👉 Data science is a broader subject than statistics.\n\n\n👉 Statistics focuses more on analyzing and learning from data, a part of the entire workflow of data science.\n\n\n\n❓ Is this like another intro CS or programming course?\n👉 Absolutely not. We learn how to code for doing data science, not for understanding computer systems and structures.\n\nOf course we will not cover every detail form A to Z of DS in one semester becuase DS is such a huge subject, just like computer science. It’s impossible to learn CS in one semester by just taking intro to CS, right? So we’ll just learn the basic and essential concepts and tools used is DS."
  },
  {
    "objectID": "slides/01-syllabus.html#what-is-not-covered-in-this-course",
    "href": "slides/01-syllabus.html#what-is-not-covered-in-this-course",
    "title": "Welcome Aboard! 🙌",
    "section": "What is NOT Covered in This Course",
    "text": "What is NOT Covered in This Course\n\n\nAdvanced data analytics and computing  \n\nMATH 4750 Statistical Computing\nMATH 4760 Time Series Analysis\nMATH 4780 Regression Analysis\nMATH 4790 Bayesian Statistics\nCOSC 4600 Fundamentals of Artificial Intelligence\nCOSC 4610 Data Mining\nCOEN 4860 Introduction to Neural Networks\n\n\n\n\n\n\nBig data: We start with small, in-memory data sets. You don’t know how to tackle big data unless you have experience with small data.\n\n\n\n\n\nDatabase: You’ll learn SQL in\n\nCOSC 4800 Principles of Database Systems\nINSY 4052 Database Management Systems.\n\n\n\n\nWe won’t talk about topics like cloud computing, parallel computing, distributed computing.\n\nBig data techniques: As a beginner, we focus on small, in-memory datasets. You can’t tackle big data unless you have experience with small data."
  },
  {
    "objectID": "slides/01-syllabus.html#what-computing-languages",
    "href": "slides/01-syllabus.html#what-computing-languages",
    "title": "Welcome Aboard! 🙌",
    "section": "What Computing Languages?",
    "text": "What Computing Languages?\n\n\n~ 70%\n\n\n\n\n\n\n\n\n\n~ 30%\n\n\n\n\n\n\n\n\n\n\n\nYou’ve learned Python in COSC 1010. Being R-Python bilingual is getting more important!\n\n\n👉 Wouldn’t it be great to add both languages to your resume! 😎 \n\n\n❌ If you do NOT want to learn R and/or Python, do NOT take this course! (3570 is offered every semester)\n❌ Drop deadline: 01/24/2024 11:59 PM."
  },
  {
    "objectID": "slides/01-syllabus.html#course-materials",
    "href": "slides/01-syllabus.html#course-materials",
    "title": "Welcome Aboard! 🙌",
    "section": "Course Materials",
    "text": "Course Materials\n\nCourse materials are grabbed from several books and resources.\nOur course website and slides would be helpful.\n\n\n\n\nR for Data Science 2nd edition\n\n\n\n\n\n\n\n\n\nTidy Modeling with R\n\n\n\n\n\n\n\n\n\nPython for Data Analysis, 3E"
  },
  {
    "objectID": "slides/01-syllabus.html#course-website---httpsmath3570-s24.github.iowebsite",
    "href": "slides/01-syllabus.html#course-website---httpsmath3570-s24.github.iowebsite",
    "title": "Welcome Aboard! 🙌",
    "section": "Course Website - https://math3570-s24.github.io/website/\n",
    "text": "Course Website - https://math3570-s24.github.io/website/\n\n\nAll course materials"
  },
  {
    "objectID": "slides/01-syllabus.html#learning-management-system-d2l",
    "href": "slides/01-syllabus.html#learning-management-system-d2l",
    "title": "Welcome Aboard! 🙌",
    "section": "Learning Management System (D2L)",
    "text": "Learning Management System (D2L)\n\n\n\n\n\n\n\nNews\nAssessments > Grades"
  },
  {
    "objectID": "slides/01-syllabus.html#grading-policy",
    "href": "slides/01-syllabus.html#grading-policy",
    "title": "Welcome Aboard! 🙌",
    "section": "Grading Policy ✨",
    "text": "Grading Policy ✨\n\n40% In-class lab exercises\n30% Homework\n30% Final project competition\nExtra credit opportunities\n\n\n❌ You have to participate (in-person) in the final presentation in order to pass the course.\n❌ You will NOT be allowed to do any extra credit projects/homework/exam to compensate for a poor grade."
  },
  {
    "objectID": "slides/01-syllabus.html#grade-percentage-conversion",
    "href": "slides/01-syllabus.html#grade-percentage-conversion",
    "title": "Welcome Aboard! 🙌",
    "section": "Grade-Percentage Conversion",
    "text": "Grade-Percentage Conversion\n\n\n\\([x, y)\\) means greater than or equal to \\(x\\) and less than \\(y\\). For example, 94.0 is in [94, 100] and the grade is A and 93.8 is in [90, 94) and the grade is A-.\n\n\n\n\n\n\n Grade \n    Percentage \n  \n\n\n A \n    [94, 100] \n  \n\n A- \n    [90, 94) \n  \n\n B+ \n    [87, 90) \n  \n\n B \n    [84, 87) \n  \n\n B- \n    [80, 84) \n  \n\n C+ \n    [77, 80) \n  \n\n C \n    [74, 77) \n  \n\n C- \n    [70, 74) \n  \n\n D+ \n    [65, 70) \n  \n\n D \n    [60, 65) \n  \n\n F \n    [0, 60)"
  },
  {
    "objectID": "slides/01-syllabus.html#lab-exercises-40",
    "href": "slides/01-syllabus.html#lab-exercises-40",
    "title": "Welcome Aboard! 🙌",
    "section": "Lab Exercises (40%)",
    "text": "Lab Exercises (40%)\n\nGraded as Complete/Incomplete and used as evidence of attendance and participation.\nYou are allowed to have two incomplete lab exercises without any penalty.\nBeyond that, 2% grade percentage will be taken off for each missing/incomplete exercise.\nYou will create a RStudio project in Posit Cloud saving all of your lab exercises. (We’ll go through know-how together)\n❌ No make-up lab exercises for any reason."
  },
  {
    "objectID": "slides/01-syllabus.html#homework-30",
    "href": "slides/01-syllabus.html#homework-30",
    "title": "Welcome Aboard! 🙌",
    "section": "Homework (30%)",
    "text": "Homework (30%)\n\nThe homework assignments are individual. Submit your own work.\n❌ You may not directly share or discuss answers/code with anyone other than the instructor. But you are welcome to discuss the problems in general and ask for advice.\n\n\n\nHomework will be assigned through GitHub:\n\nclone/pull the homework repo into Posit Cloud\nwork on the Quarto file in the repo (We’ll go through know-how together)\n\n\n\n\n\n\nYou will have at least one week to complete your assignment.\n❌ No make-up homework for any reason unless you got COVID or excused absence. 🙏"
  },
  {
    "objectID": "slides/01-syllabus.html#project-30",
    "href": "slides/01-syllabus.html#project-30",
    "title": "Welcome Aboard! 🙌",
    "section": "Project (30%)",
    "text": "Project (30%)\n\n\nYou will be doing a final group project.\n\nYour project can be:\n\nData analysis using statistical models or machine learning algorithms\nIntroduce a R or Python package not learned in class, including live demo\nIntroduce a data science tool (visualization, computing, etc) not learned in class, including live demo\nWeb development: Website or dashboard for data science, including live demo\n\n\nThe final project presentation is on Monday, 5/6, 10:30 AM - 12:30 PM\nMore information will be released later."
  },
  {
    "objectID": "slides/01-syllabus.html#sharingreusing-code-policy",
    "href": "slides/01-syllabus.html#sharingreusing-code-policy",
    "title": "Welcome Aboard! 🙌",
    "section": "Sharing/Reusing Code Policy",
    "text": "Sharing/Reusing Code Policy\n\nUnless explicitly stated otherwise, you may make use of any online resources, but you must explicitly cite where you obtained any code you directly use or use as inspiration in your solutions.\n❌ Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism, regardless of source. 😱\n\n\nYou can use any code shared online or in books. But please give the authors full credits. Cite their work and let me know whose code you are using to do your homework and project.\nI encourage you to write your own code. This way you learn the most."
  },
  {
    "objectID": "slides/01-syllabus.html#academic-integrity",
    "href": "slides/01-syllabus.html#academic-integrity",
    "title": "Welcome Aboard! 🙌",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nThis course expects all students to follow University and College statements on academic integrity.\n\n\nHonor Pledge and Honor Code: I recognize the importance of personal integrity in all aspects of life and work. I commit myself to truthfulness, honor, and responsibility, by which I earn the respect of others. I support the development of good character, and commit myself to uphold the highest standards of academic integrity as an important aspect of personal integrity. My commitment obliges me to conduct myself according to the Marquette University Honor Code."
  },
  {
    "objectID": "slides/01-syllabus.html#q-a",
    "href": "slides/01-syllabus.html#q-a",
    "title": "Welcome Aboard! 🙌",
    "section": "Q & A",
    "text": "Q & A\n❓ Will this course require any Textbook(s) or other materials to be purchased?\n👉 No required textbooks. All resources are free on line!\n\n\n❓ Is it teaching the basics of data science?\n👉 Yes, I think so.\n\n\n\n❓ What do you think will be the most interesting part of the course?\n👉 I love data visualization and web development.\n\n\n\n❓ If this course is a joint label between COSC and MATH, is there a difference between any particular class section or other that we may have registered for?\n👉 No difference. MATH 3570 and COSC 3570 are exactly the same course.\n\n\n\n❓ What kind of time estimate do you believe most students should spend on reading + assignments for the course?\n👉 Everyone is different. The more the better."
  },
  {
    "objectID": "slides/01-syllabus.html#q-a-1",
    "href": "slides/01-syllabus.html#q-a-1",
    "title": "Welcome Aboard! 🙌",
    "section": "Q & A",
    "text": "Q & A\n❓ How accessible are you outside of class AND office hours?\n👉 We can schedule a Teams/in-person meeting if you need.\n\n\n❓ Will this class help me better understand how to code proficiently?\n👉 As you learn to speak a foreign language, you need to code a lot, frequently and constantly in order to be proficient in any programming language. No shortcut.\n\n\n\n❓ Do you know of any internships or research positions offered through Marquette University that incorporate the skills learned in this Data Science course?\n👉 Quite many. Northwestern Mutual, Direct Supply, for example. I’ll share intern info with you if I get any.\n\n\n\n❓ Your question.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmath3570-s24.github.io/website"
  },
  {
    "objectID": "slides/20-kmeans.html#clustering-methods",
    "href": "slides/20-kmeans.html#clustering-methods",
    "title": "K-Means Clustering \n",
    "section": "Clustering Methods",
    "text": "Clustering Methods\n\nClustering: unsupervised learning technique for finding subgroups or clusters in a data set.\nGOAL: Homogeneous within groups; heterogeneous between groups\n\n\nWhen we group data points together, we hope the points in the same group look very much like each other, and the points in the different groups or clusters look very different.\nWhen I say they look like each other, I mean the points share similar characteristics. In other words, their variables’ values are pretty similar.\n\n\n\nCustomer/Marketing Segmentation\n\nDivide customers into clusters on age, income, etc.\nEach subgroup might be more receptive to a particular form of advertising, or more likely to purchase a particular product.\n\n\n\n\n\n\n\n\nSource: https://www.datacamp.com/community/tutorials/introduction-customer-segmentation-python\n\n\n\n\n\n\nDivide customers into clusters on the basis of common characteristics.\nStudents: the price shouldn’t be that high, and the item should be good-looking and maybe colorful, so it looks young.\nIf we target this group, the low price is not that important, but the item should be beautiful, high-class and high quality."
  },
  {
    "objectID": "slides/20-kmeans.html#k-means-clustering",
    "href": "slides/20-kmeans.html#k-means-clustering",
    "title": "K-Means Clustering \n",
    "section": "K-Means Clustering",
    "text": "K-Means Clustering\n\nPartition observations into \\(K\\) distinct, non-overlapping clusters: assign each to exactly one of the \\(K\\) clusters.\nMust pre-specify the number of clusters \\(K \\ll n\\).\n\n\n\n\n\n\nSource: Introduction to Statistical Learning Fig 12.7\n\n\n\n\n\n\nA data point cannot belong to two clusters at the same time.\nAs KNN, we have to decide how many clusters the data are partitioned into."
  },
  {
    "objectID": "slides/20-kmeans.html#k-means-illustration",
    "href": "slides/20-kmeans.html#k-means-illustration",
    "title": "K-Means Clustering \n",
    "section": "K-Means Illustration",
    "text": "K-Means Illustration\n\n\nData (Let’s choose \\(K=2\\))\n\n\n\n\n\n\n\n\n\n\nK-Means Algorithm\n\nChoose a value of \\(K\\).\n\nRandomly assign a number, from 1 to \\(K\\), to each of the observations.\nIterate until the cluster assignments stop changing:\n\n\n[1] For each of the \\(K\\) clusters, compute its cluster centroid.\n\n[2] Assign each observation to the cluster whose centroid is closest."
  },
  {
    "objectID": "slides/20-kmeans.html#k-means-illustration-1",
    "href": "slides/20-kmeans.html#k-means-illustration-1",
    "title": "K-Means Clustering \n",
    "section": "K-Means Illustration",
    "text": "K-Means Illustration\n\n\nRandom assignment\n\n\n\n\n\n\n\n\n\n\nK-Means Algorithm\n\nChoose a value of \\(K\\).\nRandomly assign a number, from 1 to \\(K\\), to each of the observations.\nIterate until the cluster assignments stop changing:\n\n\n[1] For each of the \\(K\\) clusters, compute its cluster centroid.\n\n[2] Assign each observation to the cluster whose centroid is closest."
  },
  {
    "objectID": "slides/20-kmeans.html#k-means-illustration-2",
    "href": "slides/20-kmeans.html#k-means-illustration-2",
    "title": "K-Means Clustering \n",
    "section": "K-Means Illustration",
    "text": "K-Means Illustration\n\n\nCompute the cluster centroid\n\n\n\n\n\n\n\n\n\n\nK-Means Algorithm\n\nChoose a value of \\(K\\).\n\nRandomly assign a number, from 1 to \\(K\\), to each of the observations.\nIterate until the cluster assignments stop changing:\n\n\n[1] For each of the \\(K\\) clusters, compute its cluster centroid.\n\n\n[2] Assign each observation to the cluster whose centroid is closest."
  },
  {
    "objectID": "slides/20-kmeans.html#k-means-illustration-3",
    "href": "slides/20-kmeans.html#k-means-illustration-3",
    "title": "K-Means Clustering \n",
    "section": "K-Means Illustration",
    "text": "K-Means Illustration\n\n\nDo new assignment\n\n\n\n\n\n\n\n\n\n\nK-Means Algorithm\n\nChoose a value of \\(K\\).\n\nRandomly assign a number, from 1 to \\(K\\), to each of the observations.\nIterate until the cluster assignments stop changing:\n\n\n[1] For each of the \\(K\\) clusters, compute its cluster centroid.\n\n[2] Assign each observation to the cluster whose centroid is closest."
  },
  {
    "objectID": "slides/20-kmeans.html#k-means-illustration-4",
    "href": "slides/20-kmeans.html#k-means-illustration-4",
    "title": "K-Means Clustering \n",
    "section": "K-Means Illustration",
    "text": "K-Means Illustration\n\n\nDo new assignment\n\n\n\n\n\n\n\n\n\nCompute the cluster centroid …"
  },
  {
    "objectID": "slides/20-kmeans.html#section-1",
    "href": "slides/20-kmeans.html#section-1",
    "title": "K-Means Clustering \n",
    "section": "",
    "text": "Source: Introduction to Statistical Learning Fig 12.8"
  },
  {
    "objectID": "slides/20-kmeans.html#k-means-algorithm",
    "href": "slides/20-kmeans.html#k-means-algorithm",
    "title": "K-Means Clustering \n",
    "section": "K-Means Algorithm",
    "text": "K-Means Algorithm\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe K-means algorithm finds a local rather than global optimum.\nThe results depend on the initial cluster assignment of each observation.\n\nRun the algorithm multiple times, then select the one producing the smallest within-cluster variation.\n\n\nStandardize the data so that distance is not affected by variable unit.\n\n\n\n\n\n\nWhat is within-cluster variation? I avoid using mathematical formula. But the idea is if the data points in the same group are very close to their group mean, their within-cluster variation will be small.\nWe will have K within-cluster variation, one for each group.\nSo we hope the sum of K within-cluster variations or the total within-cluster variation is as small as possible."
  },
  {
    "objectID": "slides/20-kmeans.html#section-2",
    "href": "slides/20-kmeans.html#section-2",
    "title": "K-Means Clustering \n",
    "section": "",
    "text": "Source: Introduction to Statistical Learning Fig 12.8"
  },
  {
    "objectID": "slides/20-kmeans.html#data-for-k-means",
    "href": "slides/20-kmeans.html#data-for-k-means",
    "title": "K-Means Clustering \n",
    "section": "Data for K-Means",
    "text": "Data for K-Means\n\n\n\n\n\n\ndf <- read_csv(\"./data/clus_data.csv\")\ndf  ## income in thousands\n\n# A tibble: 240 × 2\n    age income\n  <dbl>  <dbl>\n1  32.1  167. \n2  59.1   56.9\n3  54.0   52.4\n4  26.3  -13.9\n5  61.3   41.1\n6  40.7   39.8\n# ℹ 234 more rows\n\n\n\n\ndf_clust <- as_tibble(scale(df))\ndf_clust\n\n# A tibble: 240 × 2\n     age income\n   <dbl>  <dbl>\n1 -0.878  1.00 \n2  1.44  -0.601\n3  1.00  -0.668\n4 -1.38  -1.63 \n5  1.64  -0.831\n6 -0.137 -0.851\n# ℹ 234 more rows\n\n\n\n\nlibrary(factoextra) https://bookdown.org/tpinto_home/Unsupervised-learning/k-means-clustering.html#KM1 https://towardsdatascience.com/k-means-clustering-concepts-and-implementation-in-r-for-data-science-32cae6a3ceba https://www.datanovia.com/en/lessons/k-means-clustering-in-r-algorith-and-practical-examples/ https://uc-r.github.io/kmeans_clustering#optimal https://www.statology.org/k-means-clustering-in-r/"
  },
  {
    "objectID": "slides/20-kmeans.html#data-for-k-means-1",
    "href": "slides/20-kmeans.html#data-for-k-means-1",
    "title": "K-Means Clustering \n",
    "section": "Data for K-Means",
    "text": "Data for K-Means\n\ndf_clust |> ggplot(aes(x = age, \n                       y = income)) + \n    geom_point()"
  },
  {
    "objectID": "slides/20-kmeans.html#kmeans",
    "href": "slides/20-kmeans.html#kmeans",
    "title": "K-Means Clustering \n",
    "section": "kmeans()",
    "text": "kmeans()\n\n\n(kclust <- kmeans(x = df_clust, centers = 3))\n\nK-means clustering with 3 clusters of sizes 67, 88, 85\n\nCluster means:\n    age income\n1 -0.29   1.33\n2  1.04  -0.32\n3 -0.85  -0.71\n\nClustering vector:\n  [1] 1 2 2 3 2 3 2 3 1 2 2 3 3 2 3 3 1 3 1 1 3 3 3 2 3 3 2 2 3 1 3 2 1 1 2 1 3\n [38] 3 3 1 3 3 2 1 3 1 2 3 2 2 2 3 1 3 2 3 3 2 1 2 2 3 2 2 2 3 3 2 3 2 2 3 1 1\n [75] 3 2 3 2 2 3 2 2 3 2 1 1 1 2 2 2 3 3 3 3 1 2 1 3 3 2 3 3 2 3 1 2 2 3 1 1 1\n[112] 2 1 2 2 2 2 3 2 2 2 1 3 1 3 2 2 2 1 1 2 1 3 2 3 1 1 1 1 3 2 1 1 3 3 3 2 2\n[149] 3 2 1 2 1 3 1 3 2 3 1 2 2 1 3 3 3 2 2 1 3 3 2 3 1 1 2 3 2 3 1 3 1 3 1 2 3\n[186] 2 2 1 2 2 2 1 3 2 2 2 2 1 1 1 2 1 1 3 1 3 3 1 2 1 3 3 2 2 1 3 3 3 2 1 2 1\n[223] 2 3 3 3 2 1 3 1 1 1 3 2 2 2 1 3 3 1\n\nWithin cluster sum of squares by cluster:\n[1] 66 41 39\n (between_SS / total_SS =  69.5 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\""
  },
  {
    "objectID": "slides/20-kmeans.html#kmeans-1",
    "href": "slides/20-kmeans.html#kmeans-1",
    "title": "K-Means Clustering \n",
    "section": "kmeans()",
    "text": "kmeans()\n\nkclust$centers\n\n    age income\n1 -0.29   1.33\n2  1.04  -0.32\n3 -0.85  -0.71\n\nkclust$size\n\n[1] 67 88 85\n\nhead(kclust$cluster, 20)\n\n [1] 1 2 2 3 2 3 2 3 1 2 2 3 3 2 3 3 1 3 1 1"
  },
  {
    "objectID": "slides/20-kmeans.html#cluster-info",
    "href": "slides/20-kmeans.html#cluster-info",
    "title": "K-Means Clustering \n",
    "section": "Cluster Info",
    "text": "Cluster Info\n\n(df_clust_k <- augment(kclust, df_clust))\n\n# A tibble: 240 × 3\n     age income .cluster\n   <dbl>  <dbl> <fct>   \n1 -0.878  1.00  1       \n2  1.44  -0.601 2       \n3  1.00  -0.668 2       \n4 -1.38  -1.63  3       \n5  1.64  -0.831 2       \n6 -0.137 -0.851 3       \n# ℹ 234 more rows\n\n\n\n(tidy_kclust <- tidy(kclust))\n\n# A tibble: 3 × 5\n     age income  size withinss cluster\n   <dbl>  <dbl> <int>    <dbl> <fct>  \n1 -0.286  1.33     67     65.9 1      \n2  1.04  -0.325    88     40.8 2      \n3 -0.848 -0.714    85     38.9 3      \n\n\naugment adds the point classifications to the original data set:"
  },
  {
    "objectID": "slides/20-kmeans.html#k-means-in-r",
    "href": "slides/20-kmeans.html#k-means-in-r",
    "title": "K-Means Clustering \n",
    "section": "K-Means in R",
    "text": "K-Means in R\n\n\nClustering Result\nCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSteady-income family\nNew college graduates/mid-class young family\nHigh socioeconomic class\n\n\n\n\n\n\ndf_clust_k |>  \n    ggplot(aes(x = age, \n               y = income)) + \n    geom_point(aes(color = .cluster), \n               alpha = 0.8) + \n    geom_point(data = tidy_kclust |>  \n                   select(1:2),\n               size = 8,\n               fill = \"black\",\n               shape = \"o\") +\n    theme_minimal() +\n    theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "slides/20-kmeans.html#k-means-in-r-factoextra",
    "href": "slides/20-kmeans.html#k-means-in-r-factoextra",
    "title": "K-Means Clustering \n",
    "section": "K-Means in R: factoextra\n",
    "text": "K-Means in R: factoextra\n\n\nlibrary(factoextra)\nfviz_cluster(object = kclust, data = df_clust, label = NA) + \n    theme_bw()"
  },
  {
    "objectID": "slides/20-kmeans.html#choose-k-total-withing-sum-of-squares",
    "href": "slides/20-kmeans.html#choose-k-total-withing-sum-of-squares",
    "title": "K-Means Clustering \n",
    "section": "Choose K: Total Withing Sum of Squares",
    "text": "Choose K: Total Withing Sum of Squares\n\n## wss = total within sum of squares\nfviz_nbclust(x = df_clust, FUNcluster = kmeans, method = \"wss\",  \n             k.max = 10)"
  },
  {
    "objectID": "slides/20-kmeans.html#practical-issues",
    "href": "slides/20-kmeans.html#practical-issues",
    "title": "K-Means Clustering \n",
    "section": "Practical Issues",
    "text": "Practical Issues\n\nTry several different \\(K\\)s, and look for the one with the most useful or interpretable solution.\n\n\nClustering is not beneficial for decision making or strategic plan if the clusters found are not meaningful based on their features.\n\n\nThe clusters found may be heavily distorted due to outliers that do not belong to any cluster.\nClustering methods are not very robust to perturbations of the data."
  },
  {
    "objectID": "slides/20-kmeans.html#section-3",
    "href": "slides/20-kmeans.html#section-3",
    "title": "K-Means Clustering \n",
    "section": "",
    "text": "23-K means Clustering \nIn lab.qmd ## Lab 24 section,\n\nInstall R package palmerpenguins at https://allisonhorst.github.io/palmerpenguins/\nPerform K-Means to with \\(K = 3\\) to cluster penguins based on bill_length_mm and flipper_length_mm of data peng.\n\n\nlibrary(palmerpenguins)\npeng <- penguins[complete.cases(penguins), ] |> \n    select(flipper_length_mm, bill_length_mm)"
  },
  {
    "objectID": "slides/20-kmeans.html#sklearn.cluster.kmeans",
    "href": "slides/20-kmeans.html#sklearn.cluster.kmeans",
    "title": "K-Means Clustering \n",
    "section": "sklearn.cluster.KMeans",
    "text": "sklearn.cluster.KMeans\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\n\ndf_clus = pd.read_csv('./data/clus_data.csv')\n\n\nscaler = StandardScaler()\nX = scaler.fit_transform(df_clus.values)\n\n\n\n\nkmeans = KMeans(n_clusters=3,  n_init=10).fit(X)\n\n\nkmeans.labels_[0:10]\n\narray([0, 1, 1, 2, 1, 2, 1, 2, 0, 1], dtype=int32)\n\n\n\n\nnp.round(kmeans.cluster_centers_, 2)\n\narray([[-0.29,  1.33],\n       [ 1.04, -0.33],\n       [-0.85, -0.72]])\n\n\n\n\n\nmath3570-s24.github.io/website"
  },
  {
    "objectID": "slides/15-probstat.html#why-study-probability",
    "href": "slides/15-probstat.html#why-study-probability",
    "title": "Probability and Statistics 🎲",
    "section": "Why Study Probability",
    "text": "Why Study Probability\n\nWe live in a world full of chances and uncertainty!\n\n\n\n\nMar 29, 2022\n\n\n\n\n\n\n\n\n\nMar 22, 2023\n\n\n\n\n\n\n\n\n\nMar 18, 2024"
  },
  {
    "objectID": "slides/15-probstat.html#why-study-probability-1",
    "href": "slides/15-probstat.html#why-study-probability-1",
    "title": "Probability and Statistics 🎲",
    "section": "Why Study Probability",
    "text": "Why Study Probability\n\nProbability is the study of chance, the language of uncertainty.\n\nWe could do data science without any probability involved. However, what we can learn from data will be much limited. Why?\n\nEvery time you collect a data set, you obtain a different one.\nYour data are affected by some chance or random noises!\n\n\nKnowledge of probability is essential for data science, especially when we want to quantify uncertainty about what we learn from our data.\n\n\n\nProbability is the study of chance, the language of uncertainty.\nIf you wanna study chance or uncertainty formally and rigorously, you cannot do it without probability involved.\nWe could do data science without any probability involved. However, what we can learn from data will be much limited. Why?\n\nEvery time you collect a data set, you obtain a different one. Your data point is like a random variable that follows some distribution. Or your data are sampled from some population associated with some probability distribution.\nYour data are affected by chance in some way!\n\n\nKnowledge of probability becomes essential for data science, especially when we want to quantify uncertainty about what we learn from our data. We’ll talk more about how to quantify uncertainty about the accuracy of our estimators or predictions in the next lecture. OK."
  },
  {
    "objectID": "slides/15-probstat.html#probability-as-relative-frequency",
    "href": "slides/15-probstat.html#probability-as-relative-frequency",
    "title": "Probability and Statistics 🎲",
    "section": "Probability as Relative Frequency",
    "text": "Probability as Relative Frequency\n\nThe probability that some outcome of a process will be obtained is the relative frequency with which that outcome would be obtained if the process were repeated a large number of times independently under similar conditions.\n\nExample:\n\n\ntoss a coin: probability of getting heads 🪙\n\npick a ball (red/blue) in an urn: probability of getting a red ball 🔴 🔵\n\n\n\n\n\n      Frequency Relative_Frequency\nHeads         6                0.6\nTails         4                0.4\nTotal        10                1.0\n---------------------\n      Frequency Relative_Frequency\nHeads       535               0.54\nTails       465               0.47\nTotal      1000               1.00\n---------------------\n\n\n\nThere are more than one way to define a probability, and one common way is to view probability as relative frequency.\nAnd the definition is as follows.\nThe probability that some outcome/event of a process/experiment will be obtained is the relative frequency with which that outcome would be obtained if the experiment were repeated a large number of times under similar (identical theoretically) conditions.\nMathematically, we actually require infinite number of repetitions and identical condition.\nHere are examples whose probability can be interpreted as relative frequency.\nSuppose we toss or flip a coin, we can ask what’s the probability of getting heads.\nOr if the experiment is picking a ball (red/blue) in an urn, we can ask what’s the probability of getting heads.\nTake tossing a coin as example. To get the probability of getting heads or least the approximate of it. We can toss the coin many times, and then count the number of times that heads shows up, and get the relative frequency.\nSo here, if I toss a coin 10 times, the relative frequency is 0.4, and so the probability of getting heads is approximately 0.4.\nAnd if I toss a coin 1000 times instead, the approximate probability becomes 51.4%."
  },
  {
    "objectID": "slides/15-probstat.html#monte-carlo-simulation-for-categorical-data",
    "href": "slides/15-probstat.html#monte-carlo-simulation-for-categorical-data",
    "title": "Probability and Statistics 🎲",
    "section": "Monte Carlo Simulation for Categorical Data",
    "text": "Monte Carlo Simulation for Categorical Data\n\n\n\nWe get a bag of 5 balls colored in red or blue.\n\n\nWithout peeking at the bag, how do we approximate the probability of getting a red ball?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonte Carlo Simulation: Repeat drawing a ball at random a large number of times to approximate the probability by the relative frequency of getting a red ball.\n\n\n\n\n\n\n\n\n\nsample(x = bag_balls, size = 1)\n\n[1] \"red\"\n\nmc_sim <- replicate(10000, \n                    sample(bag_balls, 1))\nhead(mc_sim)\n\n[1] \"blue\" \"blue\" \"blue\" \"red\"  \"red\"  \"blue\"\n\n\n\n\n(freq_table <- table(mc_sim))\n\nmc_sim\nblue  red \n5990 4010 \n\nfreq_table / 10000\n\nmc_sim\nblue  red \n 0.6  0.4 \n\n\n\n\n\n\n\nSo how many red balls in the bag?\n\n\nA so-called Monte Carlo Simulation demonstrate the idea of relative frequency as an approximation to a probability.\nSuppose we have 2 red balls and 3 blue ones in a bag. We wanna know what is the probability of getting a red ball?\nIf each ball is equally likely to be chosen (picking at random), apparently, \\(Pr(red) = 0.4\\). # draw a ball at random from bag_balls, a # vector of length 5 with value red/blue\nThe idea of Monte Carlo Simulation: is that We repeat the experiment (drawing a ball) a large number of times to obtain the relative frequency of red ball to approximate the probability of getting a red ball, which is exactly what we do for tossing a coin before.\nThe code is shown right here.\nFirst we can use rep() function to generate the bag that has 2 red balls and 3 blue balls.\nTo draw a ball at random, we can use the function sample(). It produces one random outcome.\nTo do a Monte Carlo Simulation, we can use replicate() function, which allows us to repeat the same job number of times.\nSo here, I repeatedly draw a ball 10000 times.\nFinally, we can check the frequency table or frequency distribution using table() function.\nTo get the relative frequency, just divided by the total number of repetitions.\nYou can see that the result of MC simulation approximates the true probability very well."
  },
  {
    "objectID": "slides/15-probstat.html#random-seed-set.seed",
    "href": "slides/15-probstat.html#random-seed-set.seed",
    "title": "Probability and Statistics 🎲",
    "section": "Random Seed set.seed()\n",
    "text": "Random Seed set.seed()\n\n\nbag_balls\n\n[1] \"red\"  \"red\"  \"blue\" \"blue\" \"blue\"\n\n\n\n\nWhen doing sampling, we use random number generators, and results vary from sample to sample.\n\n\n\n\nTo ensure the results are the same every time we do sampling, set the random seed to a specific number by set.seed()\n\n\n\n## same result!\nset.seed(2024)\nsample(x = bag_balls, size = 3)\n\n[1] \"red\"  \"red\"  \"blue\"\n\nset.seed(2024)\nsample(bag_balls, 3)\n\n[1] \"red\"  \"red\"  \"blue\"\n\n\n\nWhen we do sampling, we use random number generators in the computer language, and sampling results vary from sample to sample. This time you get red, next time you get blue.\nTo ensure the results are exactly the same every time you do sampling, set the R random seed to a specific number by set.seed()\n\nFor example, if you set the seed at 1000, then every time you draw a ball, the ball is always blue."
  },
  {
    "objectID": "slides/15-probstat.html#normal-gaussian-distribution-nmu-sigma2",
    "href": "slides/15-probstat.html#normal-gaussian-distribution-nmu-sigma2",
    "title": "Probability and Statistics 🎲",
    "section": "Normal (Gaussian) Distribution \\(N(\\mu, \\sigma^2)\\)\n",
    "text": "Normal (Gaussian) Distribution \\(N(\\mu, \\sigma^2)\\)\n\n\nDensity curve\n\n\n\nNormal Distribution is also called Gaussian distribution.\nIt has two parameters that determine its density function and its density curve or distribution shape.\nOne is its mean \\(\\mu\\), the other variance \\(\\sigma^2\\).\nIt has a bell-shaped density curve symmetric about it mean."
  },
  {
    "objectID": "slides/15-probstat.html#draw-random-values-from-nmu-sigma2",
    "href": "slides/15-probstat.html#draw-random-values-from-nmu-sigma2",
    "title": "Probability and Statistics 🎲",
    "section": "Draw Random Values from \\(N(\\mu, \\sigma^2)\\)\n",
    "text": "Draw Random Values from \\(N(\\mu, \\sigma^2)\\)\n\n\n\nrnorm(n, mean, sd): Draw \\(n\\) observations from a normal distribution with mean mean and standard deviation sd.\n\n\n## the default mean = 0 and sd = 1 (standard normal)\nrnorm(5)\n\n[1] -1.189  0.438  0.017 -0.410  0.341\n\n\n\n\n\n\\(100\\) random draws from \\(N(0, 1)\\)\n\n\n\n\n\n\n\n\n\n\n\nWe use rnorm(n, mean, sd) to draw n observations from the normal distribution with mean mean and standard deviation sd.\nthe default distribution is standard normal (mean is 0 and sd is 1)\nSo we can easily draw normal samples as many as we want.\nAll those red points are random draws from the standard normal.\nYou can see that most of the points are around the mean because the density around the mean is higher.\nAlso, we can see when we draw normal samples, it is very difficult to get a sample with a very extreme value because its corresponding density value is quite small.\nTherefore, we tend to underestimate the population variance if we use the sample data to estimate it. And that’s one of the reason why we divided by n - 1 in the sample variance formula to sort of correct this underestimation."
  },
  {
    "objectID": "slides/15-probstat.html#histogram-of-normal-data-n-20",
    "href": "slides/15-probstat.html#histogram-of-normal-data-n-20",
    "title": "Probability and Statistics 🎲",
    "section": "Histogram of Normal Data (n = 20)",
    "text": "Histogram of Normal Data (n = 20)\n\nnor_sample <- rnorm(20)\n\n\n\nWe talked about this before. In statistics, we hope the sampled data to be as representative of population as possible.\nSuppose the population is normal. If the sample is a random sample, we hope the sample size to be large. The larger the sample size, the more representative of population the sample is.\nHere, the sample size is just 20. And you can see the sample data does not look very normal."
  },
  {
    "objectID": "slides/15-probstat.html#histogram-of-normal-data-n-200",
    "href": "slides/15-probstat.html#histogram-of-normal-data-n-200",
    "title": "Probability and Statistics 🎲",
    "section": "Histogram of Normal Data (n = 200)",
    "text": "Histogram of Normal Data (n = 200)\n\nnor_sample <- rnorm(200)\n\n\n\nWhen n = 200, the sample start looking like a normal distribution."
  },
  {
    "objectID": "slides/15-probstat.html#histogram-of-normal-data-n-5000",
    "href": "slides/15-probstat.html#histogram-of-normal-data-n-5000",
    "title": "Probability and Statistics 🎲",
    "section": "Histogram of Normal Data (n = 5000)",
    "text": "Histogram of Normal Data (n = 5000)\n\nnor_sample <- rnorm(5000)\n\n\n\nWhen n = 5000, the sample is very representative of its population"
  },
  {
    "objectID": "slides/15-probstat.html#compute-normal-probabilities",
    "href": "slides/15-probstat.html#compute-normal-probabilities",
    "title": "Probability and Statistics 🎲",
    "section": "Compute Normal Probabilities",
    "text": "Compute Normal Probabilities\n\n\ndnorm(x, mean, sd) to compute the density value \\(f(x)\\) (NOT probability)\n\npnorm(q, mean, sd) to compute \\(P(X \\leq q)\\)\n\n\npnorm(q, mean, sd, lower.tail = FALSE) to compute \\(P(X > q)\\)\n\n\npnorm(q2, mean, sd) - pnorm(q1, mean, sd) to compute \\(P(q_1\\leq X \\leq q_2)\\)\n\n\n\n\nWe can use dnorm(x, mean, sd) to compute the density value \\(f(x)\\) (NOT probability)\n\npnorm(q, mean, sd) to compute \\(P(X \\leq q)\\)\n\n\npnorm(q, mean, sd, lower.tail = FALSE) to compute \\(P(X > q)\\)\n\n\npnorm(q_1, mean, sd) - pnorm(q_2, mean, sd) to compute \\(P(q_1\\leq X \\leq q_2)\\)"
  },
  {
    "objectID": "slides/15-probstat.html#distributions-and-their-r-function",
    "href": "slides/15-probstat.html#distributions-and-their-r-function",
    "title": "Probability and Statistics 🎲",
    "section": "Distributions and Their R Function",
    "text": "Distributions and Their R Function\n\n\n\n\n\nSource: https://www.oreilly.com/library/view/the-r-book/9780470510247/ch007-sec013.html\n\n\n\n\n\n\nHere shows other R functions for computing other distributions. Because each distribution has its own parameters, when you use it, remember to specify the parameter values in function arguments."
  },
  {
    "objectID": "slides/15-probstat.html#normal-curve",
    "href": "slides/15-probstat.html#normal-curve",
    "title": "Probability and Statistics 🎲",
    "section": "Normal Curve",
    "text": "Normal Curve\n\nggplot() +\n    xlim(-5, 5) +\n    geom_function(fun = dnorm, args = list(mean = 2, sd = .5), color = \"blue\")"
  },
  {
    "objectID": "slides/15-probstat.html#section-3",
    "href": "slides/15-probstat.html#section-3",
    "title": "Probability and Statistics 🎲",
    "section": "",
    "text": "18-probability \nIn lab.qmd ## Lab 18 section,\n\n\nPlot the probability function \\(P(X = x)\\) of \\(X \\sim \\text{binomial}(n = 5, \\pi = 0.3)\\).\n\nTo use ggplot,\n\nCreate a data frame saving all possible values of \\(x\\) and their corresponding probability using dbinom(x, size = ___, prob = ___).\n\n\n\n# A tibble: 6 × 2\n      x       y\n  <int>   <dbl>\n1     0 0.168  \n2     1 0.360  \n3     2 0.309  \n4     3 0.132  \n5     4 0.0284 \n6     5 0.00243"
  },
  {
    "objectID": "slides/15-probstat.html#section-4",
    "href": "slides/15-probstat.html#section-4",
    "title": "Probability and Statistics 🎲",
    "section": "",
    "text": "2. Add geom_col()\n\n\n\n\n\n\n\n\n\n\nscale_x_continuous(breaks = seq(0, 5, by = 1))"
  },
  {
    "objectID": "slides/15-probstat.html#probability-vs.-statistics",
    "href": "slides/15-probstat.html#probability-vs.-statistics",
    "title": "Probability and Statistics 🎲",
    "section": "Probability vs. Statistics",
    "text": "Probability vs. Statistics\n\n\n\n Probability : We know the process generating the data and are interested in properties of observations.\n\n Statistics : We observed the data (sample) and are interested in determining what is the process generating the data (population).\n\n\n\n\n\n\nFigure 1.1 in All of Statistics (Wasserman 2003)"
  },
  {
    "objectID": "slides/15-probstat.html#terminology",
    "href": "slides/15-probstat.html#terminology",
    "title": "Probability and Statistics 🎲",
    "section": "Terminology",
    "text": "Terminology\n\nPopulation (Data generating process): a group of subjects we are interested in studying\nSample (Data): a (representative) subset of our population of interest\nParameter: a unknown fixed numerical quantity derived from the population 1\nStatistic: a numerical quantity derived from a sample\nCommon population parameters of interest and their corresponding sample statistic:\n\n\n\nQuantity\nParameter\nStatistic (Point estimate)\n\n\n\nMean\n\\(\\mu\\)\n\\(\\overline{x}\\)\n\n\nVariance\n\\(\\sigma^2\\)\n\\(s^2\\)\n\n\nStandard deviation\n\\(\\sigma\\)\n\\(s\\)\n\n\nproportion\n\\(p\\)\n\\(\\hat{p}\\)\n\n\n\n\nFirst just review some terminologies.\nA Populationis a group of individuals or objects we are interested in studying. It could be as small as a single family. Other examples are all Marquette students, all Marquette faculty, all American people, or all people in the world.\nA Sample is a (representative) subset of our population of interest.\nAn important word here is representative. The sample should look like its population. The more alike, the better for the accuracy of statistical inference. So if your population is all people in the US. The sample collected in Wisconsin is not representative at all. If your population is all Marquette students, the sample collected from computer science is not representative. The sample should include all different majors and years that have proportions similar to the proportions of all students. Right.\nA Parameter a unknown fixed numerical quantity derived from the population\nA Statistic: a numerical quantity derived from a sample\nOne goal of statistical inference is to estimate population parameters using our sample data, or statistics calculated from our sample data. (proportion of fraternity/sororities)\nAnother methodology called Bayesian statistics assumes parameters are unknown but random."
  },
  {
    "objectID": "slides/15-probstat.html#alpha100-confidence-interval-for-mu",
    "href": "slides/15-probstat.html#alpha100-confidence-interval-for-mu",
    "title": "Probability and Statistics 🎲",
    "section": "\n\\((1 - \\alpha)100\\%\\) Confidence Interval for \\(\\mu\\)\n",
    "text": "\\((1 - \\alpha)100\\%\\) Confidence Interval for \\(\\mu\\)\n\n\n\nWith \\(z_{\\alpha/2}\\) being \\((1-\\alpha/2)\\) quantile of \\(N(0, 1)\\), \\((1 - \\alpha)100\\%\\) confidence interval for \\(\\mu\\) is \\[\\left(\\overline{X} - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}, \\,\\, \\overline{X} + z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right)\\]\n\n\n\n\nWhat if \\(\\sigma\\) is unknown?\n\n\n\n\n\n\n\\((1 - \\alpha)100\\%\\) confidence interval for \\(\\mu\\) becomes \\[\\left(\\overline{X} - t_{\\alpha/2, n-1} \\frac{S}{\\sqrt{n}},\\,\\, \\overline{X} + t_{\\alpha/2, n-1}\\frac{S}{\\sqrt{n}}\\right),\\] where \\(t_{\\alpha/2, n-1}\\) is the \\((1-\\alpha/2)\\) quantile of Student-t distribution with degrees of freedom \\(n-1\\)."
  },
  {
    "objectID": "slides/15-probstat.html#interpreting-a-95-confidence-interval",
    "href": "slides/15-probstat.html#interpreting-a-95-confidence-interval",
    "title": "Probability and Statistics 🎲",
    "section": "Interpreting a 95% Confidence Interval",
    "text": "Interpreting a 95% Confidence Interval\n\n\nWe are 95% confident that blah blah blah . . .\n\n\nIf we were able to collect our sample data many times and build the corresponding confidence intervals, we would expect about 95% of those intervals would contain the true population parameter.\n\n\nHowever,\n\nWe never know if in fact 95% of them do, or whether any particular interval contains the true parameter! 😱\n❌ Cannot say “There is a 95% chance/probability that the true parameter is in the confidence interval.”\nIn practice we may only be able to collect one single data set.\n\n\nWe usually say we are 95% confident that blah blah blah. What do we mean by that?\nImagine that you ask your friends to go to a party this weekend, and they say oh I’m 95% confident that I like. You’d be like, what’s wrong with you, i’m never inviting you.\nWhen we say We are 95% confident, it has a very specific meaning.\n\nIt means that Suppose we were able to collect our sample dataset many times and build the corresponding confidence intervals.\nWe would expect about 95% of those intervals would contain the true population parameter.\n\n\nHowever, we never know if in fact 95% of them do, or whether any particular interval contains the true parameter (maybe none of them do!). Because we don’t know the true value is.\nWe cannot say\n“There is a 95% chance/probability that the true parameter is in the confidence interval.”"
  },
  {
    "objectID": "slides/15-probstat.html#confidence-interval-simulation",
    "href": "slides/15-probstat.html#confidence-interval-simulation",
    "title": "Probability and Statistics 🎲",
    "section": "95% Confidence Interval Simulation",
    "text": "95% Confidence Interval Simulation\n\\(X_1, \\dots, X_n \\sim N(\\mu, \\sigma^2)\\) where \\(\\mu = 120\\) and \\(\\sigma = 5\\)."
  },
  {
    "objectID": "slides/15-probstat.html#simulate-100-cis-for-mu-when-sigma-is-known",
    "href": "slides/15-probstat.html#simulate-100-cis-for-mu-when-sigma-is-known",
    "title": "Probability and Statistics 🎲",
    "section": "Simulate 100 CIs for \\(\\mu\\) when \\(\\sigma\\) is known",
    "text": "Simulate 100 CIs for \\(\\mu\\) when \\(\\sigma\\) is known\n\n\nAlgorithm\nSimulation Result\nCode\n\n\n\n\nAlgorithm\n\nGenerate 100 sampled data of size \\(n\\): \\((x_1^1, x_2^1, \\dots, x_n^1), \\dots (x_1^{100}, x_2^{100}, \\dots, x_n^{100})\\), where \\(x_i^m \\sim N(\\mu, \\sigma^2)\\).\nObtain 100 sample means \\((\\overline{x}^1, \\dots, \\overline{x}^{100})\\).\nFor each \\(m = 1, 2, \\dots, 100\\), compute the corresponding confidence interval \\[\\left(\\overline{x}^m - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}, \\overline{x}^m + z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right)\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmu <- 120; sig <- 5 \nal <- 0.05; M <- 100; n <- 16\n\nset.seed(2024)\nx_rep <- replicate(M, rnorm(n, mu, sig))\nxbar_rep <- apply(x_rep, 2, mean)\nE <- qnorm(p = 1 - al / 2) * sig / sqrt(n)\nci_lwr <- xbar_rep - E\nci_upr <- xbar_rep + E\n\nplot(NULL, xlim = range(c(ci_lwr, ci_upr)), ylim = c(0, 100), \n     xlab = \"95% CI\", ylab = \"Sample\", las = 1)\nmu_out <- (mu < ci_lwr | mu > ci_upr)\nsegments(x0 = ci_lwr, y0 = 1:M, x1 = ci_upr, col = \"navy\", lwd = 2)\nsegments(x0 = ci_lwr[mu_out], y0 = (1:M)[mu_out], x1 = ci_upr[mu_out], col = 2, lwd = 2)\nabline(v = mu, col = \"#FFCC00\", lwd = 2)\n\n\n\n\n\nAl right. So here illustrates the idea of confidence intervals. Suppose the true population mean is 120. And I draw a sample of the same size 100 times, and calculate 100 CIs accordingly.\nBecause each sample is different, we are gonna have a different CI as well.\nHere 4 of them are not covering the true population mean.\nAgain the CI varies from sample to sample. We will not get exactly 5 CIs not covering the true population mean among 100 CIs, but about 5 of them do. It could be 3, it could be 7."
  },
  {
    "objectID": "slides/15-probstat.html#section-5",
    "href": "slides/15-probstat.html#section-5",
    "title": "Probability and Statistics 🎲",
    "section": "",
    "text": "19-Confidence Interval  \nIn lab.qmd ## Lab 19 section,\n\nRun the code I give you for simulating 100 \\(95\\%\\) CIs. Change the random generator seed to another number you like.\n\n\nset.seed(a number you like) Birthday? Lucky number?\n\n\nHow many CIs do not cover the true mean \\(\\mu\\)?"
  },
  {
    "objectID": "slides/15-probstat.html#random-generator",
    "href": "slides/15-probstat.html#random-generator",
    "title": "Probability and Statistics 🎲",
    "section": "Random Generator",
    "text": "Random Generator\n\nimport numpy as np\n\n\n\nrandom.Generator.choice(): Generates a random sample from a given array\n\n\n\nbag_balls = ['red'] * 2 + ['blue'] * 3\n# np.random.seed(2023)\n\n## set a random number generator \nrng = np.random.default_rng(2023) ## R set.seed()\n\n## sampling from bag_balls \nrng.choice(bag_balls, size = 6, replace = True) ## R sample()\n\narray(['blue', 'red', 'red', 'red', 'red', 'red'], dtype='<U4')\n\n\n\n\n\nrng.normal(loc=0.0, scale=1.0, size=5) # R rnorm()\n\narray([ 0.22205533, -0.77586755,  0.8087058 , -0.19862826, -1.57869386])"
  },
  {
    "objectID": "slides/15-probstat.html#normal-distribution-from-scipy",
    "href": "slides/15-probstat.html#normal-distribution-from-scipy",
    "title": "Probability and Statistics 🎲",
    "section": "Normal Distribution from SciPy\n",
    "text": "Normal Distribution from SciPy\n\n\nlibrary(reticulate); py_install(\"scipy\")\n\n\nimport scipy\nfrom scipy.stats import norm\n\n\n\nx = 0\nnorm.pdf(x, loc=0, scale=1) ## R dnorm()\n\n0.3989422804014327\n\n\n\n\n\n\nnorm.cdf(x, 0, 1) ## R pnorm()\n\n0.5\n\n\n\n\n\n\nq = 0.95\nnorm.ppf(q, 0, 1)  ## R qnorm()\n\n1.6448536269514722\n\n\n\n\n\n\nnorm.rvs(loc=0, scale=1, size=1) ## R rnorm()\n\narray([0.72775615])"
  },
  {
    "objectID": "slides/15-probstat.html#normal-curve-plotting",
    "href": "slides/15-probstat.html#normal-curve-plotting",
    "title": "Probability and Statistics 🎲",
    "section": "Normal Curve Plotting",
    "text": "Normal Curve Plotting\n\n\n\nimport matplotlib.pyplot as plt\n\n\nmu = 100\nsig = 15\nx = np.arange(-4, 4, 0.1) * sig + mu\nhx = norm.pdf(x, mu, sig)\nplt.plot(x, hx)\nplt.xlabel('x')\nplt.ylabel('density')\nplt.title('N(100, 15^2)')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmath3570-s24.github.io/website"
  },
  {
    "objectID": "slides/12-dplyr-1.html#section-1",
    "href": "slides/12-dplyr-1.html#section-1",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "",
    "text": "Welcome back\nThis week: introduce tools of tidyverse for data wrangling\nlearn how to easily clean or transform our data so that we can extract some important properties of the data, and the data are ready for visualization, modeling and analysis."
  },
  {
    "objectID": "slides/12-dplyr-1.html#grammar-of-data-wrangling-dplyr",
    "href": "slides/12-dplyr-1.html#grammar-of-data-wrangling-dplyr",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "Grammar of Data Wrangling: dplyr 📦",
    "text": "Grammar of Data Wrangling: dplyr 📦\n\nbased on the concepts of functions as verbs that manipulate data frames\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmutate: create new columns from the existing1\n\n\nfilter: pick rows matching criteria\n\n\nslice: pick rows using index(es)\n\n\ndistinct: filter for unique rows\n\nselect: pick columns by name\n\n\nsummarise: reduce variables to values\n\ngroup_by: for grouped operations\n\narrange: reorder rows\n… (many more)\n\n\n\n\nSo Grammar of Data Wrangling is based on the concepts of functions as verbs that manipulate data frames. Using these verbs in coding is simple, intuitive and straightforward. They let you know what you are doing on your data.\nAnd the package that does this is called dplyr.\nThis package offers a variety of functions each of which is a verb, as listed here, mutate, filter, slice, distinct. Is distinct a verb??\nthere is no rule without an exception\nAnyway, we will be going through these functions that help us manipulate or transform our data. OK\nWe can use tibble::add_column() to add one or more columns to an existing data frame."
  },
  {
    "objectID": "slides/12-dplyr-1.html#rules-of-dplyr-functions",
    "href": "slides/12-dplyr-1.html#rules-of-dplyr-functions",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "Rules of dplyr Functions",
    "text": "Rules of dplyr Functions\n\n\n\nFirst argument is always a data frame\nSubsequent arguments say what to do with that data frame\nAlways return a data frame\nDon’t modify in place\n\n\n\n\nSo there are four rules of dplyr functions.\nFirst argument is always a data frame\n\nSubsequent arguments say what to do with that data frame\n\nAlways return a data frame, so data frame in, data frame out.\nDon’t modify in place, meaning that when we apply a function of dplyr to the data, we are not changing that data frame. We have option to re-save our result, either overwrite the existing data frame that we have or as a separate object, but we are not modifying the data frame in place."
  },
  {
    "objectID": "slides/12-dplyr-1.html#data-us-gun-murders-by-state-for-2010",
    "href": "slides/12-dplyr-1.html#data-us-gun-murders-by-state-for-2010",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "Data: US gun murders by state for 2010",
    "text": "Data: US gun murders by state for 2010\n\n(murders <- read_csv(\"./data/murders.csv\"))\n\n# A tibble: 51 × 5\n  state      abb   region population total\n  <chr>      <chr> <chr>       <dbl> <dbl>\n1 Alabama    AL    South     4779736   135\n2 Alaska     AK    West       710231    19\n3 Arizona    AZ    West      6392017   232\n4 Arkansas   AR    South     2915918    93\n5 California CA    West     37253956  1257\n6 Colorado   CO    West      5029196    65\n# ℹ 45 more rows\n\n\n\nThe data set we are going to use as an illustrative example is again the murders data set in the dslabs package with the book Intro to Data Science\nIt is of R base data frame type with 51 observations and 5 variables.\nNotice that region variable here is a factor, which may not make sense and we can convert it into a character vector if needed.\nAnd we can change its data type to modern tibble data frame, again use as_tibble() function."
  },
  {
    "objectID": "slides/12-dplyr-1.html#adding-a-new-variable-column-with-mutate",
    "href": "slides/12-dplyr-1.html#adding-a-new-variable-column-with-mutate",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "Adding a New Variable (Column) with mutate()\n",
    "text": "Adding a New Variable (Column) with mutate()\n\n\n\ndplyr::mutate() takes\n\na data frame as the 1st argument\nthe name and values of the variable as the 2nd argument using format name = values.\n\n\n\n\n(murders <- murders |>  \n     mutate(rate = total / population * 100000)) #<<\n\n# A tibble: 51 × 6\n  state      abb   region population total  rate\n  <chr>      <chr> <chr>       <dbl> <dbl> <dbl>\n1 Alabama    AL    South     4779736   135  2.82\n2 Alaska     AK    West       710231    19  2.68\n3 Arizona    AZ    West      6392017   232  3.63\n4 Arkansas   AR    South     2915918    93  3.19\n5 California CA    West     37253956  1257  3.37\n6 Colorado   CO    West      5029196    65  1.29\n# ℹ 45 more rows\n\n\n\n\ntotal and population inside the function are not defined in our R environment.\ndplyr functions know to look for variables in the data frame provided in the 1st argument.\n\n\nOK now let’s begin playing the the data set.\nFirst, we can add a New Variable into the existing data set with the function mutate().\nRemember, variables are stored by columns, and so adding new variables means add more columns to the data set.\nThe function mutate() takes the data frame as a first argument and the name and values of the variable as a second argument using the convention name = values.\nHere, I compute the murder rate as the total number of murders divided by population and times 100,000. So the rate means the incidence rate per 100,000 people\n\ntotal and population inside the function are not defined in our R environment, but we didn’t get an error.\nFunctions in dplyr know to look for variables in the data frame provided in the first argument. total will have the values in murders_tbl$total.\nNow you can see the new data set has a new column variable rate shown in the last column.\nUse relocate() to change column positions, using the same syntax as select() to make it easy to move blocks of columns at once. \n\n\nmutate() creates new columns that are functions of existing variables. It can also modify (if the name is the same as an existing column) and delete columns (by setting their value to NULL). mutate( .data, …, .by = NULL, .keep = c(“all”, “used”, “unused”, “none”), .before = NULL, .after = NULL )"
  },
  {
    "objectID": "slides/12-dplyr-1.html#filtering-observations-rows-with-filter",
    "href": "slides/12-dplyr-1.html#filtering-observations-rows-with-filter",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "Filtering Observations (Rows) with filter()\n",
    "text": "Filtering Observations (Rows) with filter()\n\n\n\ndplyr::filter() takes a\n\ndata frame as the 1st argument\n\nconditional statement as the 2nd. (pick rows matching criteria)\n\n\n\n\n# filter the data table to only show the entries for which \n# the murder rate is lower than 0.7\nmurders |> \n    filter(rate < 0.7) #<<\n\n# A tibble: 5 × 6\n  state         abb   region        population total  rate\n  <chr>         <chr> <chr>              <dbl> <dbl> <dbl>\n1 Hawaii        HI    West             1360301     7 0.515\n2 Iowa          IA    North Central    3046355    21 0.689\n3 New Hampshire NH    Northeast        1316470     5 0.380\n4 North Dakota  ND    North Central     672591     4 0.595\n5 Vermont       VT    Northeast         625741     2 0.320\n\n\n\nAll right. We add columns by mutate() function, and we pick observations (rows) with filter() function.\n\nfilter() function again takes the data frame as the first argument and then the conditional statement as the second.\nSo we pick rows by matching the criteria of the conditional statement in the second argument of the filter() function.\nFor example, here I filter the data table to only show the obs for which the murder rate is lower than 0.71"
  },
  {
    "objectID": "slides/12-dplyr-1.html#filter-for-many-conditions-at-once",
    "href": "slides/12-dplyr-1.html#filter-for-many-conditions-at-once",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "\nfilter() for Many Conditions at Once",
    "text": "filter() for Many Conditions at Once\n\nmurders |>  \n    filter(rate > 0.1 & rate < 0.7,  #<<\n           region == \"Northeast\")  #<<\n\n# A tibble: 2 × 6\n  state         abb   region    population total  rate\n  <chr>         <chr> <chr>          <dbl> <dbl> <dbl>\n1 New Hampshire NH    Northeast    1316470     5 0.380\n2 Vermont       VT    Northeast     625741     2 0.320\n\n\n\nWe can actually use several conditions at once to filter the observations\nFor example, here we want the observations whose murder rate is between 0.1 and 0.71, and their region is Northeast.\nYou can see that only two observations satisfy the conditions, New Hampshire and Vermont.\nUse one single & to have a logical vector\nYou can also write rate >= 0.1, rate <= 0.71. This way we separate the condition rate >= 0.1 & rate <= 0.71 into 2 conditions.\nTheoretically You can provide as many conditions as you want. The conditions are combined with &"
  },
  {
    "objectID": "slides/12-dplyr-1.html#logical-operators",
    "href": "slides/12-dplyr-1.html#logical-operators",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "Logical Operators",
    "text": "Logical Operators\n\n\n\n\n\n\n\n\noperator\ndefinition\noperator\ndefinition\n\n\n\n<\nless than\n\nx | y\n\n\nx OR y\n\n\n\n<=\nless than or equal to\nis.na(x)\nif x is NA\n\n\n\n>\ngreater than\n!is.na(x)\nif x is not NA\n\n\n\n>=\ngreater than or equal to\nx %in% y\nif x is in y\n\n\n\n==\nexactly equal to\n!(x %in% y)\nif x is not in y\n\n\n\n!=\nnot equal to\n!x\nnot x\n\n\n\nx & y\n\nx AND y\n\n\n\n\n\n\n\nWe have talked about this. These are logical operators you can use in the conditions of the filter() function."
  },
  {
    "objectID": "slides/12-dplyr-1.html#slice-for-certain-rows-using-indexes",
    "href": "slides/12-dplyr-1.html#slice-for-certain-rows-using-indexes",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "\nslice() for Certain Rows using Indexes",
    "text": "slice() for Certain Rows using Indexes\n\n# 3rd to 6th row\nmurders |> \n    slice(3:6)\n\n# A tibble: 4 × 6\n  state      abb   region population total  rate\n  <chr>      <chr> <chr>       <dbl> <dbl> <dbl>\n1 Arizona    AZ    West      6392017   232  3.63\n2 Arkansas   AR    South     2915918    93  3.19\n3 California CA    West     37253956  1257  3.37\n4 Colorado   CO    West      5029196    65  1.29\n\n\n\n\nHow do we subset rows using matrix indexing?\n\n\n\n\nmurders[3:6, ]\n\n# A tibble: 4 × 6\n  state      abb   region population total  rate\n  <chr>      <chr> <chr>       <dbl> <dbl> <dbl>\n1 Arizona    AZ    West      6392017   232  3.63\n2 Arkansas   AR    South     2915918    93  3.19\n3 California CA    West     37253956  1257  3.37\n4 Colorado   CO    West      5029196    65  1.29\n\n\n\nOK. Slice. We use slice() function to subset observations or rows using indexes.\nSame as other functions, the first argument is a data frame, so I use the pipe operator sending murders_tbl data set to the slice function.\nIt is the same as treating a data drame as a matrix and subsetting the rows, which is the usual way we do before learning this function.\nSo here we extract row 3 to row 6 of the data set.\nmicrobenchmark::microbenchmark(slice(murders_tbl, 3:6), murders_tbl[3:6, ])\nYou cannot(can?) write murders_tbl[3:6, ] as a function call. How? slice_head() and slice_tail() select the first or last rows. slice_sample() randomly selects rows. slice_min() and slice_max() select rows with highest or lowest values of a variable."
  },
  {
    "objectID": "slides/12-dplyr-1.html#distinct-to-filter-for-unique-rows",
    "href": "slides/12-dplyr-1.html#distinct-to-filter-for-unique-rows",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "\ndistinct() to Filter for Unique Rows",
    "text": "distinct() to Filter for Unique Rows\n\n# Select only unique/distinct rows from a data frame\nmurders |> distinct(region)  ## default\n\n# A tibble: 4 × 1\n  region       \n  <chr>        \n1 South        \n2 West         \n3 Northeast    \n4 North Central\n\nmurders |> distinct(region, .keep_all = TRUE) ## keep all other variables\n\n# A tibble: 4 × 6\n  state       abb   region        population total  rate\n  <chr>       <chr> <chr>              <dbl> <dbl> <dbl>\n1 Alabama     AL    South            4779736   135  2.82\n2 Alaska      AK    West              710231    19  2.68\n3 Connecticut CT    Northeast        3574097    97  2.71\n4 Illinois    IL    North Central   12830632   364  2.84\n\n\n\nWe can use distinct() function to select only unique/distinct rows from a data frame.\nFor example here, we use variable region to decide the unique rows. By default, the function only gives us the unique values of the variable region.\nDo you wonder why we have the order South, West, Northeast, and North Central?\nActually, these values are the values of some rows.\nIf we keep all variables, we see that the south is the south for Alabama, west for alaska, northeast for connecticut, and North central for illinois.\ndistinct() functions grabs rows that first has each of the unique values of region variable.\nSo Alabama is the first observation that has value South, Alaska is the first observation that has value West, Connecticut is the first observation that has value Northeast, and Illinois is the first observation that has value North Central.\nmicrobenchmark::microbenchmark(distinct(murders_tbl, region), as_tibble(unique(murders_tbl$region)))"
  },
  {
    "objectID": "slides/12-dplyr-1.html#distinct-grabs-first-row-of-the-unique-value",
    "href": "slides/12-dplyr-1.html#distinct-grabs-first-row-of-the-unique-value",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "\ndistinct() Grabs First Row of The Unique Value",
    "text": "distinct() Grabs First Row of The Unique Value\n\nmurders |> distinct(region, .keep_all = TRUE)\n\n# A tibble: 4 × 6\n  state       abb   region        population total  rate\n  <chr>       <chr> <chr>              <dbl> <dbl> <dbl>\n1 Alabama     AL    South            4779736   135  2.82\n2 Alaska      AK    West              710231    19  2.68\n3 Connecticut CT    Northeast        3574097    97  2.71\n4 Illinois    IL    North Central   12830632   364  2.84\n\n\n\nmurders |> slice(1:5)\n\n# A tibble: 5 × 6\n  state      abb   region population total  rate\n  <chr>      <chr> <chr>       <dbl> <dbl> <dbl>\n1 Alabama    AL    South     4779736   135  2.82\n2 Alaska     AK    West       710231    19  2.68\n3 Arizona    AZ    West      6392017   232  3.63\n4 Arkansas   AR    South     2915918    93  3.19\n5 California CA    West     37253956  1257  3.37"
  },
  {
    "objectID": "slides/12-dplyr-1.html#selecting-columns-with-select",
    "href": "slides/12-dplyr-1.html#selecting-columns-with-select",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "Selecting Columns with select()\n",
    "text": "Selecting Columns with select()\n\n\nIn dplyr::select(), the 1st argument is a data frame, followed by variable names being selected in the data.\nThe order of variable names matters!\n\n\nnames(murders)\n\n[1] \"state\"      \"abb\"        \"region\"     \"population\" \"total\"     \n[6] \"rate\"      \n\n\n\n# select three columns, assign this to a new object\nmurders |> select(region, rate, state)\n\n# A tibble: 51 × 3\n  region  rate state     \n  <chr>  <dbl> <chr>     \n1 South   2.82 Alabama   \n2 West    2.68 Alaska    \n3 West    3.63 Arizona   \n4 South   3.19 Arkansas  \n5 West    3.37 California\n6 West    1.29 Colorado  \n# ℹ 45 more rows\n\n\n\nfilter(), slice() and distinct()are functions for picking rows.\nselect() is a function for picking columns or variables.\nHere, I select three columns region, rate, and state.\nNotice that the order of variable names matters! The original data set has variable order “state” “abb” “region” “population” “total” “rate”.\nBut the resulting data frame will have the variables or columns ordered as region, rate, state because this is the order you specify in the select().\nIf you don’t know the column names, you can select by position. murders_tbl %>% select(3, 6, 1)"
  },
  {
    "objectID": "slides/12-dplyr-1.html#select-to-exclude-variables",
    "href": "slides/12-dplyr-1.html#select-to-exclude-variables",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "\nselect() to Exclude Variables",
    "text": "select() to Exclude Variables\n\n## exclude variable population\nmurders |> select(-population)\n\n# A tibble: 51 × 5\n  state      abb   region total  rate\n  <chr>      <chr> <chr>  <dbl> <dbl>\n1 Alabama    AL    South    135  2.82\n2 Alaska     AK    West      19  2.68\n3 Arizona    AZ    West     232  3.63\n4 Arkansas   AR    South     93  3.19\n5 California CA    West    1257  3.37\n6 Colorado   CO    West      65  1.29\n# ℹ 45 more rows\n\n\n\nIf you want to exclude some variables, just provide a vector of variables you want to remove, and put a minus sign in front of it.\nHere we remove the population variable."
  },
  {
    "objectID": "slides/12-dplyr-1.html#select-a-range-of-variables",
    "href": "slides/12-dplyr-1.html#select-a-range-of-variables",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "\nselect() a Range of Variables",
    "text": "select() a Range of Variables\n\nnames(murders)\n\n[1] \"state\"      \"abb\"        \"region\"     \"population\" \"total\"     \n[6] \"rate\"      \n\n## from region to rate\nmurders |> select(region:rate)\n\n# A tibble: 51 × 4\n  region population total  rate\n  <chr>       <dbl> <dbl> <dbl>\n1 South     4779736   135  2.82\n2 West       710231    19  2.68\n3 West      6392017   232  3.63\n4 South     2915918    93  3.19\n5 West     37253956  1257  3.37\n6 West      5029196    65  1.29\n# ℹ 45 more rows\n\n\n\nWe can also select a range of variables.\nFor example, we select all variables for region to rate by using region:rate, just as we create a sequence of numbers from 1 to 5 using 1:5.\nmurders_tbl %>% select(3:6)"
  },
  {
    "objectID": "slides/12-dplyr-1.html#select-variables-with-certain-characteristics",
    "href": "slides/12-dplyr-1.html#select-variables-with-certain-characteristics",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "\nselect() Variables with Certain Characteristics",
    "text": "select() Variables with Certain Characteristics\n\n\nstarts_with() is a tidy-select helper function.\n\n\nmurders |> select(starts_with(\"r\"))\n\n# A tibble: 51 × 2\n  region  rate\n  <chr>  <dbl>\n1 South   2.82\n2 West    2.68\n3 West    3.63\n4 South   3.19\n5 West    3.37\n6 West    1.29\n# ℹ 45 more rows\n\n\n\nWe can also some helper functions to select variables with some condition.\nFor example, we can use a helper function starts_with() to select variables whose name starts with letter “r”.\nAnd so region and rate variables are selected."
  },
  {
    "objectID": "slides/12-dplyr-1.html#select-variables-with-certain-characteristics-1",
    "href": "slides/12-dplyr-1.html#select-variables-with-certain-characteristics-1",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "\nselect() Variables with Certain Characteristics",
    "text": "select() Variables with Certain Characteristics\n\n\nends_with() is a tidy-select helper function.\n\n\nmurders |> select(ends_with(\"ion\"))\n\n# A tibble: 51 × 2\n  region population\n  <chr>       <dbl>\n1 South     4779736\n2 West       710231\n3 West      6392017\n4 South     2915918\n5 West     37253956\n6 West      5029196\n# ℹ 45 more rows\n\n\n\nWe can also select variables whose name ends with “ion”\nAnd so region and population variables are selected."
  },
  {
    "objectID": "slides/12-dplyr-1.html#tidy-select-helpers",
    "href": "slides/12-dplyr-1.html#tidy-select-helpers",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "\ntidy-select Helpers1\n",
    "text": "tidy-select Helpers1\n\n\n\nstarts_with(): Starts with a prefix\n\nends_with(): Ends with a suffix\n\ncontains(): Contains a literal string\n\nnum_range(): Matches a numerical range like x01, x02, x03\n\none_of(): Matches variable names in a character vector\n\neverything(): Matches all variables\n\nlast_col(): Select last variable, possibly with an offset\n\nmatches(): Matches a regular expression (a sequence of symbols/characters expressing a string/pattern to be searched for within text)\n\n\nHere is a list of select helpers.\nI am not able to go through every helper in detail. But they can be very useful depending on your goal.\nAbsolutely check their help page to learn how to use them and review the examples in the help page. OK.\nSee help for any of these functions for more info, e.g. ?num_range."
  },
  {
    "objectID": "slides/12-dplyr-1.html#rationale-for-pipe-operator",
    "href": "slides/12-dplyr-1.html#rationale-for-pipe-operator",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "Rationale for Pipe Operator",
    "text": "Rationale for Pipe Operator\n\nHow do we show three variables (state, region, rate) for states that have murder rates below 0.7?\n\n\nMethod 1: Define the intermediate object new_table\n\n\n\nnew_table <- select(murders, state, region, rate) \nfilter(new_table, rate < 0.7)\n\n# A tibble: 5 × 3\n  state         region         rate\n  <chr>         <chr>         <dbl>\n1 Hawaii        West          0.515\n2 Iowa          North Central 0.689\n3 New Hampshire Northeast     0.380\n4 North Dakota  North Central 0.595\n5 Vermont       Northeast     0.320\n\n\n\nRemember we talked about the pipe operator, right? But we haven’t really used it often.\nBut it kinda make much sense to use the pipe operator for data manipulation using dplyr functions.\nBecause the first argument of dplyr function is always a data frame and its function output is also a data frame, which can be the input of another dplyr function.\nSo if we wanna manipulate our data set via several different actions using the dplyr functions step by step, pipe operator can be very useful. Let’s see why.\nHow do we show three variables (state, region, rate) for states that have murder rate below 0.71?\nWe can first select three variables state, region, rate, and save the resulting output to an object new_table, and then apply filter() function on the new_table to get the observations having murder rate less than 0.71\nIn fact, the object new_table is unnecessary. The table is not what we want, and any object created in the R environment occupies some memory space."
  },
  {
    "objectID": "slides/12-dplyr-1.html#rationale-for-pipe-operator-1",
    "href": "slides/12-dplyr-1.html#rationale-for-pipe-operator-1",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "Rationale for Pipe Operator",
    "text": "Rationale for Pipe Operator\n\nHow do we show three variables (state, region, rate) for states that have murder rates below 0.7?\n\n\nMethod 2: Apply one function onto the other with no intermediate object\n\n\n## not so easy to read and understand\nfilter(select(murders, state, region, rate), rate < 0.7) \n\n# A tibble: 5 × 3\n  state         region         rate\n  <chr>         <chr>         <dbl>\n1 Hawaii        West          0.515\n2 Iowa          North Central 0.689\n3 New Hampshire Northeast     0.380\n4 North Dakota  North Central 0.595\n5 Vermont       Northeast     0.320\n\n\n\nIf we don’t use new_table object, we can put the code select(murders_tbl, state, region, rate) directly on the first argument of filter() function as the data input.\nThis gives us the same resulting data frame.\nHowever, the code is not very easy to read and understand because when we read the code from left to right, it starts with filter, and then select, but we actually do the selection first. Also, the second argument or the condition used in the filter() function is at the end of the code, which is far away from the function name at the beginning."
  },
  {
    "objectID": "slides/12-dplyr-1.html#rationale-for-pipe-operator-2",
    "href": "slides/12-dplyr-1.html#rationale-for-pipe-operator-2",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "Rationale for Pipe Operator",
    "text": "Rationale for Pipe Operator\n\nThe code that looks like a verbal description of what we want to do without intermediate objects:\n\n\ndata > select() > data after selecting > filter() > data after selecting and filtering\n\n\nmurders |> \n    select(state, region, rate) |>  \n    filter(rate < 0.7)\n\n# A tibble: 5 × 3\n  state         region         rate\n  <chr>         <chr>         <dbl>\n1 Hawaii        West          0.515\n2 Iowa          North Central 0.689\n3 New Hampshire Northeast     0.380\n4 North Dakota  North Central 0.595\n5 Vermont       Northeast     0.320\n\n\n\nPipe Operator let us write code that looks more like a description of what we want to do without intermediate objects.\nWe start with the original data set, then we select variables, and then we filter observations.\nSo our code can be like It’s more clear and intuitive, right?"
  },
  {
    "objectID": "slides/12-dplyr-1.html#summarizing-data-summarize",
    "href": "slides/12-dplyr-1.html#summarizing-data-summarize",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "Summarizing Data – summarize()\n",
    "text": "Summarizing Data – summarize()\n\n\n\nsummarize() provides a data frame that summarizes the statistics we compute. \n\n\n\nheights <- read_csv(\"./data/heights.csv\")\nhead(heights)\n\n# A tibble: 6 × 2\n  sex    height\n  <chr>   <dbl>\n1 Male       75\n2 Male       70\n3 Male       68\n4 Male       74\n5 Male       61\n6 Female     65\n\n\n\nThe summarize() function in dplyr provides a way to compute summary statistics.\nAgain, its first argument and output are always data frames.\nWe are gonna use another data set heights in the dslabs package to demonstrate the usage of function.\nThe data set only has two variables, sex and height.\nheight is a variable, but heights is the data set"
  },
  {
    "objectID": "slides/12-dplyr-1.html#summarizing-data-summarize-1",
    "href": "slides/12-dplyr-1.html#summarizing-data-summarize-1",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "Summarizing Data – summarize()\n",
    "text": "Summarizing Data – summarize()\n\n\n(s <- heights |> \n    filter(sex == \"Female\") |> \n    summarize(avg = mean(height),  \n              stdev = sd(height),\n              median = median(height), \n              minimum = min(height)))\n\n# A tibble: 1 × 4\n    avg stdev median minimum\n  <dbl> <dbl>  <dbl>   <dbl>\n1  64.9  3.76   65.0      51\n\ns$avg; s$minimum\n\n[1] 64.9\n\n\n[1] 51\n\n\n\nsummarise() produces a new data frame that is not any variant of the original data frame.\n\n\nSuppose we want to find the mean and standard deviation of female height and store the two values as a data frame, we can start with the data set heights, then filter or grab the observations that have sex Female, and then with the filtered data, use the summarize function to create a data frame of two variables “average” and “standard_deviation” computed from the variable height.\nYou see that the resulting object s is a data frame with two variables.\nNotice that summarise() changes the data frame entirely, it collapses rows down to a single summary statistic, and removes all columns that are irrelevant to the calculation. Basically, the resulting df is not any variant of the original df anymore.\n\nsummarise() changes the data frame entirely, it collapses rows down to a single summary statistic, and removes all columns that are irrelevant to the calculation."
  },
  {
    "objectID": "slides/12-dplyr-1.html#summarizing-data-summarize-2",
    "href": "slides/12-dplyr-1.html#summarizing-data-summarize-2",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "Summarizing Data – summarize()\n",
    "text": "Summarizing Data – summarize()\n\n\nOne variable quans that has 3 values. The output is a 3 by 1 data frame.\n\n\n(s2 <- heights |>  \n    filter(sex == \"Female\") |> \n    summarize(quans = quantile(height, c(0.1, 0.5, 0.9))))\n\n# A tibble: 3 × 1\n  quans\n  <dbl>\n1  61  \n2  65.0\n3  69  \n\n\n\nwe will get a data frame with more than one row if the function we use in summarize() return a vector.\nFor example here, we compute the 01, 0.5, and 0.9 quantile of variable height, and call it quantiles. Then the resulting data frame will have 3 rows, basically a column vector with 3 elements. str(s2)"
  },
  {
    "objectID": "slides/12-dplyr-1.html#grouping-group_by",
    "href": "slides/12-dplyr-1.html#grouping-group_by",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "Grouping – group_by()\n",
    "text": "Grouping – group_by()\n\n\n(heights_group <- heights |> \n     group_by(sex))  #<<\n\n# A tibble: 1,050 × 2\n# Groups:   sex [2]\n  sex    height\n  <chr>   <dbl>\n1 Male       75\n2 Male       70\n3 Male       68\n4 Male       74\n5 Male       61\n6 Female     65\n# ℹ 1,044 more rows\n\n\n\nclass(heights_group)\n\n[1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\nheights_group is a grouped data frame.\nTibbles are similar, but see Groups: sex [2] after grouping data by sex.\nsummarize() behaves differently when acting on grouped_df.\n\n\nA common operation in data exploration is to first split data into groups and then compute summaries for each group.\nTo group a data set by some variable, we can use group_by() function.\nHere, we group the heights data set by the variable sex.\nOutputs are similar, but we see Groups: sex [2] after grouping data by sex. [2] means there are two groups.\nAnd now the heights_group data set has a new class called grouped_df."
  },
  {
    "objectID": "slides/12-dplyr-1.html#group-and-summarize-group_by-summarize",
    "href": "slides/12-dplyr-1.html#group-and-summarize-group_by-summarize",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "Group and Summarize: group_by() + summarize()\n",
    "text": "Group and Summarize: group_by() + summarize()\n\n\n\nsummarize() applies the summarization to each group separately.\n\n\nheights |> \n    group_by(sex) |> \n    summarize(avg = mean(height), stdev = sd(height), \n              median = median(height), minimum = min(height))\n\n# A tibble: 2 × 5\n  sex      avg stdev median minimum\n  <chr>  <dbl> <dbl>  <dbl>   <dbl>\n1 Female  64.9  3.76   65.0      51\n2 Male    69.3  3.61   69        50\n\n\n\n\nmurders |>  \n    group_by(region) |> \n    summarize(median_rate = median(rate))\n\n# A tibble: 4 × 2\n  region        median_rate\n  <chr>               <dbl>\n1 North Central        1.97\n2 Northeast            1.80\n3 South                3.40\n4 West                 1.29\n\n\n\nThe summarize() function applies the summarization to each group separately.\nHere we use the same summarize() function and compute the average and sd of the variable height as we previous did.\nBut if the the data set is a grouped data frame grouped by sex, the summarized output will show the avg and sd for each group, make and female separately.\nAnother example is that we first group the murders data by region, then when we compute the median murder rate, it will get the median murder rate for each region.\nThis is quite helpful when we want to explore the relationship between numerical and categorical variables. Like here, we discover that the murder rate in the south is the highest.\nYou can absolutely include some statistics summary in your project proposal when you describe your data."
  },
  {
    "objectID": "slides/12-dplyr-1.html#sorting-rows-in-data-frames-arrange",
    "href": "slides/12-dplyr-1.html#sorting-rows-in-data-frames-arrange",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "Sorting Rows in Data Frames – arrange()\n",
    "text": "Sorting Rows in Data Frames – arrange()\n\n\n\narrange() orders entire data tables.\n\n\n## order the states by population size\nmurders |>  \n    arrange(population)\n\n# A tibble: 51 × 6\n  state                abb   region        population total   rate\n  <chr>                <chr> <chr>              <dbl> <dbl>  <dbl>\n1 Wyoming              WY    West              563626     5  0.887\n2 District of Columbia DC    South             601723    99 16.5  \n3 Vermont              VT    Northeast         625741     2  0.320\n4 North Dakota         ND    North Central     672591     4  0.595\n5 Alaska               AK    West              710231    19  2.68 \n6 South Dakota         SD    North Central     814180     8  0.983\n# ℹ 45 more rows\n\n\n\nFor ordering entire data tables, the dplyr function arrange() is useful.\nWe get to decide which column to sort by.\nIf we want to sort the observations of murders data set by population size, we just need to pipe the data set into arrange function and specify population variable.\nHow do we do the same thing with base R grammar, not tidyverse grammar?\nmurders[order(murders$population), ]"
  },
  {
    "objectID": "slides/12-dplyr-1.html#section-3",
    "href": "slides/12-dplyr-1.html#section-3",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "",
    "text": "15-dplyr  \nIn lab.qmd ## Lab 15 section, import the murders.csv data and\n\nAdd (mutate) the variable rate = total / population * 100000 to murders data (as I did).\nFilter states that are in region Northeast or West and their murder rate is less than 1.\nSelect variables state, region, rate.\n\n\nPrint the output table after you do 1. to 3., and save it as object my_states.\nGroup my_states by region. Then summarize data by creating variables avg and stdev that compute the mean and standard deviation of rate.\nArrange the summarized table by avg."
  },
  {
    "objectID": "slides/12-dplyr-1.html#section-4",
    "href": "slides/12-dplyr-1.html#section-4",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "",
    "text": "_______ <- _______ |> \n    mutate(_______) |> \n    filter(_______) |> \n    select(_______)\n\n_______ |>  \n    group_by(______) |> \n    summarize(______) |> \n    arrange(_______)\n\n\n\n\n          state    region  rate\n1        Hawaii      West 0.515\n2         Idaho      West 0.766\n3         Maine Northeast 0.828\n4 New Hampshire Northeast 0.380\n5        Oregon      West 0.940\n6          Utah      West 0.796\n7       Vermont Northeast 0.320\n8       Wyoming      West 0.887\n\n\n# A tibble: 2 × 3\n  region      avg std_dev\n  <fct>     <dbl>   <dbl>\n1 Northeast 0.509   0.278\n2 West      0.781   0.164"
  },
  {
    "objectID": "slides/12-dplyr-1.html#section-6",
    "href": "slides/12-dplyr-1.html#section-6",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\n\n\nmurders = pd.read_csv('./data/murders.csv')\nmurders.head(5)\n\n        state abb region  population  total\n0     Alabama  AL  South     4779736    135\n1      Alaska  AK   West      710231     19\n2     Arizona  AZ   West     6392017    232\n3    Arkansas  AR  South     2915918     93\n4  California  CA   West    37253956   1257"
  },
  {
    "objectID": "slides/12-dplyr-1.html#new-variables-.assign",
    "href": "slides/12-dplyr-1.html#new-variables-.assign",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "New Variables .assign\n",
    "text": "New Variables .assign\n\n\ndplyr::mutate()\nHave to use murders.total and murders.population instead of total and popution.\n\n\nmurders = murders.assign(\n    rate = round(murders.total / murders.population * 100000, 2))\n\n\nmurders.head(5)\n\n        state abb region  population  total  rate\n0     Alabama  AL  South     4779736    135  2.82\n1      Alaska  AK   West      710231     19  2.68\n2     Arizona  AZ   West     6392017    232  3.63\n3    Arkansas  AR  South     2915918     93  3.19\n4  California  CA   West    37253956   1257  3.37"
  },
  {
    "objectID": "slides/12-dplyr-1.html#filter-rows-.query",
    "href": "slides/12-dplyr-1.html#filter-rows-.query",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "Filter Rows .query\n",
    "text": "Filter Rows .query\n\n\ndplyr::filter()\nConditions must be a string to be evaluated!\nCannot write murders.rate, and should use rate.\n\n\nmurders.query(\"rate < 0.7\")\n\n            state abb         region  population  total  rate\n11         Hawaii  HI           West     1360301      7  0.51\n15           Iowa  IA  North Central     3046355     21  0.69\n29  New Hampshire  NH      Northeast     1316470      5  0.38\n34   North Dakota  ND  North Central      672591      4  0.59\n45        Vermont  VT      Northeast      625741      2  0.32"
  },
  {
    "objectID": "slides/12-dplyr-1.html#select-columns-.filter",
    "href": "slides/12-dplyr-1.html#select-columns-.filter",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "Select Columns .filter\n",
    "text": "Select Columns .filter\n\n\ndplyr::select()\nHave to be strings\n\n\nmurders.filter(items = ['region', 'rate', 'state'])\n\n           region   rate                 state\n0           South   2.82               Alabama\n1            West   2.68                Alaska\n2            West   3.63               Arizona\n3           South   3.19              Arkansas\n4            West   3.37            California\n5            West   1.29              Colorado\n6       Northeast   2.71           Connecticut\n7           South   4.23              Delaware\n8           South  16.45  District of Columbia\n9           South   3.40               Florida\n10          South   3.79               Georgia\n11           West   0.51                Hawaii\n12           West   0.77                 Idaho\n13  North Central   2.84              Illinois\n14  North Central   2.19               Indiana\n15  North Central   0.69                  Iowa\n16  North Central   2.21                Kansas\n17          South   2.67              Kentucky\n18          South   7.74             Louisiana\n19      Northeast   0.83                 Maine\n20          South   5.07              Maryland\n21      Northeast   1.80         Massachusetts\n22  North Central   4.18              Michigan\n23  North Central   1.00             Minnesota\n24          South   4.04           Mississippi\n25  North Central   5.36              Missouri\n26           West   1.21               Montana\n27  North Central   1.75              Nebraska\n28           West   3.11                Nevada\n29      Northeast   0.38         New Hampshire\n30      Northeast   2.80            New Jersey\n31           West   3.25            New Mexico\n32      Northeast   2.67              New York\n33          South   3.00        North Carolina\n34  North Central   0.59          North Dakota\n35  North Central   2.69                  Ohio\n36          South   2.96              Oklahoma\n37           West   0.94                Oregon\n38      Northeast   3.60          Pennsylvania\n39      Northeast   1.52          Rhode Island\n40          South   4.48        South Carolina\n41  North Central   0.98          South Dakota\n42          South   3.45             Tennessee\n43          South   3.20                 Texas\n44           West   0.80                  Utah\n45      Northeast   0.32               Vermont\n46          South   3.12              Virginia\n47           West   1.38            Washington\n48          South   1.46         West Virginia\n49  North Central   1.71             Wisconsin\n50           West   0.89               Wyoming"
  },
  {
    "objectID": "slides/12-dplyr-1.html#grouping-.groupby-.agg",
    "href": "slides/12-dplyr-1.html#grouping-.groupby-.agg",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "Grouping .groupby + .agg\n",
    "text": "Grouping .groupby + .agg\n\n\ndplyr::group_by() + dplyr::summarize()\n\n\nheights = pd.read_csv('./data/heights.csv')\n\n\nheights.groupby('sex')\n\n<pandas.core.groupby.generic.DataFrameGroupBy object at 0x145164fd0>\n\n\n\n\n## a data frame\nheights.groupby('sex').agg(['mean', 'std', 'median', 'min'])\n\n           height                           \n             mean       std     median   min\nsex                                         \nFemale  64.939424  3.760656  64.980315  51.0\nMale    69.314755  3.611024  69.000000  50.0"
  },
  {
    "objectID": "slides/12-dplyr-1.html#sorting-.sort_values",
    "href": "slides/12-dplyr-1.html#sorting-.sort_values",
    "title": "Data Wrangling - one data frame 🛠",
    "section": "Sorting .sort_values\n",
    "text": "Sorting .sort_values\n\n\ndplyr::arrange()\n\n\nmurders.sort_values('population').head(5)\n\n                   state abb         region  population  total   rate\n50               Wyoming  WY           West      563626      5   0.89\n8   District of Columbia  DC          South      601723     99  16.45\n45               Vermont  VT      Northeast      625741      2   0.32\n34          North Dakota  ND  North Central      672591      4   0.59\n1                 Alaska  AK           West      710231     19   2.68\n\n\n\n\ndplyr::arrange(desc())\n\n\nmurders.sort_values('rate', ascending = False).head(5)\n\n                   state abb         region  population  total   rate\n8   District of Columbia  DC          South      601723     99  16.45\n18             Louisiana  LA          South     4533372    351   7.74\n25              Missouri  MO  North Central     5988927    321   5.36\n20              Maryland  MD          South     5773552    293   5.07\n40        South Carolina  SC          South     4625364    207   4.48\n\n\n\n\n\nmath3570-s24.github.io/website"
  },
  {
    "objectID": "slides/07-package.html#tidyverse",
    "href": "slides/07-package.html#tidyverse",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "tidyverse 📦",
    "text": "tidyverse 📦\n\n\n\nThe tidyverse is a 📦 for data science.\nAll packages share common design philosophy, grammar, and data structures.\nThe core tidyverse packages include\n\n\nggplot2 - data visualisation\n\ndplyr - data manipulation\n\ntidyr - data tidying\n\nreadr - data importing\n\npurrr - functional programming\n\ntibble - improved data frame\n\nstringr - working with strings\n\nforcats - working with factors\n\nlubridate - working with date/times\n\n\n\n\n\n\n\n\n\nSource: https://github.com/spcanelon/tour-of-the-tidyverse"
  },
  {
    "objectID": "slides/07-package.html#workflow-of-data-science-with-r-packages",
    "href": "slides/07-package.html#workflow-of-data-science-with-r-packages",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "Workflow of Data Science with R packages",
    "text": "Workflow of Data Science with R packages\n\n\n\n\n\nSource: https://oliviergimenez.github.io/intro_tidyverse/#7"
  },
  {
    "objectID": "slides/07-package.html#install-and-load-tidyverse",
    "href": "slides/07-package.html#install-and-load-tidyverse",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "Install and Load tidyverse 📦",
    "text": "Install and Load tidyverse 📦\n\ntidyverse is loading all the core packages for us!\n\n\nlibrary(tidyverse)\n── Attaching core tidyverse packages ──────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ──────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package to force all conflicts to become errors\n\n\nPackages in the tidyverse change frequently. To check if updates are available:\n\n\ntidyverse_update()\nThe following packages are out of date:\n● dbplyr  (2.0.0 -> 2.1.0)\n● forcats (0.5.0 -> 0.5.1)\n● pillar  (1.4.7 -> 1.5.0)\nStart a clean R session then run:\ninstall.packages(c(\"dbplyr\", \"forcats\", \"pillar\"))"
  },
  {
    "objectID": "slides/07-package.html#tidy-data-data-matrix",
    "href": "slides/07-package.html#tidy-data-data-matrix",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "Tidy Data (Data Matrix)",
    "text": "Tidy Data (Data Matrix)\n\n“Happy families are all alike; every unhappy family is unhappy in its own way.” – Leo Tolstoy\n\n\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell. (match the corresponding row observation and column variable)"
  },
  {
    "objectID": "slides/07-package.html#tidy-data-data-matrix-1",
    "href": "slides/07-package.html#tidy-data-data-matrix-1",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "Tidy Data (Data Matrix)",
    "text": "Tidy Data (Data Matrix)\n\n“Tidy datasets are all alike, but every messy dataset is messy in its own way.” – Hadley Wickham\n\n\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell. (match the corresponding row observation and column variable)\n\n\nRemember in this course we are working on the so-called tidy data.\nOne of the goals of tidyverse is to make our data tidy.\nSo before we actually get into tidyverse, Let’s define tidy data first.\n“Tidy datasets are all alike, but every messy dataset is messy in its own way.”\nTidy data require that\n\nEach variable must have its own column. (name-value match)\nEach observation must have its own row. The observation could be human being or any object observed in the data set. A row corresponds to an observation.\nEach value must have its own cell."
  },
  {
    "objectID": "slides/07-package.html#why-tidy-data",
    "href": "slides/07-package.html#why-tidy-data",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "Why Tidy Data?",
    "text": "Why Tidy Data?\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\n\n\nAdvantages of tidy data:\n\nIf you store all data in a tidy way, you only need to learn the tools that work with them.\nPlacing variables in columns allows R’s vectorised nature to shine. That makes transforming tidy data feel natural.\n\n\n\n\n\n\n\nPractical instructions:\n\nPut each dataset in a data frame.\n\n\nPut each variable in a column.\n\n\n\n\nThese three rules are interrelated; it’s impossible to only satisfy two of the three.\nRemember, “Tidy datasets are all alike, but every messy dataset is messy in its own way.”\n\nRemember, R prefer vectorized programming style. If each variable corresponds to a column vector, it is easy to transform variables using vectorised functions.\nWe will learn how to make our data tidy using tidyr, dplyr and other packages in Data Wrangling weeks."
  },
  {
    "objectID": "slides/07-package.html#data-frames-store-tidy-data",
    "href": "slides/07-package.html#data-frames-store-tidy-data",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "Data Frames Store Tidy Data",
    "text": "Data Frames Store Tidy Data\n\nCollecting information about the distributions of colors and defects in a bag of M&Ms.\n\n\n\n\nNon-tidy Data\n\n\nIf you import data in this format into R/Python, you will be in a mess.\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Data\n\n\nEach row is for one M&M. Each variable is in each column. One value is in a cell.\nDon’t code “Red” in one place and “RED” in another. Be consistent!\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou are gonna find it difficult to deal with this data when you do later visualization and data analysis.\nDon’t code “Red” with capital R in one place and “RED” with all capital letters in another. R treats those as two values.\nconsistent in labeling values for categorical variables."
  },
  {
    "objectID": "slides/07-package.html#tibbles",
    "href": "slides/07-package.html#tibbles",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "Tibbles",
    "text": "Tibbles\n\nTibbles are modern version of R data frames.\nCreate a new tibble using tibble().\nIt is like base::data.frame(), but with a couple differences.\n\n\n\n\ndf <- data.frame(x = 1:5, \n                 y = letters[1:5], \n                 z = 5:1)\ndf\n\n  x y z\n1 1 a 5\n2 2 b 4\n3 3 c 3\n4 4 d 2\n5 5 e 1\n\n\n\nclass(df)\n\n[1] \"data.frame\"\n\n\n\n\n\ntib <- tibble(x = 1:5, \n              y = letters[1:5], \n              z = 5:1)\ntib\n\n# A tibble: 5 × 3\n      x y         z\n  <int> <chr> <int>\n1     1 a         5\n2     2 b         4\n3     3 c         3\n4     4 d         2\n5     5 e         1\n\n\n\nclass(tib)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\n\n\nNow we are going discuss the differences between tibble and data frame."
  },
  {
    "objectID": "slides/07-package.html#printing-of-data.frame-class",
    "href": "slides/07-package.html#printing-of-data.frame-class",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "Printing of data.frame Class",
    "text": "Printing of data.frame Class\n\n\nHow the printing method of data.frame can be improved? (Check iris in your R console)\n\n\nclass(iris)\n\n[1] \"data.frame\"\n\n\n\niris\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1            5.1         3.5          1.4         0.2     setosa\n2            4.9         3.0          1.4         0.2     setosa\n3            4.7         3.2          1.3         0.2     setosa\n4            4.6         3.1          1.5         0.2     setosa\n5            5.0         3.6          1.4         0.2     setosa\n6            5.4         3.9          1.7         0.4     setosa\n7            4.6         3.4          1.4         0.3     setosa\n8            5.0         3.4          1.5         0.2     setosa\n9            4.4         2.9          1.4         0.2     setosa\n10           4.9         3.1          1.5         0.1     setosa\n11           5.4         3.7          1.5         0.2     setosa\n12           4.8         3.4          1.6         0.2     setosa\n13           4.8         3.0          1.4         0.1     setosa\n14           4.3         3.0          1.1         0.1     setosa\n15           5.8         4.0          1.2         0.2     setosa\n16           5.7         4.4          1.5         0.4     setosa\n17           5.4         3.9          1.3         0.4     setosa\n18           5.1         3.5          1.4         0.3     setosa\n19           5.7         3.8          1.7         0.3     setosa\n20           5.1         3.8          1.5         0.3     setosa\n21           5.4         3.4          1.7         0.2     setosa\n22           5.1         3.7          1.5         0.4     setosa\n23           4.6         3.6          1.0         0.2     setosa\n24           5.1         3.3          1.7         0.5     setosa\n25           4.8         3.4          1.9         0.2     setosa\n26           5.0         3.0          1.6         0.2     setosa\n27           5.0         3.4          1.6         0.4     setosa\n28           5.2         3.5          1.5         0.2     setosa\n29           5.2         3.4          1.4         0.2     setosa\n30           4.7         3.2          1.6         0.2     setosa\n31           4.8         3.1          1.6         0.2     setosa\n32           5.4         3.4          1.5         0.4     setosa\n33           5.2         4.1          1.5         0.1     setosa\n34           5.5         4.2          1.4         0.2     setosa\n35           4.9         3.1          1.5         0.2     setosa\n36           5.0         3.2          1.2         0.2     setosa\n37           5.5         3.5          1.3         0.2     setosa\n38           4.9         3.6          1.4         0.1     setosa\n39           4.4         3.0          1.3         0.2     setosa\n40           5.1         3.4          1.5         0.2     setosa\n41           5.0         3.5          1.3         0.3     setosa\n42           4.5         2.3          1.3         0.3     setosa\n43           4.4         3.2          1.3         0.2     setosa\n44           5.0         3.5          1.6         0.6     setosa\n45           5.1         3.8          1.9         0.4     setosa\n46           4.8         3.0          1.4         0.3     setosa\n47           5.1         3.8          1.6         0.2     setosa\n48           4.6         3.2          1.4         0.2     setosa\n49           5.3         3.7          1.5         0.2     setosa\n50           5.0         3.3          1.4         0.2     setosa\n51           7.0         3.2          4.7         1.4 versicolor\n52           6.4         3.2          4.5         1.5 versicolor\n53           6.9         3.1          4.9         1.5 versicolor\n54           5.5         2.3          4.0         1.3 versicolor\n55           6.5         2.8          4.6         1.5 versicolor\n56           5.7         2.8          4.5         1.3 versicolor\n57           6.3         3.3          4.7         1.6 versicolor\n58           4.9         2.4          3.3         1.0 versicolor\n59           6.6         2.9          4.6         1.3 versicolor\n60           5.2         2.7          3.9         1.4 versicolor\n61           5.0         2.0          3.5         1.0 versicolor\n62           5.9         3.0          4.2         1.5 versicolor\n63           6.0         2.2          4.0         1.0 versicolor\n64           6.1         2.9          4.7         1.4 versicolor\n65           5.6         2.9          3.6         1.3 versicolor\n66           6.7         3.1          4.4         1.4 versicolor\n67           5.6         3.0          4.5         1.5 versicolor\n68           5.8         2.7          4.1         1.0 versicolor\n69           6.2         2.2          4.5         1.5 versicolor\n70           5.6         2.5          3.9         1.1 versicolor\n71           5.9         3.2          4.8         1.8 versicolor\n72           6.1         2.8          4.0         1.3 versicolor\n73           6.3         2.5          4.9         1.5 versicolor\n74           6.1         2.8          4.7         1.2 versicolor\n75           6.4         2.9          4.3         1.3 versicolor\n76           6.6         3.0          4.4         1.4 versicolor\n77           6.8         2.8          4.8         1.4 versicolor\n78           6.7         3.0          5.0         1.7 versicolor\n79           6.0         2.9          4.5         1.5 versicolor\n80           5.7         2.6          3.5         1.0 versicolor\n81           5.5         2.4          3.8         1.1 versicolor\n82           5.5         2.4          3.7         1.0 versicolor\n83           5.8         2.7          3.9         1.2 versicolor\n84           6.0         2.7          5.1         1.6 versicolor\n85           5.4         3.0          4.5         1.5 versicolor\n86           6.0         3.4          4.5         1.6 versicolor\n87           6.7         3.1          4.7         1.5 versicolor\n88           6.3         2.3          4.4         1.3 versicolor\n89           5.6         3.0          4.1         1.3 versicolor\n90           5.5         2.5          4.0         1.3 versicolor\n91           5.5         2.6          4.4         1.2 versicolor\n92           6.1         3.0          4.6         1.4 versicolor\n93           5.8         2.6          4.0         1.2 versicolor\n94           5.0         2.3          3.3         1.0 versicolor\n95           5.6         2.7          4.2         1.3 versicolor\n96           5.7         3.0          4.2         1.2 versicolor\n97           5.7         2.9          4.2         1.3 versicolor\n98           6.2         2.9          4.3         1.3 versicolor\n99           5.1         2.5          3.0         1.1 versicolor\n100          5.7         2.8          4.1         1.3 versicolor\n101          6.3         3.3          6.0         2.5  virginica\n102          5.8         2.7          5.1         1.9  virginica\n103          7.1         3.0          5.9         2.1  virginica\n104          6.3         2.9          5.6         1.8  virginica\n105          6.5         3.0          5.8         2.2  virginica\n106          7.6         3.0          6.6         2.1  virginica\n107          4.9         2.5          4.5         1.7  virginica\n108          7.3         2.9          6.3         1.8  virginica\n109          6.7         2.5          5.8         1.8  virginica\n110          7.2         3.6          6.1         2.5  virginica\n111          6.5         3.2          5.1         2.0  virginica\n112          6.4         2.7          5.3         1.9  virginica\n113          6.8         3.0          5.5         2.1  virginica\n114          5.7         2.5          5.0         2.0  virginica\n115          5.8         2.8          5.1         2.4  virginica\n116          6.4         3.2          5.3         2.3  virginica\n117          6.5         3.0          5.5         1.8  virginica\n118          7.7         3.8          6.7         2.2  virginica\n119          7.7         2.6          6.9         2.3  virginica\n120          6.0         2.2          5.0         1.5  virginica\n121          6.9         3.2          5.7         2.3  virginica\n122          5.6         2.8          4.9         2.0  virginica\n123          7.7         2.8          6.7         2.0  virginica\n124          6.3         2.7          4.9         1.8  virginica\n125          6.7         3.3          5.7         2.1  virginica\n126          7.2         3.2          6.0         1.8  virginica\n127          6.2         2.8          4.8         1.8  virginica\n128          6.1         3.0          4.9         1.8  virginica\n129          6.4         2.8          5.6         2.1  virginica\n130          7.2         3.0          5.8         1.6  virginica\n131          7.4         2.8          6.1         1.9  virginica\n132          7.9         3.8          6.4         2.0  virginica\n133          6.4         2.8          5.6         2.2  virginica\n134          6.3         2.8          5.1         1.5  virginica\n135          6.1         2.6          5.6         1.4  virginica\n136          7.7         3.0          6.1         2.3  virginica\n137          6.3         3.4          5.6         2.4  virginica\n138          6.4         3.1          5.5         1.8  virginica\n139          6.0         3.0          4.8         1.8  virginica\n140          6.9         3.1          5.4         2.1  virginica\n141          6.7         3.1          5.6         2.4  virginica\n142          6.9         3.1          5.1         2.3  virginica\n143          5.8         2.7          5.1         1.9  virginica\n144          6.8         3.2          5.9         2.3  virginica\n145          6.7         3.3          5.7         2.5  virginica\n146          6.7         3.0          5.2         2.3  virginica\n147          6.3         2.5          5.0         1.9  virginica\n148          6.5         3.0          5.2         2.0  virginica\n149          6.2         3.4          5.4         2.3  virginica\n150          5.9         3.0          5.1         1.8  virginica\n\n\n\nprint entire data set out.\ndoes not show much information."
  },
  {
    "objectID": "slides/07-package.html#tibbles-display-better",
    "href": "slides/07-package.html#tibbles-display-better",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "Tibbles Display Better",
    "text": "Tibbles Display Better\n\n\nas_tibble() turns a data frame or matrix into a tibble.\n\n\n(iris_tbl <- as_tibble(iris))  ## check iris_tbl in your R console\n\n# A tibble: 150 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n         <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n1          5.1         3.5          1.4         0.2 setosa \n2          4.9         3            1.4         0.2 setosa \n3          4.7         3.2          1.3         0.2 setosa \n4          4.6         3.1          1.5         0.2 setosa \n5          5           3.6          1.4         0.2 setosa \n6          5.4         3.9          1.7         0.4 setosa \n# ℹ 144 more rows\n\n\n\nOnly shows the first couple of rows.\nPrints data size and column type.\n\n\nOnly shows the first 10 rows and all the columns that fit on one screen. This makes it easier to work with large data.\nPrints data size and column type.\nControl the default appearance, like options(tibble.print_max = n, tibble.print_min = m): if there are more than \\(n\\) rows, print only the first \\(m\\) rows.\n\nhttps://stackoverflow.com/questions/36848785/how-do-i-reset-all-options-arguments-to-their-default-values"
  },
  {
    "objectID": "slides/07-package.html#subsets-of-basedata.frame-may-not-be-data-frames",
    "href": "slides/07-package.html#subsets-of-basedata.frame-may-not-be-data-frames",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "Subsets of base::data.frame May Not be Data Frames",
    "text": "Subsets of base::data.frame May Not be Data Frames\n\nSometimes [] returns a data frame and sometimes it just returns a vector.\n\n\n\n\ndf <- data.frame(x = 1:3, \n                 y = 3:1, \n                 z = LETTERS[1:3])\ndf[, 1:2]\n\n  x y\n1 1 3\n2 2 2\n3 3 1\n\nclass(df[, 1:2])\n\n[1] \"data.frame\"\n\n\n\ndf[, 1]\n\n[1] 1 2 3\n\nclass(df[, 1])\n\n[1] \"integer\"\n\n\n\n\n\nTreat the df as a list. How do we grab the 1st column and preserve its data frame type?\n\n\n\ndf[1]\n\n  x\n1 1\n2 2\n3 3\n\n\n\n\n\n\n\n\n\nRemember we can use [] to subset a data frame. Right?\ndf[, 1, drop=FALSE]"
  },
  {
    "objectID": "slides/07-package.html#subsets-of-tibbles-are-tibbles",
    "href": "slides/07-package.html#subsets-of-tibbles-are-tibbles",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "Subsets of Tibbles Are Tibbles",
    "text": "Subsets of Tibbles Are Tibbles\n\n\n\n[] always returns another tibble.\n\n\ndf_tbl <- tibble(x = 1:2, y = 2:1, \n                 z = LETTERS[1:2])\ndf_tbl[, 1]\n\n# A tibble: 2 × 1\n      x\n  <int>\n1     1\n2     2\n\ndf_tbl[1]\n\n# A tibble: 2 × 1\n      x\n  <int>\n1     1\n2     2\n\n\n\nclass(df_tbl[, 1])\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nclass(df_tbl[1])\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\n\n$ and [[]] return a vector.\n\n\ndf_tbl$x\n\n[1] 1 2\n\nclass(df_tbl$x) \n\n[1] \"integer\"\n\n\n\n\ndf_tbl[[1]]\n\n[1] 1 2\n\nclass(df_tbl[[1]])  \n\n[1] \"integer\"\n\n\n\n\n\n\nTibbles are quite strict about subsetting"
  },
  {
    "objectID": "slides/07-package.html#tibbles-never-do-partial-matching",
    "href": "slides/07-package.html#tibbles-never-do-partial-matching",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "Tibbles Never Do Partial Matching",
    "text": "Tibbles Never Do Partial Matching\n\n\n\nData frames do partial matching\nTreat name “a” as “abc”!\n\n\n(df <- data.frame(abc = 1))\n\n  abc\n1   1\n\ndf$a\n\n[1] 1\n\n\n\n\n\nTibbles never do partial matching\nName “a” cannot be recognized!\n\n\n(tib <- tibble(abc = 1))\n\n# A tibble: 1 × 1\n    abc\n  <dbl>\n1     1\n\ntib$a\n\nWarning: Unknown or uninitialised column: `a`.\n\n\nNULL"
  },
  {
    "objectID": "slides/07-package.html#tibbles-can-have-complex-entries",
    "href": "slides/07-package.html#tibbles-can-have-complex-entries",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "Tibbles Can Have Complex Entries",
    "text": "Tibbles Can Have Complex Entries\n\n\n\nData frame: Columns can’t be defined using other created variables.\n\n\ndata.frame(x = 1:5, \n           y = 1:5, \n           z = x + 3)\n# object 'x' not found\n\n\n\nTibble: Allow to refer to created variables.\n\n\ntibble(x = 1:5, \n       y = 1:5, \n       z = x + 3)\n\n# A tibble: 5 × 3\n      x     y     z\n  <int> <int> <dbl>\n1     1     1     4\n2     2     2     5\n3     3     3     6\n4     4     4     7\n5     5     5     8\n\n\n\n\n\nColumns need to be atomic vectors of numbers, strings, or logical values\nCan have more complex objects, such as lists or functions."
  },
  {
    "objectID": "slides/07-package.html#pipes",
    "href": "slides/07-package.html#pipes",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "Pipes",
    "text": "Pipes\n\nThe pipe %>% comes from the magrittr package of tidyverse.\n\n\n\nR (4.1+) has native base pipe operator |>. Tools > Global Options > Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor simple cases |> and %>% behave identically. The base pipe is recommended because we can use |> anywhere anytime in R, even we don’t use tidyverse.\n\n\n\n\nhttps://r4ds.hadley.nz/data-transform.html#sec-the-pipe"
  },
  {
    "objectID": "slides/07-package.html#what-and-how-to-use-pipe",
    "href": "slides/07-package.html#what-and-how-to-use-pipe",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "What and How to Use Pipe",
    "text": "What and How to Use Pipe\n\nTo add the pipe, use keyboard shortcut Ctrl/Cmd + Shift + M\nThe pipe sends the result of the left side of the pipe to be the first argument of the function on the right side of the pipe.\n\n\n\n16 |> sqrt() |> log2()\n\n[1] 2\n\nlog2(sqrt(16))\n\n[1] 2\n\n## We can define other arguments as if the first argument is already defined\n16 |> sqrt() |> log(base = 2)\n\n[1] 2"
  },
  {
    "objectID": "slides/07-package.html#why-pipe-operator",
    "href": "slides/07-package.html#why-pipe-operator",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "Why Pipe Operator?",
    "text": "Why Pipe Operator?\n\nNested vs. Sequential-piped\nMore natural and easier-to-read structure\n\n\n\n\n\n\nSource: https://www.andrewheiss.com/\n\n\n\n\n\n\nNested vs. Sequential-piped\nWriting it out using pipes give it a more natural and easier-to-read structure.\nYou can think about the following sequence of actions - wake up, get out of bed, get dressed, and leave house.\nExpressed as a set of nested functions in R would look like\nWriting it out using pipes give it a more natural and easier to read structure (if you think so)\nLike if-else statements, people don’t like too many nested layers. We can easily get lost about what is actually doing.\nUsing pipe operator, we have a clear sequential order, and all the actions or functions will be shown at the beginning of each line, making the code easier to read."
  },
  {
    "objectID": "slides/07-package.html#section-6",
    "href": "slides/07-package.html#section-6",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "",
    "text": "08-Tibbles and Pipes \nIn lab.qmd ## Lab 8 section,\n\nCompare and contrast the following operations on a data.frame and equivalent tibble. What are the differences? Please comment.\n\n\n\n\ndf <- data.frame(abc = 1:2, \n                 xyz = c(\"a\", \"b\"))\n# list method\ndf$x\ndf[[2]]\ndf[\"xyz\"]\ndf[c(\"abc\", \"xyz\")]\n# matrix method\ndf[, 2]\ndf[, \"xyz\"]\ndf[, c(\"abc\", \"xyz\")]\n\n\n\ntib <- tibble(abc = 1:2, \n              xyz = c(\"a\", \"b\"))\n# list method\ntib$x\ntib[[2]]\ntib[\"xyz\"]\ntib[c(\"abc\", \"xyz\")]\n# matrix method\ntib[, 2]\ntib[, \"xyz\"]\ntib[, c(\"abc\", \"xyz\")]\n\n\n\n\nUse |> to first select last 12 rows of iris data set using tail(), then provides summary statistics on its columns using summary()."
  },
  {
    "objectID": "slides/07-package.html#pandas",
    "href": "slides/07-package.html#pandas",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "Pandas",
    "text": "Pandas\n\nLike tidyverse in R, pandas is a Python library that provides data structures, manipulation and analysis tools for data science.\n\n\nimport numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "slides/07-package.html#pandas-data-frame",
    "href": "slides/07-package.html#pandas-data-frame",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "Pandas Data Frame",
    "text": "Pandas Data Frame\n\nCreate a data frame from a dictionary\n\n\ndata = {\"math\": [99, 65, 87], \"stats\": [92, 48, 88], \"cs\": [50, 88, 94]}\n\ndf = pd.DataFrame(data)\nprint(df) \n\n   math  stats  cs\n0    99     92  50\n1    65     48  88\n2    87     88  94\n\n\n\n\nRow and column names\n\n\ndf.index = [\"s1\", \"s2\", \"s3\"]\ndf.columns = [\"Math\", \"Stat\", \"CS\"]\ndf\n\n    Math  Stat  CS\ns1    99    92  50\ns2    65    48  88\ns3    87    88  94"
  },
  {
    "objectID": "slides/07-package.html#subsetting-columns",
    "href": "slides/07-package.html#subsetting-columns",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "Subsetting Columns",
    "text": "Subsetting Columns\n\n\n\n\n\n\nWarning\n\n\n\n\nIn Python, [] returns Series, [[]] returns DataFrame!\nIn R, [] returns tibble, [[]] returns vector!\n\n\n\n\n\n\n\n\n\n## Series\ndf[\"Math\"]\n\ns1    99\ns2    65\ns3    87\nName: Math, dtype: int64\n\ntype(df[\"Math\"])\n\n<class 'pandas.core.series.Series'>\n\n\n\n\n\n# ## DataFrame\ndf[[\"Math\"]]\n\n    Math\ns1    99\ns2    65\ns3    87\n\ntype(df[[\"Math\"]])\n\n<class 'pandas.core.frame.DataFrame'>\n\n\n\ndf[[\"Math\", \"CS\"]]\n\n    Math  CS\ns1    99  50\ns2    65  88\ns3    87  94\n\n\n\n\nisinstance(df[[“Math”]], pd.DataFrame)"
  },
  {
    "objectID": "slides/07-package.html#subsetting-rows-dataframe.iloc",
    "href": "slides/07-package.html#subsetting-rows-dataframe.iloc",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "Subsetting Rows DataFrame.iloc\n",
    "text": "Subsetting Rows DataFrame.iloc\n\n\n\ninteger-location based indexing for selection by position\n\n\n\n\ndf \n\n    Math  Stat  CS\ns1    99    92  50\ns2    65    48  88\ns3    87    88  94\n\n\n\n## first row Series\ndf.iloc[0] \n\nMath    99\nStat    92\nCS      50\nName: s1, dtype: int64\n\n\n\n## first row DataFrame\ndf.iloc[[0]]\n\n    Math  Stat  CS\ns1    99    92  50\n\n\n\n\n## first 2 rows\ndf.iloc[[0, 1]]\n\n    Math  Stat  CS\ns1    99    92  50\ns2    65    48  88\n\n\n\n## 1st and 3rd row\ndf.iloc[[True, False, True]]\n\n    Math  Stat  CS\ns1    99    92  50\ns3    87    88  94"
  },
  {
    "objectID": "slides/07-package.html#subsetting-rows-and-columns-dataframe.iloc",
    "href": "slides/07-package.html#subsetting-rows-and-columns-dataframe.iloc",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "Subsetting Rows and Columns DataFrame.iloc\n",
    "text": "Subsetting Rows and Columns DataFrame.iloc\n\n\n\n\ndf \n\n    Math  Stat  CS\ns1    99    92  50\ns2    65    48  88\ns3    87    88  94\n\n\n\n## (1, 3) row and (1, 3) col\ndf.iloc[[0, 2], [0, 2]]\n\n    Math  CS\ns1    99  50\ns3    87  94\n\n\n\n## all rows and 1st col\ndf.iloc[:, [True, False, False]]\n\n    Math\ns1    99\ns2    65\ns3    87\n\n\n\n\ndf.iloc[0:2, 1:3]\n\n    Stat  CS\ns1    92  50\ns2    48  88"
  },
  {
    "objectID": "slides/07-package.html#subsetting-rows-and-columns-dataframe.loc",
    "href": "slides/07-package.html#subsetting-rows-and-columns-dataframe.loc",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "Subsetting Rows and Columns DataFrame.loc\n",
    "text": "Subsetting Rows and Columns DataFrame.loc\n\nAccess a group of rows and columns by label(s)\n\n\n\ndf \n\n    Math  Stat  CS\ns1    99    92  50\ns2    65    48  88\ns3    87    88  94\n\n\n\ndf.loc['s1', \"CS\"]\n\n50\n\n\n\n## all rows and 1st col\ndf.loc['s1':'s3', [True, False, False]]\n\n    Math\ns1    99\ns2    65\ns3    87\n\n\n\n\ndf.loc['s2', ['Math', 'Stat']]\n\nMath    65\nStat    48\nName: s2, dtype: int64"
  },
  {
    "objectID": "slides/07-package.html#obtain-a-single-cell-value-dataframe.iat-dataframe.at",
    "href": "slides/07-package.html#obtain-a-single-cell-value-dataframe.iat-dataframe.at",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "Obtain a Single Cell Value DataFrame.iat/ DataFrame.at\n",
    "text": "Obtain a Single Cell Value DataFrame.iat/ DataFrame.at\n\n\n\n\ndf \n\n    Math  Stat  CS\ns1    99    92  50\ns2    65    48  88\ns3    87    88  94\n\n\n\ndf.iat[1, 2]\n\n88\n\n\n\ndf.iloc[0].iat[1]\n\n92\n\n\n\n\ndf.at['s2', 'Stat']\n\n48\n\n\n\ndf.loc['s1'].at['Stat']\n\n92"
  },
  {
    "objectID": "slides/07-package.html#new-columns-dataframe.insert-and-new-rows-pd.concat",
    "href": "slides/07-package.html#new-columns-dataframe.insert-and-new-rows-pd.concat",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "New Columns DataFrame.insert and New Rows pd.concat\n",
    "text": "New Columns DataFrame.insert and New Rows pd.concat\n\n\n\n\ndf \n\n    Math  Stat  CS\ns1    99    92  50\ns2    65    48  88\ns3    87    88  94\n\n\n\ndf.insert(loc = 2, \n          column = \"Chem\", \n          value = [77, 89, 76])\ndf\n\n    Math  Stat  Chem  CS\ns1    99    92    77  50\ns2    65    48    89  88\ns3    87    88    76  94\n\n\n\n\n\ndf1 = pd.DataFrame({\n    \"Math\": 88, \n    \"Stat\": 99, \n    \"Chem\": 0, \n    \"CS\": 100\n    }, index = ['s4'])\n\n\npd.concat(objs = [df, df1])\n\n    Math  Stat  Chem   CS\ns1    99    92    77   50\ns2    65    48    89   88\ns3    87    88    76   94\ns4    88    99     0  100\n\n\n\n\n\n\npd.concat(objs = [df, df1], \n          ignore_index = True)"
  },
  {
    "objectID": "slides/07-package.html#numpy-for-arraysmatrices",
    "href": "slides/07-package.html#numpy-for-arraysmatrices",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "NumPy for arrays/matrices",
    "text": "NumPy for arrays/matrices\n\nThe array object in NumPy is called ndarray.\nUse array() to create an array.\n\n\nrange(0, 5, 1) # a seq of number from 0 to 4 with increment of 1\n\nrange(0, 5)\n\nlist(range(0, 5, 1))\n\n[0, 1, 2, 3, 4]\n\n\n\n\nimport numpy as np\narr = np.array(range(0, 5, 1)) ## One-dim array \narr\n\narray([0, 1, 2, 3, 4])\n\ntype(arr)\n\n<class 'numpy.ndarray'>\n\n\n\n\nNumPy is used to work with arrays/matrices"
  },
  {
    "objectID": "slides/07-package.html#d-array-vector-and-2d-array-matrix",
    "href": "slides/07-package.html#d-array-vector-and-2d-array-matrix",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "1D Array (Vector) and 2D Array (Matrix)",
    "text": "1D Array (Vector) and 2D Array (Matrix)\n\n\nnp.arange: Efficient way to create a one-dim array of sequence of numbers\n\n\nnp.arange(2, 5)\n\narray([2, 3, 4])\n\nnp.arange(6, 0, -1)\n\narray([6, 5, 4, 3, 2, 1])\n\n\n\n\n2D array\n\n\nnp.array([[1, 2, 3], [4, 5, 6]])\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\n\nnp.array([[[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6]]])\n\narray([[[1, 2, 3],\n        [4, 5, 6]],\n\n       [[1, 2, 3],\n        [4, 5, 6]]])"
  },
  {
    "objectID": "slides/07-package.html#np.reshape",
    "href": "slides/07-package.html#np.reshape",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "np.reshape()",
    "text": "np.reshape()\n\narr2 = np.arange(8).reshape(2, 4)\narr2\n\narray([[0, 1, 2, 3],\n       [4, 5, 6, 7]])\n\narr2.shape  \n\n(2, 4)\n\n\n\narr2.ndim\n\n2\n\n\n\narr2.size\n\n8"
  },
  {
    "objectID": "slides/07-package.html#stacking-arrays",
    "href": "slides/07-package.html#stacking-arrays",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "Stacking Arrays",
    "text": "Stacking Arrays\n\na = np.array([1, 2, 3, 4]).reshape(2, 2)\nb = np.array([5, 6, 7, 8]).reshape(2, 2)\n\nnp.vstack((a, b))\n\narray([[1, 2],\n       [3, 4],\n       [5, 6],\n       [7, 8]])\n\n\n\nnp.hstack((a, b))\n\narray([[1, 2, 5, 6],\n       [3, 4, 7, 8]])"
  },
  {
    "objectID": "slides/07-package.html#section-9",
    "href": "slides/07-package.html#section-9",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "",
    "text": "09-NumPy and pandas \nIn lab.qmd ## Lab 9 section, create a Python pandas.DataFrame equivalent to the R tibble\n\ntibble(x = 1:5, y = 5:1, z = LETTERS[1:5])\n\n# A tibble: 5 × 3\n      x     y z    \n  <int> <int> <chr>\n1     1     5 A    \n2     2     4 B    \n3     3     3 C    \n4     4     2 D    \n5     5     1 E    \n\n\n\nimport numpy as np\nimport pandas as pd\nimport string as st\nlist(st.ascii_uppercase)\ndic = {'__': np.arange(__, __), \n       '__': np.arange(__, __, __),\n       '__': list(__.ascii_uppercase)[___]}\npd._____________(dic)"
  },
  {
    "objectID": "slides/07-package.html#lab-bonus",
    "href": "slides/07-package.html#lab-bonus",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "Lab Bonus!",
    "text": "Lab Bonus!\n\nHappy Ralentine’s Day! ❤️\n\n\nx <- seq(0, 2*pi, by = 0.01)\nxhrt <- 16 * sin(x) ^ 3\nyhrt <- 13 * cos(x) - 5 * cos(2*x) - 2 * cos(3*x) - cos(4*x)\npar(mar = c(0, 0, 0, 0))\nplot(xhrt, yhrt, type = \"l\", axes = FALSE, xlab = \"\", ylab = \"\")\npolygon(xhrt, yhrt, col = \"red\", border = NA)\npoints(c(10,-10, -15, 15), c(-10, -10, 10, 10), pch = 169, font = 5)\ntext(0, 0, \"Happy Valentine's Day!\", font = 2, cex = 2, col = \"pink\")"
  },
  {
    "objectID": "slides/07-package.html#lab-bonus-1",
    "href": "slides/07-package.html#lab-bonus-1",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "Lab Bonus!",
    "text": "Lab Bonus!\n\nHappy Pylentine’s Day! ❤️\n\n\nlines = []\nmsg = \"~Happy Valentine's Day!~\"\nfor y in range(15, -15, -1):\n    line = \"\"\n    for x in range(-30, 30):\n        f = ((x * 0.05) ** 2 + (y * 0.1) ** 2 - 1) ** 3 - (x * 0.05) ** 2 * (y * 0.1) ** 3\n        line += msg[(x - y) % len(msg)] if f <= 0 else \" \"\n    lines.append(line)\n\nprint(\"\\n\".join(lines))"
  },
  {
    "objectID": "slides/07-package.html#resources",
    "href": "slides/07-package.html#resources",
    "title": "R/Python Data Frames for Data Science \n",
    "section": "Resources",
    "text": "Resources\n\ntibble\npipes\nNumPy\npandas\n\n\n\n\nmath3570-s24.github.io/website"
  },
  {
    "objectID": "slides/04-git.html#git-and-github",
    "href": "slides/04-git.html#git-and-github",
    "title": "Git and GitHub \n",
    "section": "Git and GitHub",
    "text": "Git and GitHub\n\n\n\n\n\n\n\n\n\n\n\n\nGit is a version control system – like Track Changes features from MS Word, on steroids.\nIt’s not the only version control system, but it’s a popular one.\n\n\n\n\n\n\n\n\n\n\n\n\nGitHub is a home for Git-based projects (repository, or repo) on the cloud – like Dropbox but much better.\nWe’ll use GitHub for web hosting homework."
  },
  {
    "objectID": "slides/04-git.html#versioning",
    "href": "slides/04-git.html#versioning",
    "title": "Git and GitHub \n",
    "section": "Versioning",
    "text": "Versioning\n\n\n\n\n\nSource: Data Science in a Box Unit 1 - Deck 3\n\n\n\n\n\n\nSo think about this figure here. Versioning is like this. If you are playing Lego, and you want to build a house using Lego bricks, you would lay them out layer by layer, and you might along the way actually record what’s happening at each step.\nBase floor, walls, pillars, roof."
  },
  {
    "objectID": "slides/04-git.html#versioning-with-human-readable-messages",
    "href": "slides/04-git.html#versioning-with-human-readable-messages",
    "title": "Git and GitHub \n",
    "section": "Versioning with human readable messages",
    "text": "Versioning with human readable messages\n\nInformative messages: let the next person know what happened with each change.\n\n\n\n\n\n\nSource: Data Science in a Box Unit 1 - Deck 3\n\n\n\n\n\n\nVersion control with git feels a lit more like this, where you have the versions, but then you actually have some human readable messages.\nRemember usually there are several people working on the same project, Adding a message on a particular version of your work let other people know what changes you made in this version comparing to the previous version.\nEven the project is an individual project, adding messages remind ourselves of what we did previously at each stage of the work.\nIn Git, The first commit which is a human readable message you make in a repository is always called the first commit or initial commit.\nThen after that you get to choose what you wanna say about it.\nLike here in version 2, we have built back and front of the base, and then in version 3, finished building base, so we are actually letting the next person who come after you that could be you looking back at a project of yours or somebody else you are collaborating with know what happened with each change  \n\nInstallation instruction.\nfurniture assembly guide"
  },
  {
    "objectID": "slides/04-git.html#why-need-version-control",
    "href": "slides/04-git.html#why-need-version-control",
    "title": "Git and GitHub \n",
    "section": "Why Need Version Control",
    "text": "Why Need Version Control\n\n\nThink if you are not using version control, you very much can end up in a situation like this where you have a document you are working on, and you know we are humans feel like ok i’m almost done, I’m going to call this particular version I have final.doc.\nBut then you actually get some maybe revision comments from your advisor, and then that becomes final_rev2.doc. And you go back and forth lots of times. At the end, it’s everything like final, final3, final10, final_last whatever, it just keeps going.\nThe thing is these file names are not really very informative because final or final revisions don’t really say anything.\nBut if you really had tagged the changes you make between the final and the final 2, that could actually be informative if you actually have to go back to them."
  },
  {
    "objectID": "slides/04-git.html#how-we-use-git-and-github",
    "href": "slides/04-git.html#how-we-use-git-and-github",
    "title": "Git and GitHub \n",
    "section": "How We Use Git and GitHub",
    "text": "How We Use Git and GitHub\n\nOn GitHub, I create organization math3570-s24 and repos.\n\n\n\n\n\n\nSource: Data Science in a Box Unit 1 - Deck 3\n\n\n\n\n\n\nHow are we going to use Git/GitHub in this class?\nWe have a course organization (2024-spring-math-3570) I created on GitHub.\nWithin the organization, there will be a bunch of repositories I created for you.\nThink each repo as a project you will be working on. It is basically your homework assignment."
  },
  {
    "objectID": "slides/04-git.html#how-we-use-git-and-github-1",
    "href": "slides/04-git.html#how-we-use-git-and-github-1",
    "title": "Git and GitHub \n",
    "section": "How We Use Git and GitHub",
    "text": "How We Use Git and GitHub\n\nYou clone the repo I create for you to your Posit Cloud as an project.\n\n\n\n\n\n\nSource: Data Science in a Box Unit 1 - Deck 3\n\n\n\n\n\n\nAnd what you will do is you are going to go to your GitHub, find the repo that’s named after you, and then clone this as an rstudio project.\nWe talked about creating projects in our rstudio, but just lauching them there within Rstudio.\nThis time we are going to create a project from GitHub, and that’s called cloning"
  },
  {
    "objectID": "slides/04-git.html#how-we-use-git-and-github-2",
    "href": "slides/04-git.html#how-we-use-git-and-github-2",
    "title": "Git and GitHub \n",
    "section": "How We Use Git and GitHub",
    "text": "How We Use Git and GitHub\n\nYou push your updated work in Posit Cloud back to GitHub.\n\n\n\n\n\n\nSource: Data Science in a Box Unit 1 - Deck 3\n\n\n\n\n\n\nSo you are going to clone them in Posit Cloud, and then you are going to do your assignment, write R code, and finally version control your files with Git.\nSo along the way, as you make some changes, you are going to tag these changes as commits, then every once a while, you are going to push your changes back to the repo, which means that by the deadline of an assignment, your changes need to appear in your repo, because what I will be looking at for you is not what’s happening in Posit Cloud, but actually what’s happening on GitHub. The process of of uploading your new version of work into GitHub is called Push\n\nYou push your updated work in Rstudio back to GitHub, so that as your collaborator, I can see your updated work."
  },
  {
    "objectID": "slides/04-git.html#git-ready-for-data-science",
    "href": "slides/04-git.html#git-ready-for-data-science",
    "title": "Git and GitHub \n",
    "section": "Git Ready for Data Science",
    "text": "Git Ready for Data Science\n\n\nGo to https://github.com/ to create a GitHub account if you don’t have one.\n\nUse the same username and email address of your Posit Cloud.\n\n\n\nAccount (topright) > Settings\n\n\nEmails > Uncheck “Keep my email address private” (Link your R/Python work to GitHub)\n\nPublic Profile > Cool name and nice picture!\n\n\nShare your GitHub username at https://forms.office.com/r/k0L5M5TyS7, so I can invite you to join our course organization math3570-s24.\n\n\n\nVerify your GitHub email Adjust your GitHub settings for a more pleasant GitHub experience.\nWe’ll work with R, RStudio, Git, and GitHub together, like a real data scientist!"
  },
  {
    "objectID": "slides/04-git.html#git-ready-for-data-science-1",
    "href": "slides/04-git.html#git-ready-for-data-science-1",
    "title": "Git and GitHub \n",
    "section": "Git Ready for Data Science",
    "text": "Git Ready for Data Science\n\nPlease accept my invitation to join the GitHub organization math3570-s24.\n\n\n\nuse ghclass_code.R and add usernames in students vector.\nCheck Who is already in and make sure there are 29 of them."
  },
  {
    "objectID": "slides/04-git.html#git-and-github-tips",
    "href": "slides/04-git.html#git-and-github-tips",
    "title": "Git and GitHub \n",
    "section": "Git and GitHub Tips",
    "text": "Git and GitHub Tips\n\nPosit Cloud has installed Git for us! 😄\nThere are millions of git commands, and very few people know them all.\n99% of the time you will use Git to add, commit, push, and pull.\n\nok, that’s an exaggeration, but there are a lot of them\n\n\n\n\nIf you google for help and this is the solution…\nSkip that and move on to the next resource!\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will be doing Git things and interfacing with GitHub through RStudio. \n\nIf you google for help you might come across methods for doing these things in the command line.\nSkip that and move on to the next resource unless you feel comfortable trying it out."
  },
  {
    "objectID": "slides/04-git.html#connect-posit-cloud-and-github-step-1",
    "href": "slides/04-git.html#connect-posit-cloud-and-github-step-1",
    "title": "Git and GitHub \n",
    "section": "Connect Posit Cloud and GitHub: Step 1",
    "text": "Connect Posit Cloud and GitHub: Step 1\n\nPosit Cloud cannot recognize your GitHub account unless you connect them each other.\nIn Posit Cloud, click on your name on the top-right corner to open the right menu.\nClick on Authentication."
  },
  {
    "objectID": "slides/04-git.html#connect-posit-cloud-and-github-step-2",
    "href": "slides/04-git.html#connect-posit-cloud-and-github-step-2",
    "title": "Git and GitHub \n",
    "section": "Connect Posit Cloud and GitHub: Step 2",
    "text": "Connect Posit Cloud and GitHub: Step 2\n\nIn the Authentication window, check the box for Enabled.\n\n\n\nWhen check Enabled, will jump to GitHub page shown in the next."
  },
  {
    "objectID": "slides/04-git.html#connect-posit-cloud-and-github-step-3",
    "href": "slides/04-git.html#connect-posit-cloud-and-github-step-3",
    "title": "Git and GitHub \n",
    "section": "Connect Posit Cloud and GitHub: Step 3",
    "text": "Connect Posit Cloud and GitHub: Step 3\n\nFor your GitHub page, click on the green box that says “Authorize rstudio”."
  },
  {
    "objectID": "slides/04-git.html#connect-posit-cloud-and-github-step-4",
    "href": "slides/04-git.html#connect-posit-cloud-and-github-step-4",
    "title": "Git and GitHub \n",
    "section": "Connect Posit Cloud and GitHub: Step 4",
    "text": "Connect Posit Cloud and GitHub: Step 4\n\n\n\nBack to the Authentication of Posit Cloud, check Private repo access also enabled.\n\n\n\n\n\n\n\n\n\n\nMake sure math3570-s24 shows up under Organization access.\nClick on Request\nClick on the green box “Authorize rstudio”.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen check Private repo access also enabled, will jump to GitHub page as shown."
  },
  {
    "objectID": "slides/04-git.html#connect-posit-cloud-and-github-step-5",
    "href": "slides/04-git.html#connect-posit-cloud-and-github-step-5",
    "title": "Git and GitHub \n",
    "section": "Connect Posit Cloud and GitHub: Step 5",
    "text": "Connect Posit Cloud and GitHub: Step 5\n\nOnce you’re done, both of these boxes should be checked."
  },
  {
    "objectID": "slides/04-git.html#connect-posit-cloud-and-github-step-5-1",
    "href": "slides/04-git.html#connect-posit-cloud-and-github-step-5-1",
    "title": "Git and GitHub \n",
    "section": "Connect Posit Cloud and GitHub: Step 5-1",
    "text": "Connect Posit Cloud and GitHub: Step 5-1"
  },
  {
    "objectID": "slides/04-git.html#connect-posit-cloud-and-github-step-6",
    "href": "slides/04-git.html#connect-posit-cloud-and-github-step-6",
    "title": "Git and GitHub \n",
    "section": "Connect Posit Cloud and GitHub: Step 6",
    "text": "Connect Posit Cloud and GitHub: Step 6\n\nConfirm that you’ve linked up your GitHub and Posit Cloud accounts GitHub settings > Applications. Should see Posit Cloud listed as an authorized app under Authorized OAuth Apps.\n\n\n\nIf you see RStudio is under the Authorized Apps, congratulations! Your RStudio and GitHub are now linked together!"
  },
  {
    "objectID": "slides/04-git.html#github-to-posit-cloud-step-1",
    "href": "slides/04-git.html#github-to-posit-cloud-step-1",
    "title": "Git and GitHub \n",
    "section": "GitHub to Posit Cloud: Step 1",
    "text": "GitHub to Posit Cloud: Step 1\n\nEach of your assignments will begin with the following steps.\nGo to the repo named hw00-yourusername I created for you.\n\n\nhttps://mine-cetinkaya-rundel.github.io/teach-r-online/ - OK. Now let’s see how to clone a repo in GitHub to your RStudio.\n\nUse the code\n\n\n\n\n\nThis repo contains a template you can build on to complete your assignment."
  },
  {
    "objectID": "slides/04-git.html#github-to-posit-cloud-step-2",
    "href": "slides/04-git.html#github-to-posit-cloud-step-2",
    "title": "Git and GitHub \n",
    "section": "GitHub to Posit Cloud: Step 2",
    "text": "GitHub to Posit Cloud: Step 2\n\n\nOn GitHub,\n\nClick on the green Code button, select HTTPS.\nClick on the clipboard icon on the right to copy the repo URL, such as https://github.com/math3570-s24/hw00-chenghanyu.git"
  },
  {
    "objectID": "slides/04-git.html#github-to-posit-cloud-step-3",
    "href": "slides/04-git.html#github-to-posit-cloud-step-3",
    "title": "Git and GitHub \n",
    "section": "GitHub to Posit Cloud: Step 3",
    "text": "GitHub to Posit Cloud: Step 3\n\nGo to Posit Cloud and into the course workspace 2024-spring-math-3570.\nNew Project > New Project from Git Repo.\n\n\nYou will need to click on the down arrow next to the New Project button to see this option."
  },
  {
    "objectID": "slides/04-git.html#github-to-posit-cloud-step-4",
    "href": "slides/04-git.html#github-to-posit-cloud-step-4",
    "title": "Git and GitHub \n",
    "section": "GitHub to Posit Cloud: Step 4",
    "text": "GitHub to Posit Cloud: Step 4\n\nCopy and paste the URL of your assignment repo into the dialog box.\nHit OK, and you’re good to go!"
  },
  {
    "objectID": "slides/04-git.html#github-to-posit-cloud-step-5",
    "href": "slides/04-git.html#github-to-posit-cloud-step-5",
    "title": "Git and GitHub \n",
    "section": "GitHub to Posit Cloud: Step 5",
    "text": "GitHub to Posit Cloud: Step 5\n\nClick hw00-yourusername to do your assignment in Posit Cloud!\n\n\n\nDone! We learned the entire process of cloning a repo on GitHub to Posit Cloud as a project.\nNext, we’ll see how to keep your revision record (commit) and send (push) the latest revised version of your work from Posit Cloud to GitHub!"
  },
  {
    "objectID": "slides/04-git.html#personal-access-token-pat-step-1",
    "href": "slides/04-git.html#personal-access-token-pat-step-1",
    "title": "Git and GitHub \n",
    "section": "Personal Access Token (PAT): Step 1",
    "text": "Personal Access Token (PAT): Step 1\n\nGitHub has removed the support for Password Authentication for Git operations.\nBefore we can send our work in Posit Cloud to GitHub, we need Personal Access Token (PAT)\nSettings > Developer settings\n\n\n\nGitHub has removed the support for Password Authentication for Git operations.\nfor more safety.\nfrom 08/13/2021."
  },
  {
    "objectID": "slides/04-git.html#personal-access-token-pat-step-2",
    "href": "slides/04-git.html#personal-access-token-pat-step-2",
    "title": "Git and GitHub \n",
    "section": "Personal Access Token (PAT): Step 2",
    "text": "Personal Access Token (PAT): Step 2"
  },
  {
    "objectID": "slides/04-git.html#personal-access-token-pat-step-3",
    "href": "slides/04-git.html#personal-access-token-pat-step-3",
    "title": "Git and GitHub \n",
    "section": "Personal Access Token (PAT): Step 3",
    "text": "Personal Access Token (PAT): Step 3"
  },
  {
    "objectID": "slides/04-git.html#personal-access-token-pat-step-4",
    "href": "slides/04-git.html#personal-access-token-pat-step-4",
    "title": "Git and GitHub \n",
    "section": "Personal Access Token (PAT): Step 4",
    "text": "Personal Access Token (PAT): Step 4"
  },
  {
    "objectID": "slides/04-git.html#personal-access-token-pat-step-5",
    "href": "slides/04-git.html#personal-access-token-pat-step-5",
    "title": "Git and GitHub \n",
    "section": "Personal Access Token (PAT): Step 5",
    "text": "Personal Access Token (PAT): Step 5\n\nCopy and paste your PAT to a secrete and safe space!!"
  },
  {
    "objectID": "slides/04-git.html#posit-cloud-to-github-step-1---edit-your-file",
    "href": "slides/04-git.html#posit-cloud-to-github-step-1---edit-your-file",
    "title": "Git and GitHub \n",
    "section": "Posit Cloud to GitHub: Step 1 - Edit your file",
    "text": "Posit Cloud to GitHub: Step 1 - Edit your file\n\nOpen a Quarto (qmd) file in your project, in YAML change the author name to your name.\nClick Render to generate your beautiful document. (If you are asked to install any packages, please do!)"
  },
  {
    "objectID": "slides/04-git.html#posit-cloud-to-github-step-2---commit-changes",
    "href": "slides/04-git.html#posit-cloud-to-github-step-2---commit-changes",
    "title": "Git and GitHub \n",
    "section": "Posit Cloud to GitHub: Step 2 - Commit changes",
    "text": "Posit Cloud to GitHub: Step 2 - Commit changes\n\nGo to the Git tab in your RStudio.\nClick on Diff. This shows you the difference between the last committed state of the document and its current state that includes your changes.\nCheck Staged box to add files to be committed.\nWrite “Update author’s name” in the Commit message box and hit Commit."
  },
  {
    "objectID": "slides/04-git.html#posit-cloud-to-github-step-3---push-changes",
    "href": "slides/04-git.html#posit-cloud-to-github-step-3---push-changes",
    "title": "Git and GitHub \n",
    "section": "Posit Cloud to GitHub: Step 3 - Push changes",
    "text": "Posit Cloud to GitHub: Step 3 - Push changes\n\nWe’ve made an update and committed this change locally.\nIt’s time to push the changes to your repo on GitHub, so that others (Dr. Yu) can see your changes.\nClick on Push.\nIn the prompted dialogue box, enter your GitHub user name, and your password (PAT).\n\n\nhttps://inbo.github.io/git-course/course_rstudio.html"
  },
  {
    "objectID": "slides/04-git.html#posit-cloud-to-github-step-3---updated-repo",
    "href": "slides/04-git.html#posit-cloud-to-github-step-3---updated-repo",
    "title": "Git and GitHub \n",
    "section": "Posit Cloud to GitHub: Step 3 - Updated Repo",
    "text": "Posit Cloud to GitHub: Step 3 - Updated Repo\n\nBack to your GitHub repo and refresh it.\nThe online repo is now synced with your local project in Posit Cloud."
  },
  {
    "objectID": "slides/04-git.html#resources",
    "href": "slides/04-git.html#resources",
    "title": "Git and GitHub \n",
    "section": "Resources",
    "text": "Resources\n\n\n\nCreate a personal access token (PAT)\nTwo-factor authentication\nGit hands-on session within RStudio\nHappy Git and GitHub for the useR\nHappier version control with Git and GitHub\n\n\n\n\n\n\n\n\n\n\n\n\nExercise: Click on the more button in the git pane and select Shell Type git config –global credential.helper store Type exit to quit the shell \n\n\n\n\n\n\n\n\nmath3570-s24.github.io/website"
  },
  {
    "objectID": "slides/05-quarto.html#sharing-data-science-products-using-quarto",
    "href": "slides/05-quarto.html#sharing-data-science-products-using-quarto",
    "title": "Quarto 📖",
    "section": "Sharing Data Science Products using Quarto",
    "text": "Sharing Data Science Products using Quarto\n\n\nRemember that in DS, our ultimate goal is share our data science outputs and communicate with others.\nToday, we are going to learn Quarto that is an amazing tool for generating various different kinds of data science output."
  },
  {
    "objectID": "slides/05-quarto.html#what-and-why-full-reproducibility",
    "href": "slides/05-quarto.html#what-and-why-full-reproducibility",
    "title": "Quarto 📖",
    "section": "What and Why – Full Reproducibility",
    "text": "What and Why – Full Reproducibility\n\n\n\n\n[What] data science publishing system\nUse a single Quarto file (.qmd) to\n\nweave together narrative text and code\n\nproduce elegantly formatted outputs: word/pdf, webpages, blogs, books, etc.\n\n\n\n[Why] Fully reproducible reports\n\nHave code, results, and text in the same document\nResults are generated from the source code, and your documents are automatically updated if your data or code changed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWHAT\nRemember in the last stage of DS, we need to communicate our DS results with others.\nqmd is a very useful communication tool that integrates prose/words, code, and analysis results.\nWHY\nResults are generated from the source code, and your documents are automatically updated if your data changed.\nSo if your data or code changed, or you need to redo your analysis or rerun your algorithm, you don’t need to actually redo your analysis and copy all of your numbers, tables and figures generated from your code and paste them manually to your Word or Powerpoint file.\nSince your code, results, and text are all in one qmd file, if your data change, your results will be automatically updated with that change. No more copy and paste.\nThis greatly saves lots of time. Also, it reduces the chance of making mistakes of copying and pasting, and increases reproducibility of your study, meaning that your code can produce exactly the same result shown in your document.\nLiterate Programming"
  },
  {
    "objectID": "slides/05-quarto.html#section-1",
    "href": "slides/05-quarto.html#section-1",
    "title": "Quarto 📖",
    "section": "",
    "text": "I found an error in the data and added two more cases. Find the attached the modified excel. Can you redo the analysis? - Excel to R - Re-Make the graphs and re-run all the analysis - Copy and paste every updated output from R to your document This is an non-reproducible way fo doing data science.\nBut there is another way, a better way, a reproducible way."
  },
  {
    "objectID": "slides/05-quarto.html#why-quarto-various-types-of-output",
    "href": "slides/05-quarto.html#why-quarto-various-types-of-output",
    "title": "Quarto 📖",
    "section": "Why Quarto – Various Types of Output",
    "text": "Why Quarto – Various Types of Output\n\nSupport dozens of static and dynamic/interactive output formats!\n\n\n\nOne more reason to use Quarto is qmd can generate various types of output!\nIt can produce a PDF, blog, HTML. It can generate slides, website, dashboard, word document, e-books. You name it!\nSo once you learn this Rmarkdown, you can prepare any type of output you want for different purposes. You can prepare a paper for submitting it to a journal, prepare slides for your presentation, prepare a website/dashboard for data science results.\nIt’s really handy! In fact, the slides I make for this course are all qmd files."
  },
  {
    "objectID": "slides/05-quarto.html#moving-between-formats-straightforward",
    "href": "slides/05-quarto.html#moving-between-formats-straightforward",
    "title": "Quarto 📖",
    "section": "Moving Between Formats Straightforward",
    "text": "Moving Between Formats Straightforward\n\n\nHTML Document\n\n datascience.qmd\n\ntitle: \"Data Science\"\nformat: html\n\n\nPresentation Slides\n\n datascience.qmd\n\ntitle: \"Data Science\"\nformat: revealjs\n\n\n\nWebsite\n\n _quarto.yml\n\nproject:\n  type: website\n\nwebsite: \n  navbar: \n    left:\n      - datascience.qmd\n\n\n\n–> a lesson in document form  –> the same content in presentation form  –> the same content on a page in a website,\nand you can see that all that needed to change going between these formats is a few lines in the yaml. Nothing in the content part of my document. No slide breaks to remove, no citation style to change, no headings to re-level. This ease of transition has freed up time to focus my time on content, and that, folks, is the dream!"
  },
  {
    "objectID": "slides/05-quarto.html#why-quarto1-integrate-multiple-languages",
    "href": "slides/05-quarto.html#why-quarto1-integrate-multiple-languages",
    "title": "Quarto 📖",
    "section": "Why Quarto1 – Integrate Multiple Languages",
    "text": "Why Quarto1 – Integrate Multiple Languages\n\nQuarto is actually a command line interface that can render .qmd files into different output formats using commands in terminal."
  },
  {
    "objectID": "slides/05-quarto.html#quarto-is-built-on-pandoc",
    "href": "slides/05-quarto.html#quarto-is-built-on-pandoc",
    "title": "Quarto 📖",
    "section": "Quarto Is Built on Pandoc\n",
    "text": "Quarto Is Built on Pandoc\n\n\nR uses knitr and Python/Julia uses Jupyter to evaluate our code and turn qmd into md file.\n\n\n\nSo what does Quarto do so amazingly for us?\nWhen we hit Knit, your qmd first uses the knitr package to execute and translate our code, and then creates a new markdown document which includes the code and its output.\nThe markdown file is then processed by a document converter called pandoc which is responsible for creating various document format you like.\nThe entire process looks and sounds complicated, and indeed it’s complicated. But qmd makes it simple. All we need is to click on the Knit button, and qmd does everything for us."
  },
  {
    "objectID": "slides/05-quarto.html#why-quarto-comfort-of-your-own-workspace",
    "href": "slides/05-quarto.html#why-quarto-comfort-of-your-own-workspace",
    "title": "Quarto 📖",
    "section": "Why Quarto – Comfort of Your Own Workspace",
    "text": "Why Quarto – Comfort of Your Own Workspace"
  },
  {
    "objectID": "slides/05-quarto.html#why-quarto-simple-markdown-syntax-for-text",
    "href": "slides/05-quarto.html#why-quarto-simple-markdown-syntax-for-text",
    "title": "Quarto 📖",
    "section": "Why Quarto – Simple Markdown Syntax for Text",
    "text": "Why Quarto – Simple Markdown Syntax for Text\nTo generate a PDF report, you prefer writing this with 24 lines…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother reason to use Quarto is that its syntax is simple.\nWe can generate this PDF report including code and figures using this concise qmd file on the right."
  },
  {
    "objectID": "slides/05-quarto.html#why-quarto-simple-markdown-syntax-for-text-1",
    "href": "slides/05-quarto.html#why-quarto-simple-markdown-syntax-for-text-1",
    "title": "Quarto 📖",
    "section": "Why Quarto – Simple Markdown Syntax for Text",
    "text": "Why Quarto – Simple Markdown Syntax for Text\nOr this with 250 lines!?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also generate the same PDF report using this document processing format.\nWhich one you prefer?\nDoes anyone know what tool is used for generating the PDF? It’s called LaTex.\nYou can see the qmd file is cleaner and much more concise."
  },
  {
    "objectID": "slides/05-quarto.html#markdown",
    "href": "slides/05-quarto.html#markdown",
    "title": "Quarto 📖",
    "section": "Markdown",
    "text": "Markdown\n\n\n\nQuarto is based on markdown, a markup language that is widely used to generate HTML pages.\nMarkdown is a lightweight and easy-to-use syntax for styling the writing on the GitHub platform.\nWe go through basic (Pandoc’s) Markdown syntax together, and you can learn more at:\n\nMarkdown Tutorial\nMastering Markdown GitHub Guides\nMarkdown Guide\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn my opinion, comparing to HTML, Markdown format is more friendly and easier to learn."
  },
  {
    "objectID": "slides/05-quarto.html#quarto-file-plain-text-file-with-extension-.qmd",
    "href": "slides/05-quarto.html#quarto-file-plain-text-file-with-extension-.qmd",
    "title": "Quarto 📖",
    "section": "Quarto file = plain text file with extension .qmd\n",
    "text": "Quarto file = plain text file with extension .qmd\n\n---\ntitle: \"ggplot2 demo\"\ndate: \"1/25/2024\"\nformat: html\n---\n\n## Cars\nThere is a relationship between *miles per gallon* and *displacement*.\n\n```{r}\nmpg |> ggplot(aes(x = displ, y = hwy)) + \n  geom_point()\n```\n\n\n\n\n\nYAML Header (“YAML Ain’t Markup Language”)\n---\nkey: value\n---\n\n\nThe first part is the YAML header, which is surrounded by a pair of three dashes.\nYAML is a human readable language that is commonly used in configuration files of programs.\nHere, YAML header defines metadata of the document, for example, title, date, and output format.\n\n\n\n\n\n\n\nMarkdown Text\n\n\nThe second part is text.\nAnd as we’ve discussed, the syntax for text is Markdown. We’ll learn some basic syntax later.\n\n\n\n\n\n\n\nCode Chunk\n\n```{r}\n## code right here\n```\n\n\n\nThe third main part of a qmd document is the so-called code chunk. which is surrounded by a pair of three backticks.\nThe code written in code chunks will be run once you compile your qmd file, and any output generated by your code will be printed out in the final document. All right!\nAny questions"
  },
  {
    "objectID": "slides/05-quarto.html#section-2",
    "href": "slides/05-quarto.html#section-2",
    "title": "Quarto 📖",
    "section": "",
    "text": "02-Quarto File\n\nGo to your GitHub repo lab-yourusername. Clone it to your Posit Cloud as a project in 2024-Spring-Math-3570 workspace.\nOpen the file lab.qmd.\nChange author in YAML.\n\n\nClick on  or Ctrl/Cmd + Shift + K to produce a HTML document.\nHow can we show the current date every time we compile the file? [Hint:] Check your hw00. Compile your document and make sure the date shows up.\nHow do we produce a pdf? Describe it in ## Lab 2: Quarto\nOnce done, commit with message “02-Quarto File” and push your version to GitHub.\n\n\n\nClick on the inverted triangle right next to  to produce a PDF/Word document.\n\nr Sys.Date()\ndate: “2024-01-25”"
  },
  {
    "objectID": "slides/05-quarto.html#text-formatting",
    "href": "slides/05-quarto.html#text-formatting",
    "title": "Quarto 📖",
    "section": "Text Formatting",
    "text": "Text Formatting\n\n\n\n\n\n\nMarkdown Syntax\nOutput\n\n\n\n*italics* and **bold**\n\nitalics and bold\n\n\n\nsuperscript^2^ / subscript~2~\nsuperscript2 / subscript2\n\n\n\n~~strikethrough~~\nstrikethrough\n\n\n`verbatim code`\nverbatim code\n\n\n> here is the quote\n\nhere is the quote"
  },
  {
    "objectID": "slides/05-quarto.html#headings",
    "href": "slides/05-quarto.html#headings",
    "title": "Quarto 📖",
    "section": "Headings",
    "text": "Headings\n\n\n\n\n\n\nMarkdown Syntax\nOutput\n\n\n\n# Header 1\nHeader 1\n\n\n## Header 2\nHeader 2\n\n\n### Header 3\nHeader 3\n\n\n#### Header 4\nHeader 4\n\n\n##### Header 5\nHeader 5\n\n\n###### Header 6\nHeader 6"
  },
  {
    "objectID": "slides/05-quarto.html#lists",
    "href": "slides/05-quarto.html#lists",
    "title": "Quarto 📖",
    "section": "Lists",
    "text": "Lists\n\n\n\n\n\n\n\nMarkdown Syntax\nOutput\n\n\n\n* unordered list\n    + sub-item 1\n    + sub-item 2\n        - sub-sub-item 1\n\n\nunordered list\n\nsub-item 1\n\nsub-item 2\n\nsub-sub-item 1\n\n\n\n\n\n\n\n*   item 2\n    <new line>\n    Continued (indent 4 spaces)\n\n\nitem 2\nContinued (indent 4 spaces)\n\n\n\n\n1. ordered list\n2. item 2\n    i) sub-item 1\n         A.  sub-sub-item 1\n\nordered list\n\nitem 2\n\n\nsub-item 1\n\nsub-sub-item 1\n\n\n\n\n\n\n\n\n\nAt least two spaces"
  },
  {
    "objectID": "slides/05-quarto.html#math-and-links",
    "href": "slides/05-quarto.html#math-and-links",
    "title": "Quarto 📖",
    "section": "Math and Links",
    "text": "Math and Links\n\n\nMarkdown syntax\ninline-math: $A = r^{2}$\n\n\nmath-block: \n\n$$A = r^{2}$$\n\n<https://www.google.com > \n [Google link](https://www.google.com)\n\nOutput\ninline-math: \\(A = r^{2}\\) math-block: \\[A = r^{2}\\] https://www.google.comGoogle link"
  },
  {
    "objectID": "slides/05-quarto.html#tables",
    "href": "slides/05-quarto.html#tables",
    "title": "Quarto 📖",
    "section": "Tables",
    "text": "Tables\n\n\n| Right | Left | Default | Center |\n|------:|:-----|---------|:------:|\n|   12  |  12  |    12   |    12  |\n|  123  |  123 |   123   |   123  |\n|    1  |    1 |     1   |     1  |\n\n\n\nRight\nLeft\nDefault\nCenter\n\n\n\n12\n12\n12\n12\n\n\n123\n123\n123\n123\n\n\n1\n1\n1\n1"
  },
  {
    "objectID": "slides/05-quarto.html#source-vs.-visual-mode",
    "href": "slides/05-quarto.html#source-vs.-visual-mode",
    "title": "Quarto 📖",
    "section": "Source vs. Visual Mode",
    "text": "Source vs. Visual Mode\nSource Mode\n\n\n\n\n\n\n\n\nVisual Mode (What You See Is What You Mean (WYSIWYM))"
  },
  {
    "objectID": "slides/05-quarto.html#data-as-table",
    "href": "slides/05-quarto.html#data-as-table",
    "title": "Quarto 📖",
    "section": "Data as Table",
    "text": "Data as Table\nknitr::kable() can turn dataframes into tables.\n\nhead(mtcars) |> \n    knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.62\n16.5\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.88\n17.0\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.32\n18.6\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.21\n19.4\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.44\n17.0\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.46\n20.2\n1\n0\n3\n1"
  },
  {
    "objectID": "slides/05-quarto.html#section-3",
    "href": "slides/05-quarto.html#section-3",
    "title": "Quarto 📖",
    "section": "",
    "text": "03-Markdown \n\nBack to your lab.qmd. In ## Lab 3: Markdown section, add a self-introduction paragraph containing a header, bold and italic text.\n\nAdd another paragraph that contains\n\nlisted items\na hyperlink\na blockquote\nmath expression\n\n\nOnce done, commit with message “03-Markdown” and push your updated work to GitHub."
  },
  {
    "objectID": "slides/05-quarto.html#anatomy-of-a-code-chunk",
    "href": "slides/05-quarto.html#anatomy-of-a-code-chunk",
    "title": "Quarto 📖",
    "section": "Anatomy of a Code Chunk",
    "text": "Anatomy of a Code Chunk\n\n\n```{r}\n#| label: car-stuff\n#| echo: false\nmtcars |> \n  distinct(cyl)\n```\n\n```{python}\n#| label: string\n#| eval: false\nx = 'hello, python world!'\nprint(x.split(' '))\n```\n\n\nHas 3x backticks ``` on each end\nTo insert a code chunk,\n\n\n\n\n\n\n\n\n\n\nAlt + Ctrl + I (Win)\nOption + Cmd + I (Mac)\nIndicate engine (r) between curly braces {r}\nPlace options underneath, behind the #| (hashpipe): #| option1: value\nTools > Modify Keyboard Shortcuts > Filter… > Insert Chunk Python > Cmd + P"
  },
  {
    "objectID": "slides/05-quarto.html#option-echo",
    "href": "slides/05-quarto.html#option-echo",
    "title": "Quarto 📖",
    "section": "Option echo\n",
    "text": "Option echo\n\n\nIf you simply want code highlighting, you can use 3x backticks + the language ```r\n\n\n\n```r\nhead(mtcars)\n```\n\n\nWhich returns the below but is not executed since there aren’t {} around the language:\nhead(mtcars)\n\n\n\n\nIf you instead want to see source code and evaluate it, you could use echo: true where echo: false would instead hide the code but still evaluate it.\n\n\n\n\n```{r}\n#| echo: true\n1 + 1\n```\n\n\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "slides/05-quarto.html#chunk-options",
    "href": "slides/05-quarto.html#chunk-options",
    "title": "Quarto 📖",
    "section": "Chunk Options",
    "text": "Chunk Options\n\nThe following table summarises which types of output each option suppresses:\n\n\n\n\n\n\n\n\n\n\n\n\nOption\nRun code\nShow code\nOutput\nPlots\nMessages\nWarnings\n\n\n\neval: false\nx\n\nx\nx\nx\nx\n\n\ninclude: false\n\nx\nx\nx\nx\nx\n\n\necho: false\n\nx\n\n\n\n\n\n\nresults: \"hide\"\n\n\nx\n\n\n\n\n\nfig-show: \"hide\"\n\n\n\nx\n\n\n\n\nmessage: false\n\n\n\n\nx\n\n\n\nwarning: false\n\n\n\n\n\nx\n\n\n\n\nCheck knitr for more chunk options.\n\n\nHere shows some code chunk options.\nThe table summarises which types of output each option suppresses.\nSo eval = FALSE means that the code is not run, but the code is still shown in the compiled document. Because the code is not run, there will be no output, no plots, no messages and no warnings.\nWhen include = FALSE, the code will be run, but nothing will be included in the final document.\nWhen echo = FALSE, the code will not be shown in the document, but it will be run, and all results will be shown in the document.\nYou get the idea, right.\nSo results controls output, fig.show controls plots, message and warning control if messages and warnings are shown.\nThere are so many chunk options out there. If you want to learn more, definitely check this knitr website. OK"
  },
  {
    "objectID": "slides/05-quarto.html#global-options-execute",
    "href": "slides/05-quarto.html#global-options-execute",
    "title": "Quarto 📖",
    "section": "Global Options: execute",
    "text": "Global Options: execute\n\nShould be specified within the execute key.\n\n\n---\nexecute: \n  echo: false\n  eval: false\n---\n\nCheck HTML Options for more options!\n\n```{r}\n#| label: setup\n#| include: false\n# don't show code unless we explicitly set echo = TRUE\nknitr::opts_chunk$set(echo = FALSE)\n```\n\n\nknitr::opts_chunk$set() sets default options for all chunks."
  },
  {
    "objectID": "slides/05-quarto.html#images",
    "href": "slides/05-quarto.html#images",
    "title": "Quarto 📖",
    "section": "Images",
    "text": "Images\nBasic markdown syntax:\n![Maru](images/05-quarto/maru1.jpg)\n\nMaru"
  },
  {
    "objectID": "slides/05-quarto.html#figures-w-code",
    "href": "slides/05-quarto.html#figures-w-code",
    "title": "Quarto 📖",
    "section": "Figures w/ code",
    "text": "Figures w/ code\n\n```{r}\n#| out-width: 40%\n#| fig-align: right\n\nknitr::include_graphics(\"images/05-quarto/maru1.jpg\")\n```\n\n\nWidth of the plot in the output document, which can be different from its physical fig-width, i.e., plots can be scaled in the output document. When used without a unit, the unit is assumed to be pixels. However, any of the following unit identifiers can be used: px, cm, mm, in, inch and %, for example, 3in, 8cm, 300px or 50%."
  },
  {
    "objectID": "slides/05-quarto.html#including-plots",
    "href": "slides/05-quarto.html#including-plots",
    "title": "Quarto 📖",
    "section": "Including Plots",
    "text": "Including Plots\n\nMany chunk options for figures and images start with fig-, for example fig-width, fig-height, fig-show, etc.\n\n\n\n\n\n```{r}\n#| eval: false\n#| fig-cap: \"Fig. 1: Car stuff\"\nplot(x = cars$speed, y = cars$dist)\n```\n\n\n\n\n\n\nFig. 1: Car stuff"
  },
  {
    "objectID": "slides/05-quarto.html#divs-and-spans",
    "href": "slides/05-quarto.html#divs-and-spans",
    "title": "Quarto 📖",
    "section": "Divs and Spans",
    "text": "Divs and Spans\nThis is text with [special]{style=\"color:red\"} formatting.\nThis is text with special formatting.\n\n\n::: {style=\"color:red\"}\nThis content can be styled with a border\n:::\n\nThis content can be styled with a border\n\n\n\nDivs and Spans\n\nThink of a ::: div as a HTML <div> but it can also apply in specific situations to content in PDF\n\n[text]{.class} spans can be thought of a <span .class>Text</span>\n::: {.border}\nThis content can be styled with a border\n:::\n\n<div class=\"border\">\n  <p>This content can be styled with a border</p>\n</div>\n\n\n<!-- ::: {style=\"border-left:10px solid red\"} -->\n<!-- This content can be styled with a border -->\n<!-- ::: -->\nDivs start with a fence containing at least three consecutive colons plus some attributes. The attributes may optionally be followed by another string of consecutive colons. The Div ends with another line containing a string of at least three consecutive colons. The Div should be separated by blank lines from preceding and following blocks."
  },
  {
    "objectID": "slides/05-quarto.html#subfigures-fenced-div-class",
    "href": "slides/05-quarto.html#subfigures-fenced-div-class",
    "title": "Quarto 📖",
    "section": "Subfigures Fenced div Class",
    "text": "Subfigures Fenced div Class\n::: {#fig-maru layout-ncol=2}\n\n![Loaf](images/05-quarto/maru2.jpg){#fig-loaf width=\"250px\"}\n![Lick](images/05-quarto/maru3.jpg){#fig-lick width=\"250px\"}\nTwo states of Maru\n\n:::\n\n\n\n\n\n(a) Loaf\n\n\n\n\n\n\n(b) Lick\n\n\n\n\nFigure 1: Two states of Maru"
  },
  {
    "objectID": "slides/05-quarto.html#inline-code",
    "href": "slides/05-quarto.html#inline-code",
    "title": "Quarto 📖",
    "section": "Inline Code",
    "text": "Inline Code\n\nInside your text you can include code with the syntax `r your-r-code`.\nFor example, `r 4 + 5` would output 9 in your text.\n\n\nhead(cars)\n\n  speed dist\n1     4    2\n2     4   10\n3     7    4\n4     7   22\n5     8   16\n6     9   10\n\nnum_cars <- nrow(cars)\n\n\n\n\nCode in Quarto\nThere are `r num_cars` rows in the cars dataset. Four plus five is `r 4 + 5`\n\nOutput\nThere are 50 rows in the cars dataset. Four plus five is 9\n\n\n\nWe;ve learned that Inside your text you can include code, and inline code is created with backticks.\nIn fact, the inline code can actually execute code, if we put r in the first place.\nFor example, 9 would output 9 in your text."
  },
  {
    "objectID": "slides/05-quarto.html#section-4",
    "href": "slides/05-quarto.html#section-4",
    "title": "Quarto 📖",
    "section": "",
    "text": "04-Code Chunk \n\n\nIn lab.qmd ## Lab 4: Code Chunk, use code chunks to\n\ninclude an image with knitr::include_graphics(\"URL or file path\") https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/ggplot2.png\ninclude a plot plot(mtcars$disp, mtcars$mpg)\nShow dataset mtcars as a table using knitr::kable()\ndo some inline code calculation like `r ncol(mtcars)`, `r log(100, base = 10) + sqrt(4)`.\n\n\nAdd option fig-height: 4, fig-width: 6 and fig-align: right to your plot. What are the changes?\nHow do we set global chunk options to hide and run code in every chunk?\nOnce done, commit with message “04-Code Chunk” and push your work to GitHub.\n\n\n\nRun the following in the console\n\n\n\n\nThen, add the following chunk in your lab02-rmarkdown.qmd and re-knit it.\n\n\n\nWhat happens? - The environment of your Quarto document is separate from the Console! Remember this, and expect it to bite you a few times as you’re learning to work with Quarto! - fig.dim = c(8, 6) means fig.width = 8 and fig.height = 6 in inches - out.width=“50%”: half of the width of the image container"
  },
  {
    "objectID": "slides/05-quarto.html#quarto-skills-to-the-next-level",
    "href": "slides/05-quarto.html#quarto-skills-to-the-next-level",
    "title": "Quarto 📖",
    "section": "Quarto Skills to the Next Level",
    "text": "Quarto Skills to the Next Level\n\nQuarto Website\nGet Started with Quarto\nR for Data Science Ch-28 Quarto\n\n\n\n\nmath3570-s24.github.io/website"
  },
  {
    "objectID": "slides/19-pca.html#unsupervised-learning-1",
    "href": "slides/19-pca.html#unsupervised-learning-1",
    "title": "Principal Component Analysis \n",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\nSupervised Learning: response \\(Y\\) and features \\(X_1, X_2, \\dots, X_p\\) measured on \\(n\\) observations.\n\nUnsupervised Learning: only features \\(X_1, X_2, \\dots, X_p\\) measured on \\(n\\) observations.\n\nNot interested in prediction (no response to be predicted)\nDiscover any interesting pattern or relationships among these features.\n\n\n\n\n\n\nDimension reduction for effective data visualization or extracting most important information those features contain.\n\nplot a bunch of points of \\(\\boldsymbol{x} = (x_1, x_2, \\dots, x_p)\\) in a 2-D scatter plot (manifold). (reduce dimension from \\(p\\) to 2)\n use 2 variables to explain most variations or represents high data density in the \\(p\\) variables \n\n\n\n\n\n\n\nClustering discovers unknown subgroups/clusters in data\n\nfind 3 sub-groups of people based on variables income, occupation, age, etc"
  },
  {
    "objectID": "slides/19-pca.html#one-dimension-1d-number-line",
    "href": "slides/19-pca.html#one-dimension-1d-number-line",
    "title": "Principal Component Analysis \n",
    "section": "One-Dimension (1D) Number line",
    "text": "One-Dimension (1D) Number line\n\n\n\n\n# A tibble: 50 × 1\n   English\n     <int>\n 1      41\n 2      65\n 3      55\n 4      94\n 5      66\n 6      85\n 7      44\n 8      44\n 9      67\n10      73\n# ℹ 40 more rows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s go back to your first grade. You remember that one-dimension equals a number line.\nNow suppose we have 50 English grades. The first 3 scores are 41, 65, and 55.\nWe can plot these values on the number line just like we did in the elementary school.\n1st student with score 41 it’s a dot at 41, 2nd student with score 65, it’s a dot at 65, and so on."
  },
  {
    "objectID": "slides/19-pca.html#one-dimension-1d-number-line-uniform-students",
    "href": "slides/19-pca.html#one-dimension-1d-number-line-uniform-students",
    "title": "Principal Component Analysis \n",
    "section": "One-Dimension (1D) Number line: Uniform students",
    "text": "One-Dimension (1D) Number line: Uniform students\n\n\n\n\n# A tibble: 50 × 1\n   English\n     <int>\n 1      41\n 2      65\n 3      55\n 4      94\n 5      66\n 6      85\n 7      44\n 8      44\n 9      67\n10      73\n# ℹ 40 more rows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you plot all students scores, we might see something like this: an uniform distribution of English scores."
  },
  {
    "objectID": "slides/19-pca.html#d-number-line-non-uniform-students",
    "href": "slides/19-pca.html#d-number-line-non-uniform-students",
    "title": "Principal Component Analysis \n",
    "section": "1D Number line: Non-uniform students",
    "text": "1D Number line: Non-uniform students\n\n\n\n\n# A tibble: 50 × 1\n   English\n     <int>\n 1      77\n 2      78\n 3      81\n 4      78\n 5      52\n 6      62\n 7      47\n 8      58\n 9      43\n10      59\n# ℹ 40 more rows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOr we might get a non-uniform distribution of English scores. Some students obtain a very high grade, but some students are falling behind and get a very low grade."
  },
  {
    "objectID": "slides/19-pca.html#two-dimensions-2d-x-y-scatter-plot-high-correlated",
    "href": "slides/19-pca.html#two-dimensions-2d-x-y-scatter-plot-high-correlated",
    "title": "Principal Component Analysis \n",
    "section": "Two-Dimensions (2D) X-Y Scatter plot: High Correlated",
    "text": "Two-Dimensions (2D) X-Y Scatter plot: High Correlated\nEnglish and Math measure an overall academic performance.\n\n\n\n\n# A tibble: 50 × 2\n   English  Math\n     <int> <dbl>\n 1      41  33.2\n 2      65  63.6\n 3      55  44.6\n 4      94  95  \n 5      66  65.6\n 6      85  73.1\n 7      44  46.6\n 8      44  51  \n 9      67  69.4\n10      73  66.5\n# ℹ 40 more rows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow let’s go to sixth grade when we learned about 2 dimensional graphs.\nNow we have two axes instead of one, and now we can plot data from two different subjects instead of just one.\nAdditional to English scores, here we also have Math scores of some students.\nAnd we all know how to map a English and math score pair to a point on a 2D plot.\nIf we put all students English and Math scores on the plot, we might see something like this that English and Math scores are positively correlated, meaning that a student having high English score tends to have high Math score too.\nThis might be due to the fact that English and Math measure an overall academic performance. A good student tends to have both high English and Math scores."
  },
  {
    "objectID": "slides/19-pca.html#two-dimensions-2d-x-y-scatter-plot-no-correlated",
    "href": "slides/19-pca.html#two-dimensions-2d-x-y-scatter-plot-no-correlated",
    "title": "Principal Component Analysis \n",
    "section": "Two-Dimensions (2D) X-Y Scatter plot: No correlated",
    "text": "Two-Dimensions (2D) X-Y Scatter plot: No correlated\nEnglish and Math measure different abilities.\n\n\n\n\n# A tibble: 50 × 2\n   English  Math\n     <int> <int>\n 1      41    27\n 2      65    47\n 3      55    32\n 4      94    44\n 5      66    20\n 6      85    30\n 7      44    16\n 8      44    72\n 9      67    86\n10      73    82\n# ℹ 40 more rows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOr we might see English and Math scores are not correlated, meaning that a high English score does not tell us anything about Math score, whether it is high or low.\nBecause English and Math might measure different abilities.\nEnglish measures verbal and communication skills and Math measures logic and quantitative skills"
  },
  {
    "objectID": "slides/19-pca.html#three-dimensions-3d-x-y-z-scatter-plot",
    "href": "slides/19-pca.html#three-dimensions-3d-x-y-z-scatter-plot",
    "title": "Principal Component Analysis \n",
    "section": "Three-Dimensions (3D) X-Y-Z Scatter plot",
    "text": "Three-Dimensions (3D) X-Y-Z Scatter plot\n\n\n\n\n# A tibble: 50 × 3\n   English  Math Biology\n     <int> <dbl>   <dbl>\n 1      41  33.2    39.2\n 2      65  63.6    61.6\n 3      55  44.6    41.6\n 4      94  95      92  \n 5      66  65.6    73.6\n 6      85  73.1    71.1\n 7      44  46.6    56.6\n 8      44  51      56  \n 9      67  69.4    79.4\n10      73  66.5    59.5\n# ℹ 40 more rows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOK. Now maybe in college, we started drawing 3D graph, having height, width and depth.\nWith 3 separate axes, we can now plot data from three different subjects.\nSo now we have three subjects, English, Math and Biology.\nWith the same logic as 2D plot, we put English score on X-axis, Math score on Y-axis, and biology score on Z-axis, then draw lines perpendicular to each axis to put the point in the 3D space.\nAnd this is our 3D scatter plot.\nSo, if we have one subject data, we can make a 1D graph.\nIf we have two subject data, we can make a 2D graph.\nIf we have three subject data, we can make a 3D graph."
  },
  {
    "objectID": "slides/19-pca.html#four-dimensions-4d-x-y-z--scatter-plot",
    "href": "slides/19-pca.html#four-dimensions-4d-x-y-z--scatter-plot",
    "title": "Principal Component Analysis \n",
    "section": "Four-Dimensions (4D) X-Y-Z-? Scatter plot",
    "text": "Four-Dimensions (4D) X-Y-Z-? Scatter plot\n\n\n\n\n# A tibble: 50 × 4\n   English  Math Biology History\n     <int> <dbl>   <dbl>   <dbl>\n 1      41  33.2    39.2      51\n 2      65  63.6    61.6      53\n 3      55  44.6    41.6      63\n 4      94  95      92        83\n 5      66  65.6    73.6      51\n 6      85  73.1    71.1      74\n 7      44  46.6    56.6      34\n 8      44  51      56        33\n 9      67  69.4    79.4      76\n10      73  66.5    59.5      74\n# ℹ 40 more rows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBut what happens if we have data from 4 academic subjects?? here English, Math, Biology and History.\nWell we need 4D graph. The problem is You can’t draw a 4D plot on paper!\nWhat if a student has 20 different grades from 20 different courses?? We need a 20 dimensional graph?\nBut what is that?? There is no way we can draw that."
  },
  {
    "objectID": "slides/19-pca.html#how-about-pair-plots",
    "href": "slides/19-pca.html#how-about-pair-plots",
    "title": "Principal Component Analysis \n",
    "section": "How about Pair Plots?",
    "text": "How about Pair Plots?\n\n\nPlotting all 2-D scatter plots of all possible pairs might be a solution to check the relationship between variables, or exploratory data analysis.\nLike here we see basically any two variables or two subject scores are positively correlated."
  },
  {
    "objectID": "slides/19-pca.html#tooooo-many-pair-plots",
    "href": "slides/19-pca.html#tooooo-many-pair-plots",
    "title": "Principal Component Analysis \n",
    "section": "Tooooo Many Pair Plots!",
    "text": "Tooooo Many Pair Plots!\n\nIf we have \\(p\\) variables, there are \\({p \\choose 2} = p(p-1)/2\\) pairs.\nIf \\(p = 10\\), we have 45 such scatter plots to look at!\nIn real data science work, we may encounter over 100 variables!!\n\n\nBut still, there is a problem.\nIf you have \\(p\\) variables, you are gonna have \\({p \\choose 2} = p(p-1)/2\\) pairs.\nIf \\(p = 10\\), you have 45 such scatter plots to look at!\nNot to mention that in real data science work, you may encounter over 100 variables!!"
  },
  {
    "objectID": "slides/19-pca.html#dimension-reduction",
    "href": "slides/19-pca.html#dimension-reduction",
    "title": "Principal Component Analysis \n",
    "section": "Dimension Reduction",
    "text": "Dimension Reduction\n\nOne variable represents one dimension.\nWith many variables in the data, we live in a high dimensional world.\n\n\nGOAL:\n\nFind a low-dimensional (usually 2D) representation of the data that captures as much of the information all of those variables provide as possible.\nUse two created variables to represent all \\(p\\) variables, and make a scatter plot of the two created variables to learn what our observations look like as if they lived in the high dimensional space. \n\n\n\n\nWhy and when can we omit dimensions?\n\n\nSo in order to meaningfully represent the relationship of all variables, we need a technique, Dimension Reduction.\nIn mathematics, One variable represents one dimension, so with many variables in the data, we live in a high dimensional world.\nWe would like to find a low-dimensional (usually 2D) representation of the data that captures as much of the information all of those variables provide as possible.\nWe use two created variables to represent all \\(p\\) variables, and make a scatter plot of the two created variables to learn what our observations look like as if they lived in the high dimensional space.\nOf course, it’s not always a good idea to just use two variables to represent all \\(p\\) variables. Some information will be missing when we just use two variables to represent or explain all the relationships of \\(p\\) variables.\nBut sometimes, a low-dimensional representation looks very like a high dimensional space, and does not lose much information. In this situation, a low-dimensional representation is very useful.\nSo Let’s see why and when can we omit dimensions?"
  },
  {
    "objectID": "slides/19-pca.html#variation-mostly-from-one-variable",
    "href": "slides/19-pca.html#variation-mostly-from-one-variable",
    "title": "Principal Component Analysis \n",
    "section": "Variation mostly from One Variable",
    "text": "Variation mostly from One Variable\n\nAlmost all of the variation in the data is from left to right.\n\n\n\nLet’s suppose we have two variables, and their data look like this.\nHere we see almost all of the variation in the data from left to right.\nIt means that for variable X, some observations have low values, and some observations have high values.\nBut it looks like all observations have variable Y at about the same level.\nSince X varies more than Y, having larger variation, X contains more information than Y in the data."
  },
  {
    "objectID": "slides/19-pca.html#variation-mostly-from-one-variable-1",
    "href": "slides/19-pca.html#variation-mostly-from-one-variable-1",
    "title": "Principal Component Analysis \n",
    "section": "Variation mostly from One Variable",
    "text": "Variation mostly from One Variable\n\nIf we flattened the data, the graph would not look much different.\n\n\n\nIf we flattened the data, removing the up and down variation, our graph would not look much different from what it look like before.\nMost of the variation, or information is still kept in the data."
  },
  {
    "objectID": "slides/19-pca.html#variation-mostly-from-one-variable-2",
    "href": "slides/19-pca.html#variation-mostly-from-one-variable-2",
    "title": "Principal Component Analysis \n",
    "section": "Variation mostly from One Variable",
    "text": "Variation mostly from One Variable\n\nIf we flattened the data, we could graph it with a 1D number line!\n\n\n\nNow, with up-and-down variation removed, the variable Y becomes redundant, and we can just represent the flattened data using 1D single number line."
  },
  {
    "objectID": "slides/19-pca.html#variation-mostly-from-one-variable-3",
    "href": "slides/19-pca.html#variation-mostly-from-one-variable-3",
    "title": "Principal Component Analysis \n",
    "section": "Variation mostly from One Variable",
    "text": "Variation mostly from One Variable\n\nBoth graphs say “the important variation is left to right.”\n\n\n\n\nSo in this case, we can display 2D data on a 1D plot without losing too much information!\nBoth graphs say “the important variation is left to right.”\n\nAnother example is watching TV. TV is a 2D thing, but we watch TV for 3D shows. The 2D TV represents the 3D shows, programs or drama very well because the 2D representation does not lose much information the 3D shows provide.\nSo through the example, we know some dimensions are more important than others. Like here Variable X is more important than variable Y because variable X explains most of the variation stored in the data."
  },
  {
    "objectID": "slides/19-pca.html#idea-of-pca",
    "href": "slides/19-pca.html#idea-of-pca",
    "title": "Principal Component Analysis \n",
    "section": "Idea of PCA",
    "text": "Idea of PCA\n\nPCA is a dimension reduction tool that finds a low-dimensional representation of a data set that contains as much as possible of variation.\nEach of the observations lives in a high-dimensional space (lots of variables), but not all of these dimensions (variables) are equally interesting/important.\nThe concept of interesting/important is measured by the amount that the data vary along each dimension.\n\n\nPCA is a dimension reduction tool that finds a low-dimensional representation of a data set that contains as much as possible of variation stored in the data set.\nAs we’ve seen before, each of the observations lives in a high-dimensional space, meaning that each observation has lots of variables associated with it, but not all of these dimensions (variables) are equally interesting/important.\nThe concept of interesting/important is measured by the amount that the observations vary along each dimension.\nA characteristic or attribute of observations is called variable because its value varies from sample to sample.\nIf the variable does not vary, it becomes an irrelevant or un-important variable because we cannot use the variable to differentiate or distinguish observations. If everyone in this class gets grade A, then the data science grade is not an important variable to learn which students perform academically better than others."
  },
  {
    "objectID": "slides/19-pca.html#pca-illustration-2-variable-example",
    "href": "slides/19-pca.html#pca-illustration-2-variable-example",
    "title": "Principal Component Analysis \n",
    "section": "PCA Illustration: 2 Variable Example",
    "text": "PCA Illustration: 2 Variable Example\n\n\n\n\n# A tibble: 50 × 2\n   English  Math\n     <int> <dbl>\n 1      41  33.2\n 2      65  63.6\n 3      55  44.6\n 4      94  95  \n 5      66  65.6\n 6      85  73.1\n 7      44  46.6\n 8      44  51  \n 9      67  69.4\n10      73  66.5\n11      47  58.9\n12      66  66.5\n13      57  51  \n14      57  33.2\n15      77  83.6\n16      83  78.8\n# ℹ 34 more rows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe use the English and Math score example to illustrate how PCA works.\nFrom the data, we have a 2D scatter plot like this.\nWith the data, we can have the average of English score and the average of math score, shown as the red squared point in the scatter plot."
  },
  {
    "objectID": "slides/19-pca.html#step-1-shift-or-standardize-the-data",
    "href": "slides/19-pca.html#step-1-shift-or-standardize-the-data",
    "title": "Principal Component Analysis \n",
    "section": "Step 1: Shift (or standardize) the Data",
    "text": "Step 1: Shift (or standardize) the Data\n\nSo the two variables have both mean 0. If the variables are measured in a different unit, consider standardization, \\(\\frac{x_i - \\bar{x}}{s_x}\\).\nShifting does not change how the data points are positioned relative to each other.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPCA first shift or standardize our data so that the center is on top of the origin (0, 0) in the graph.\nIf the two variables are measured in a different scale, standardization is highly recommended. Here because both English and Math scores are measured using the same scale. We only do the shifting, and no scaling.\nAfter standardization, all variables will use the same scale to measure their variation, and that makes more sense because PCA wants to find some axis or coordinate lower dimensional representation that contains the most variation, and we don’t want the variation depends on variable’s measurement units.\nNotice that Shifting does not change how the data points are positioned relative to each other. The scattering shape remains the same."
  },
  {
    "objectID": "slides/19-pca.html#step-2-find-a-line-that-fits-the-data-the-best",
    "href": "slides/19-pca.html#step-2-find-a-line-that-fits-the-data-the-best",
    "title": "Principal Component Analysis \n",
    "section": "Step 2: Find a Line that Fits the Data the Best",
    "text": "Step 2: Find a Line that Fits the Data the Best\n\nStart with a line going through the origin.\n\nRotate the line until it fits the data as well as it can, given that it goes through the origin.\n\n\n\nThe second step of PCA is going to fit a line to the centered or normalized data set. \n\nTo do this, we start by drawing a random line that goes through the origin."
  },
  {
    "objectID": "slides/19-pca.html#step-2-find-a-line-that-fits-the-data-the-best-1",
    "href": "slides/19-pca.html#step-2-find-a-line-that-fits-the-data-the-best-1",
    "title": "Principal Component Analysis \n",
    "section": "Step 2: Find a Line that Fits the Data the Best",
    "text": "Step 2: Find a Line that Fits the Data the Best\n\nStart with a line going through the origin.\n\nRotate the line until it fits the data as well as it can, given that it goes through the origin.\n\n\n\nThen we rotate the line until if fits the data as well as it can, given that it has to go through the origin."
  },
  {
    "objectID": "slides/19-pca.html#step-2-find-a-line-that-fits-the-data-the-best-2",
    "href": "slides/19-pca.html#step-2-find-a-line-that-fits-the-data-the-best-2",
    "title": "Principal Component Analysis \n",
    "section": "Step 2: Find a Line that Fits the Data the Best",
    "text": "Step 2: Find a Line that Fits the Data the Best\n\nStart with a line going through the origin.\n\nRotate the line until it fits the data as well as it can, given that it goes through the origin.\n\n\n\nUltimately, this line fits best. \n\nSo let’s see what makes this line the best fit, and the meaning of this line."
  },
  {
    "objectID": "slides/19-pca.html#the-meaning-of-the-best-line",
    "href": "slides/19-pca.html#the-meaning-of-the-best-line",
    "title": "Principal Component Analysis \n",
    "section": "The Meaning of the Best line",
    "text": "The Meaning of the Best line\n\n\n\nThe best line maximizes the variance of the projected points from the data points onto the line! It is called the 1st Principal Component (PC1)\n\nPC1 is the line in the Eng-Math space that is closest to the \\(n\\) observations\n\nPC1 minimizes the sum of squared distances between the data points and the PC1.\n\n\nPC1 is the best 1D representation of the 2D data\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo quantify how good this line fits the data, PCA projects the data onto it.\nThen the idea is that we can either measure the distances from the data to the line and try to find the line that minimizes those distances.\nOr we can try to find the line that maximizes the distances from the projected points to the origin.\nThe two criteria are equivalent.\nDEMO\nSo we are find the line that maximizes the variation of the projected points from the data points onto the line!\nThe best line is called Principal Component 1 (PC1), which maximizes the sum of squared distances between projected points and the origin.\n\nRegression line: minimizes the sum of squared residuals (vertical lines from the data points to the line)\n\nPrincipal Component 1 (PC1): maximizes the sum of squared distances between between projected points and the origin.\nPC1 is the line in the Eng-Math 2 dimensional space that is closest to the \\(n\\) observations, i.e., PC1 minimizes the sum of squared distances between the data points and the PC1.\nPC1 is the best 1D representation of the 2D data"
  },
  {
    "objectID": "slides/19-pca.html#the-meaning-of-the-best-line-1",
    "href": "slides/19-pca.html#the-meaning-of-the-best-line-1",
    "title": "Principal Component Analysis \n",
    "section": "The Meaning of the Best line",
    "text": "The Meaning of the Best line\n\n\n\n\n\nhttps://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues\n\n\n\n\n\n\nI am going to conclude the idea of PCA using this gif.\nThis gif shows you the idea of finding principal components.\nAs the line rotates, you can see the locations of the projected points on the line keeps changing as well.\nAnd the PC1 is the line that maximizes the variation of the projected points.\nAlso, the PC1 is the line that minimizes the distance between the data points and the line. In the figure, the sum of those red lines will be the smallest.\nQuestions?"
  },
  {
    "objectID": "slides/19-pca.html#pc1-and-pc2",
    "href": "slides/19-pca.html#pc1-and-pc2",
    "title": "Principal Component Analysis \n",
    "section": "PC1 and PC2",
    "text": "PC1 and PC2\n\n\n\nThe data points are also spread out a little above and below the PC1.\nThere are some variation that is not explained by the PC1.\nFind the second PC, PC2, that\n\nexplains the remaining variation\nis the line through the origin and perpendicular to PC1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlong with the PC1, the data points are also spread out a little above and below the PC1.\nThere are some variation of the two variables that is not explained by the PC1.\nTo find another PC that explains the remaining variation, we find the second PC, called PC2 that is the line through the origin and perpendicular to PC1."
  },
  {
    "objectID": "slides/19-pca.html#linear-combinations",
    "href": "slides/19-pca.html#linear-combinations",
    "title": "Principal Component Analysis \n",
    "section": "Linear Combinations",
    "text": "Linear Combinations\n\n\n\n\n\n\n\n\n\n\n\n\nPC1 = 0.68 \\(\\times\\) English \\(+\\) 0.74 \\(\\times\\) Math\nPC2 = 0.74 \\(\\times\\) English \\(-\\) 0.68 \\(\\times\\) Math\nPC1 is like an overall intelligence index as it is a weighted average combining verbal and quantitative abilities.\nPC2 accounts for individual difference in English and Math scores. \n\nThe combination weights 0.68, 0.74, etc are called PC loadings.\n\n\n\n\nNow let’s look at PC1 and PC2 a little more carefully.\nFirst PC1 and PC2 are just a vector in 2 dimensional space, right?\nIn other words, they are linear combinations of two standard basis, here our English axis and Math axis.\nPCA help us get the linear combinations.\nHere PC1 = 0.68 \\(\\times\\) English + 0.74 \\(\\times\\) Math\nPC2 = 0.74 \\(\\times\\) English - 0.68 \\(\\times\\) Math\nSo to make PC1, we mix 0.68 part of English score with 0.74 parts of Math score.\nOne unit of PC1 consists of 0.68 parts of English and 0.74 parts of Math.\nAnd because the weight of math score is a little bit larger, Math score is a little bit more important when it comes to describing how the data are spread out.\nPC1 can be viewed as an overall intelligence index because it is an weighted average combining both verbal and quantitative reasoning abilities.\nPC2 accounts for individual difference in English and Math scores.\n\n\\(0.68^2 + 0.74^2 = 1\\) (Pythagorean theorem)\nThe combination weights 0.68, 0.74, etc are called loadings of PC."
  },
  {
    "objectID": "slides/19-pca.html#variation",
    "href": "slides/19-pca.html#variation",
    "title": "Principal Component Analysis \n",
    "section": "Variation",
    "text": "Variation\n\n\n\nIf the variance for PC1 is \\(17\\) and the variance for PC2 is \\(2\\), the total variation presented in the data is \\(17+2=19\\).\nPC1 accounts for \\(17/19 = 89\\%\\) of the total variation, and PC2 accounts for \\(2/19 = 11\\%\\) of the total variation.\n\n\nOK how do we quantify variation. Let’s give it a definition.\nVariation for PC1 \\(= \\frac{\\text{Sum of squared distances of projected points on PC1}}{n-1}\\)\n\nVariation for PC2 \\(= \\frac{\\text{Sum of squared distances of projected points on PC2}}{n-1}\\)\n\nIf the variation for PC1 is \\(17\\) and the variation for PC2 is \\(2\\), the total variation presented in the data is \\(17+2=19\\).\nPC1 accounts for \\(17/19 = 89\\%\\) of the total variation, and PC2 accounts for \\(2/19 = 11\\%\\) of the total variation."
  },
  {
    "objectID": "slides/19-pca.html#how-about-3-or-more-variables",
    "href": "slides/19-pca.html#how-about-3-or-more-variables",
    "title": "Principal Component Analysis \n",
    "section": "How about 3 or More Variables?",
    "text": "How about 3 or More Variables?\n\nPC1 spans the direction of the most variation\n\n\n\nPC2 spans the direction of the 2nd most variation\n\n\n\n\nPC3 spans the direction of the 3rd most variation\n\n\n\n\nPC4 spans the direction of the 4th most variation\n\n\n\n\nIf we have \\(n\\) observations and \\(p\\) variables (dimensions), there are at most \\(\\min(n - 1, p)\\) PCs.\n\n\nOK. We use a 2D data to illustrate PCA. How about 3 or More Variables?\nWell the idea of 2D data can be exactly applied to 3 or more dimensional data."
  },
  {
    "objectID": "slides/19-pca.html#us-arrest-data-in-1973",
    "href": "slides/19-pca.html#us-arrest-data-in-1973",
    "title": "Principal Component Analysis \n",
    "section": "US Arrest Data in 1973",
    "text": "US Arrest Data in 1973\n\ndim(USArrests)\n\n[1] 50  4\n\n\n\nhead(USArrests, 16)\n\n            Murder Assault UrbanPop Rape\nAlabama       13.2     236       58   21\nAlaska        10.0     263       48   44\nArizona        8.1     294       80   31\nArkansas       8.8     190       50   20\nCalifornia     9.0     276       91   41\nColorado       7.9     204       78   39\nConnecticut    3.3     110       77   11\nDelaware       5.9     238       72   16\nFlorida       15.4     335       80   32\nGeorgia       17.4     211       60   26\nHawaii         5.3      46       83   20\nIdaho          2.6     120       54   14\nIllinois      10.4     249       83   24\nIndiana        7.2     113       65   21\nIowa           2.2      56       57   11\nKansas         6.0     115       66   18\n\n\n\nOK. Time to learn how to perform PCA in R.\nThe data set used for PCA is USArrests data set in 1973.\nEach observation or subject is a state, and we have 4 features or dimensions, Murder, Assault, UrbanPop, and Rape."
  },
  {
    "objectID": "slides/19-pca.html#pc-loading-vectors-on-usarrests",
    "href": "slides/19-pca.html#pc-loading-vectors-on-usarrests",
    "title": "Principal Component Analysis \n",
    "section": "PC Loading Vectors on USArrests\n",
    "text": "PC Loading Vectors on USArrests\n\n\npca_output <- prcomp(USArrests, scale = TRUE)\n\n## rotation matrix provides PC loadings\n(pca_output$rotation <- -pca_output$rotation)\n\n          PC1   PC2   PC3    PC4\nMurder   0.54  0.42 -0.34 -0.649\nAssault  0.58  0.19 -0.27  0.743\nUrbanPop 0.28 -0.87 -0.38 -0.134\nRape     0.54 -0.17  0.82 -0.089\n\n\n\nPCs are unique up to a sign change, so -pca_output$rotation gives us the same PCs as pca_output$rotation does.\n\n\n\\(\\text{PC1} = 0.54 \\times \\text{Murder} + 0.58 \\times \\text{Assault} + 0.28 \\times \\text{UrbanPop} + 0.54 \\times \\text{Rape}\\)\n\n\\(\\text{PC2} = -0.42 \\times \\text{Murder} - 0.19 \\times \\text{Assault} + 0.87 \\times \\text{UrbanPop} + 0.17 \\times \\text{Rape}\\)\n\n\n\n\nWe have 4 PCs because \\(\\min(n-1, p) = \\min(50-1, 4) = 4\\).\n\n\nTo perform PCA in R, it cannot be easier.\nWe just need to use the function prcomp(), and put the data set in the function. Then we get everything we want.\nHere I choose to scale the data because variables are not measured in the same scale. For example, Murder rate and UrbanPop are measured in different units.\nThis makes sure that every variable has variance 1, and our analysis is not affected by units.\nThe PCA results are stored as a list in pca_output object.\nOK first we can look at the rotation matrix because it provides PC loadings, and so we know what PC1 and PC2 are.\nChanging signs for easier interpretation of PCs\nThose PC loadings define how we rotates the coordinates to obtain the PCs. ???\nAgain, PC1 is just a linear combination or weighted average of the 4 variables, same as PC2. ???\nAgain, PC1 is just a linear combination or weighted average of the 4 variables, same as PC2.\nPCs are unique up to a sign change, so -pca_output$rotation gives us the same PCs as pca_output$rotation does.\nWe have 4 PCs because \\(\\min(n-1, k) = \\min(50-1, 4) = 4\\)."
  },
  {
    "objectID": "slides/19-pca.html#pc-scores",
    "href": "slides/19-pca.html#pc-scores",
    "title": "Principal Component Analysis \n",
    "section": "PC Scores",
    "text": "PC Scores\n\nThe value of the rotated data, the data values of each PC are stored in pca_output$x\n\n\n\nhead(pca_output$x <- -pca_output$x, 16) |> round(2)\n\n              PC1   PC2   PC3   PC4\nAlabama      0.98  1.12 -0.44 -0.15\nAlaska       1.93  1.06  2.02  0.43\nArizona      1.75 -0.74  0.05  0.83\nArkansas    -0.14  1.11  0.11  0.18\nCalifornia   2.50 -1.53  0.59  0.34\nColorado     1.50 -0.98  1.08  0.00\nConnecticut -1.34 -1.08 -0.64  0.12\nDelaware     0.05 -0.32 -0.71  0.87\nFlorida      2.98  0.04 -0.57  0.10\nGeorgia      1.62  1.27 -0.34 -1.07\nHawaii      -0.90 -1.55  0.05 -0.89\nIdaho       -1.62  0.21  0.26  0.49\nIllinois     1.37 -0.67 -0.67  0.12\nIndiana     -0.50 -0.15  0.23 -0.42\nIowa        -2.23 -0.10  0.16 -0.02\nKansas      -0.79 -0.27  0.03 -0.20\n\n\n\nThe value of the rotated data, the data values of each PC are stored in pca_output$x.\nIn other words, PC1 column here shows the projected values of observations onto PC1. PC2 column shows the projected values of observations onto PC2, and so on."
  },
  {
    "objectID": "slides/19-pca.html#interpretation-of-pcs",
    "href": "slides/19-pca.html#interpretation-of-pcs",
    "title": "Principal Component Analysis \n",
    "section": "Interpretation of PCs",
    "text": "Interpretation of PCs\n\npca_output$rotation\n\n          PC1   PC2   PC3    PC4\nMurder   0.54  0.42 -0.34 -0.649\nAssault  0.58  0.19 -0.27  0.743\nUrbanPop 0.28 -0.87 -0.38 -0.134\nRape     0.54 -0.17  0.82 -0.089\n\n\n\nPCs are less interpretable than original features.\nThe first loading vector places approximately equal weight on Assualt, Murder and Rape, with much less weights on UrbanPop.\nPC1 roughly corresponds to a overall serious crime rate.\n\n\n\nThe second loading vector places most of its weight on UrbanPop, and much less weight on the other 3 features.\nPC2 roughly corresponds to the level of urbanization.\n\n\nIntepretability decreases with the order of PCs.\nSo it’s easier to give PC1 a meaningful name than PC2, and PC2 is more meaningful than PC3, and so on. Because the PCs after the first 2 PCs usually explain quite small variation in the data, and some of them may be just noises.\nLet’s see if we can interpret these PCs.\nFirst keep in mind that PCs are less interpretable than original features. Sometimes we even don’t know how to interpret it, especially for PCs that explain small variations. So this is the price we pay for dimension reduction.\nBut let’s look at this example.\nThe first loading vector places approximately equal weight on Assualt, Murder and Rape, with much less weights on UrbanPop.\nSo PC1 roughly corresponds to a overall serious crime rate because PC1 explains the variations of data caused by those crime variables Assualt, Murder and Rape.\nOn the contrary, the second loading vector places most of its weight on UrbanPop, and much less weight on the other 3 features.\nSo we can say PC2 roughly corresponds to the level of urbanization.\nSo you get the idea, Assualt, Murder and Rape are similar each other because they all are measures of crime rate.\nSo when reducing dimensions, we sort of combine the three similar variables together to become a one single index that measures an overall crime rate.\nUrban population measures a totally different thing. So the variation created by this variable cannot be explained well by the crime rate, and it should be absorbed in PC2."
  },
  {
    "objectID": "slides/19-pca.html#d-representation-of-the-4d-data",
    "href": "slides/19-pca.html#d-representation-of-the-4d-data",
    "title": "Principal Component Analysis \n",
    "section": "2D Representation of the 4D data",
    "text": "2D Representation of the 4D data\n\n\n\npca_output$x |> tail(2) |> round(2)\n\n            PC1   PC2   PC3   PC4\nWisconsin -2.06 -0.61 -0.14 -0.18\nWyoming   -0.62  0.32 -0.24  0.16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHigher value of PC1 means higher crime rate (roughly).\nHigher value of PC2 means higher level of urbanization (roughly).\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd we can show our 2D Representation of the 4D data.\nHigher value of PC1 means higher crime rates (roughly).\nHigher value of PC2 means higher level of urbanization (roughly).\nWe may be able to further analysis on the reduced dimension data, for example, we may want to partition the data into 2 clusters.\nOne cluster has high crime rate and low urbanization, the other group shows low crime rate and high urbanization.\nOne cluster may be the low economically-developed states, and the other high economically-developed states.\nSo you see, we can tell lots of stories by analyzing our data."
  },
  {
    "objectID": "slides/19-pca.html#d-representation-of-the-4d-data-biplot",
    "href": "slides/19-pca.html#d-representation-of-the-4d-data-biplot",
    "title": "Principal Component Analysis \n",
    "section": "2D Representation of the 4D data: biplot",
    "text": "2D Representation of the 4D data: biplot\n\n\n\nbiplot(pca_output, xlabs = state.abb, \n       scale = 0)\n\n\nTop axis: PC1 loadings\nRight axis: PC2 loadings\nRed arrows: PC1 and PC2 loading vector, e.g., (0.28, 0.87) for UrbanPop.\nCrime-related variables (Assualt, Murder and Rape) are located close to each other.\n\nUrbanPop is far from the other three.\n\nAssualt, Murder and Rape are more correlated, and UrbanPop is less correlated with the other three.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can simply use the function biplot() to show the 2D Representation of the data.\nThis function also provide loading vector of PC1 and PC2 that gives us an idea of which direction means a large value of a variable/feature.\nThis is why it is called biplot because we plot two things in one single plot.\nHere, Top axis is for PC1 loadings\nRight axis is PC2 loadings\nRed arrows: PC1 and PC2 loading vector, e.g., (0.28, 0.87) for UrbanPop.\nSo NJ and Ca have pretty high urban pop rate because they are large in the UrbanPop arrow direction.\nCrime-related variables (Assualt, Murder and Rape) are located close to each other.\n\nUrbanPop is far from the other three.\n\nAssualt, Murder and Rape are more correlated, and UrbanPop is less correlated with the other three.\n\nAssualt, Murder and Rape sort of point to the same direction as PC1 and UrbanPop points to the same direction as PC2."
  },
  {
    "objectID": "slides/19-pca.html#proportion-of-variance-explained",
    "href": "slides/19-pca.html#proportion-of-variance-explained",
    "title": "Principal Component Analysis \n",
    "section": "Proportion of Variance Explained",
    "text": "Proportion of Variance Explained\n\nsummary(pca_output)\n\nImportance of components:\n                        PC1   PC2    PC3    PC4\nStandard deviation     1.57 0.995 0.5971 0.4164\nProportion of Variance 0.62 0.247 0.0891 0.0434\nCumulative Proportion  0.62 0.868 0.9566 1.0000\n\n\n\nPC1 explains \\(62\\%\\) of the variations in the data, and PC2 explains \\(24.7\\%\\) of the variance.\nPC1 and PC2 explain about \\(87\\%\\) of the variance, and the last two PCs explain only \\(13\\%\\).\n2D plot provides pretty accurate summary of the data.\n\n\nFinally I want to talk a little bit about Proportion of Variance Explained.\nIn the pca_output, we have SD of each PC.\nWe square it to get variance.\nThen if we divided by the sum of variance, we get the Proportion of Variance Explained by each PC.\nSo PC1 explains \\(62\\%\\) of the variations in the data, and PC2 explains \\(24.7\\%\\) of the variance.\nPC1 and PC2 explain about \\(87\\%\\) of the variance, and the last two PCs explain only \\(13\\%\\).\n2D plot provides pretty accurate summary of the data."
  },
  {
    "objectID": "slides/19-pca.html#scree-plot",
    "href": "slides/19-pca.html#scree-plot",
    "title": "Principal Component Analysis \n",
    "section": "Scree Plot",
    "text": "Scree Plot\nLook for a point at which the proportion of variance explained by each subsequent PC drops off.\n\n\nFinally we can plot the proportion of variance explained by PCs. The plot is called scree plot.\nWe can Look for a point at which the proportion of variance explained by each subsequent PC drops off.\nand learn how many PCs we need to appropriately represent a high-dimensional data set."
  },
  {
    "objectID": "slides/19-pca.html#section-1",
    "href": "slides/19-pca.html#section-1",
    "title": "Principal Component Analysis \n",
    "section": "",
    "text": "23-Principal Component Analysis \nIn lab.qmd ## Lab 23 section,\n\nUse slice() to print the first six rows of iris data.\nPerform PCA on Sepal.Length, Sepal.Width, Petal.Length, and Petal.Width.\nGenerate biplot, and explain it.\n\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa"
  },
  {
    "objectID": "slides/19-pca.html#section-5",
    "href": "slides/19-pca.html#section-5",
    "title": "Principal Component Analysis \n",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n\nUSArrests = pd.read_csv('https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/datasets/USArrests.csv')\nUSArrests.head(4)\n\n   rownames  Murder  Assault  UrbanPop  Rape\n0   Alabama    13.2      236        58  21.2\n1    Alaska    10.0      263        48  44.5\n2   Arizona     8.1      294        80  31.0\n3  Arkansas     8.8      190        50  19.5\n\n\n\n\nUSArr = USArrests.drop(['rownames'], axis = 1)\nUSArr.index = USArrests['rownames']\nUSArr.head(4)\n\n          Murder  Assault  UrbanPop  Rape\nrownames                                 \nAlabama     13.2      236        58  21.2\nAlaska      10.0      263        48  44.5\nArizona      8.1      294        80  31.0\nArkansas     8.8      190        50  19.5\n\n\nhttps://stackoverflow.com/questions/68275857/urllib-error-urlerror-urlopen-error-ssl-certificate-verify-failed-certifica"
  },
  {
    "objectID": "slides/19-pca.html#section-6",
    "href": "slides/19-pca.html#section-6",
    "title": "Principal Component Analysis \n",
    "section": "",
    "text": "Standardization\n\n\nscaler = StandardScaler()\nX = scaler.fit_transform(USArr.values) ## Array\n\n\n\n\nPerform PCA (prcomp(USArrests, scale = TRUE))\n\n\npca = PCA(n_components=4)\npca.fit(X)\n\n\n\n\nPCA(n_components=4)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nPCAPCA(n_components=4)\n\n\n\n\n\n\n\n\nPCA Components (pca_output$rotation)\n\n\nresult = np.round(pca.components_.T, 2)\npd.DataFrame(result, columns=['PC1', 'PC2', 'PC3', 'PC4'], index=USArr.columns)\n\n           PC1   PC2   PC3   PC4\nMurder    0.54  0.42 -0.34  0.65\nAssault   0.58  0.19 -0.27 -0.74\nUrbanPop  0.28 -0.87 -0.38  0.13\nRape      0.54 -0.17  0.82  0.09"
  },
  {
    "objectID": "slides/19-pca.html#section-7",
    "href": "slides/19-pca.html#section-7",
    "title": "Principal Component Analysis \n",
    "section": "",
    "text": "Data on PCs (pca_output$x)\n\n\nX_pc = np.round(pca.transform(X), 2)\npd.DataFrame(X_pc, columns=['PC1', 'PC2', 'PC3', 'PC4'], index=USArr.index)\n\n                 PC1   PC2   PC3   PC4\nrownames                              \nAlabama         0.99  1.13 -0.44  0.16\nAlaska          1.95  1.07  2.04 -0.44\nArizona         1.76 -0.75  0.05 -0.83\nArkansas       -0.14  1.12  0.11 -0.18\nCalifornia      2.52 -1.54  0.60 -0.34\nColorado        1.51 -0.99  1.10  0.00\nConnecticut    -1.36 -1.09 -0.64 -0.12\nDelaware        0.05 -0.33 -0.72 -0.88\nFlorida         3.01  0.04 -0.58 -0.10\nGeorgia         1.64  1.28 -0.34  1.08\nHawaii         -0.91 -1.57  0.05  0.90\nIdaho          -1.64  0.21  0.26 -0.50\nIllinois        1.38 -0.68 -0.68 -0.12\nIndiana        -0.51 -0.15  0.23  0.42\nIowa           -2.25 -0.10  0.16  0.02\nKansas         -0.80 -0.27  0.03  0.21\nKentucky       -0.75  0.96 -0.03  0.67\nLouisiana       1.56  0.87 -0.78  0.45\nMaine          -2.40  0.38 -0.07 -0.33\nMaryland        1.76  0.43 -0.16 -0.56\nMassachusetts  -0.49 -1.47 -0.61 -0.18\nMichigan        2.11 -0.16  0.38  0.10\nMinnesota      -1.69 -0.63  0.15  0.07\nMississippi     1.00  2.39 -0.74  0.22\nMissouri        0.70 -0.26  0.38  0.23\nMontana        -1.19  0.54  0.25  0.12\nNebraska       -1.27 -0.19  0.18  0.02\nNevada          2.87 -0.78  1.16  0.31\nNew Hampshire  -2.38 -0.02  0.04 -0.03\nNew Jersey      0.18 -1.45 -0.76  0.24\nNew Mexico      1.98  0.14  0.18 -0.34\nNew York        1.68 -0.82 -0.64 -0.01\nNorth Carolina  1.12  2.23 -0.86 -0.95\nNorth Dakota   -2.99  0.60  0.30 -0.25\nOhio           -0.23 -0.74 -0.03  0.47\nOklahoma       -0.31 -0.29 -0.02  0.01\nOregon          0.06 -0.54  0.94 -0.24\nPennsylvania   -0.89 -0.57 -0.40  0.36\nRhode Island   -0.86 -1.49 -1.37 -0.61\nSouth Carolina  1.32  1.93 -0.30 -0.13\nSouth Dakota   -1.99  0.82  0.39 -0.11\nTennessee       1.00  0.86  0.19  0.65\nTexas           1.36 -0.41 -0.49  0.64\nUtah           -0.55 -1.47  0.29 -0.08\nVermont        -2.80  1.40  0.84 -0.14\nVirginia       -0.10  0.20  0.01  0.21\nWashington     -0.22 -0.97  0.62 -0.22\nWest Virginia  -2.11  1.42  0.10  0.13\nWisconsin      -2.08 -0.61 -0.14  0.18\nWyoming        -0.63  0.32 -0.24 -0.17\n\n\n\n\nExplained Variance (pca_output$sdev ^ 2)\n\n\nnp.round(pca.explained_variance_, 2)\n\narray([2.53, 1.01, 0.36, 0.18])\n\n\n\n\n\nmath3570-s24.github.io/website"
  },
  {
    "objectID": "slides/03-posit.html#integrated-development-environment",
    "href": "slides/03-posit.html#integrated-development-environment",
    "title": "Posit Cloud ☁️",
    "section": "Integrated Development Environment",
    "text": "Integrated Development Environment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR and Python are programming languages.\n\nPosit Cloud offers two integrated development environments (IDE):\n\nRStudio\nJupyterLab/Jupyter Notebook\n\n\n\nwhich are software for efficiently writing computer programs.\n\nWe’ll use Posit Cloud to write R and Python code."
  },
  {
    "objectID": "slides/03-posit.html#posit-cloud---data-science-wo-hardware-hassles",
    "href": "slides/03-posit.html#posit-cloud---data-science-wo-hardware-hassles",
    "title": "Posit Cloud ☁️",
    "section": "☁️ Posit Cloud - Data Science w/o hardware hassles",
    "text": "☁️ Posit Cloud - Data Science w/o hardware hassles\n\n😎 Implement R/Python programs without needing to install R/Python and the IDE in your laptop!\n😎 Posit Cloud lets you do, share and learn data science online for free!\n\n\n\n😞 Get everything ready locally: Lots of friction\n\nDownload and install R/Python\nDownload and install IDE\nInstall wanted R/Python packages:\n\ntidymodels\ntidyverse\nNumPy\n…\n\n\nDownload and install tools like Git\n\n\n\n\n🤓 Posit Cloud: Much less friction\n\n\n\n\n\n\n\n\n\nGo to https://posit.cloud/\n\nLog in\n\n\n> hello world!"
  },
  {
    "objectID": "slides/03-posit.html#install-posit-cloud",
    "href": "slides/03-posit.html#install-posit-cloud",
    "title": "Posit Cloud ☁️",
    "section": "Install Posit Cloud",
    "text": "Install Posit Cloud\n\n\n\nStep 1: In the Posit website https://posit.co/, choose Products > Posit Cloud as shown below."
  },
  {
    "objectID": "slides/03-posit.html#install-posit-cloud-1",
    "href": "slides/03-posit.html#install-posit-cloud-1",
    "title": "Posit Cloud ☁️",
    "section": "Install Posit Cloud",
    "text": "Install Posit Cloud\n\n\nStep 2: Click GET STARTED.\nStep 3: Free or Student > Sign Up. Please sign up with GitHub if you have one or use your Marquette email address.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfile:///Users/chenghanyu/Dropbox/academia/books/r/teach-r-online-master/01-cloud/01-cloud.html#37"
  },
  {
    "objectID": "slides/03-posit.html#new-projects",
    "href": "slides/03-posit.html#new-projects",
    "title": "Posit Cloud ☁️",
    "section": "New Projects",
    "text": "New Projects\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will talk about Git/GitHub shortly."
  },
  {
    "objectID": "slides/03-posit.html#workspaces",
    "href": "slides/03-posit.html#workspaces",
    "title": "Posit Cloud ☁️",
    "section": "Workspaces",
    "text": "Workspaces\n\nWhen you create an account on Posit Cloud you get a workspace of your own.\nYou can add a new workspace (click + New Space in sidebar) and control its permissions."
  },
  {
    "objectID": "slides/03-posit.html#welcome-to-3570-data-science",
    "href": "slides/03-posit.html#welcome-to-3570-data-science",
    "title": "Posit Cloud ☁️",
    "section": "Welcome to 3570 Data Science!",
    "text": "Welcome to 3570 Data Science!\n\n\nI’m sending you a link via email for joining the course workspace 2024-spring-math-3570. Please join.\n\n\n\n\n\n\n\n\n\n\nGo to D2l to send Posit Cloud invitation\nSubject Posit Cloud Invitation Message https://posit.cloud/spaces/459948/join?access_code=JY6N0CSQWE5qa7QFYxYKqWT9awi6wP2JqV3UNfj6\nBest, Dr. Yu"
  },
  {
    "objectID": "slides/03-posit.html#section-1",
    "href": "slides/03-posit.html#section-1",
    "title": "Posit Cloud ☁️",
    "section": "",
    "text": "In the bar, click workspace 2024-spring-math-3570.\nClick New Project > New RStudio Project to get into the IDE.\nIn Untitled Project, name your project as 3570-project.\nIn the Console pane, write R code: a string \"Hello WoRld!\" or math 2 + 4.\nTools > Global Options > Appearance to select your favorite editor theme."
  },
  {
    "objectID": "slides/03-posit.html#more-tips",
    "href": "slides/03-posit.html#more-tips",
    "title": "Posit Cloud ☁️",
    "section": "More Tips",
    "text": "More Tips\n\nRead the Posit Cloud guide"
  },
  {
    "objectID": "slides/03-posit.html#panes",
    "href": "slides/03-posit.html#panes",
    "title": "Posit Cloud ☁️",
    "section": "Panes",
    "text": "Panes"
  },
  {
    "objectID": "slides/03-posit.html#r-script",
    "href": "slides/03-posit.html#r-script",
    "title": "Posit Cloud ☁️",
    "section": "R Script",
    "text": "R Script\n\nA R script is a .R file that contains R code.\nTo create a R script, go to File > New > R Script, or click the green-plus icon on the topleft corner, and select R Script."
  },
  {
    "objectID": "slides/03-posit.html#python-script",
    "href": "slides/03-posit.html#python-script",
    "title": "Posit Cloud ☁️",
    "section": "Python Script",
    "text": "Python Script\n\nA Python script is a .py file that contains Python code.\nTo create a Python script, go to File > New > Python Script, or click the green-plus icon on the topleft corner, and select Python Script."
  },
  {
    "objectID": "slides/03-posit.html#run-code",
    "href": "slides/03-posit.html#run-code",
    "title": "Posit Cloud ☁️",
    "section": "Run Code",
    "text": "Run Code\n\n\n Run : run the current line or selection of code.\n\n\nctrl + enter (Win) or cmd + enter (Mac)\n\n\n\n Icon right to the Run : re-run the previous code.\n\n\nalt + ctrl + p (Win) or option + cmd + p (Mac)\n\n\n\n Source : run all the code in the R script.\n\n\nshift + ctrl + s (Win) or shift + cmd + s (Mac)\n\n\n\n Source with Echo : run all the code in the R script with the code printed in the console.\n\n\nshift + ctrl + enter (Win) or shift + cmd + enter (Mac)"
  },
  {
    "objectID": "slides/03-posit.html#run-python-code",
    "href": "slides/03-posit.html#run-python-code",
    "title": "Posit Cloud ☁️",
    "section": "Run Python Code",
    "text": "Run Python Code\n\nRunning Python code may need to update some packages. Please say YES!\nWhen you run the Python code in the R console, the console will switch from R to Python.\nType quit in the Python console to switch back to the R console.\n\n\nREPL = Read, Evaluate, Print, and Loop\nor type reticulate::repl_python()"
  },
  {
    "objectID": "slides/03-posit.html#environment-tab",
    "href": "slides/03-posit.html#environment-tab",
    "title": "Posit Cloud ☁️",
    "section": "Environment Tab",
    "text": "Environment Tab\n\nThe (global) environment is where we are currently working.\nAnything created or imported into the current R/Python session is stored in the environment and shown in the Environment tab.\n\nAfter we run the R script, objects stored in the environment are\n\nData set mtcars\n\nObject x storing integer values 1 to 10.\nObject y storing three numeric values 3, 5, 9."
  },
  {
    "objectID": "slides/03-posit.html#environment-tab-1",
    "href": "slides/03-posit.html#environment-tab-1",
    "title": "Posit Cloud ☁️",
    "section": "Environment Tab",
    "text": "Environment Tab\n\nAfter we run the Python script, the object stored in the environment is\n\nObject b storing a string Hello World!"
  },
  {
    "objectID": "slides/03-posit.html#history-tab",
    "href": "slides/03-posit.html#history-tab",
    "title": "Posit Cloud ☁️",
    "section": "History Tab",
    "text": "History Tab\n\nThe History tab keeps a record of all previous commands.\n\n\nsave icon: save all history to a file\n\nTo Console: send the selected commands to the console.\n\nTo Source : inserted the selected commands into the current script."
  },
  {
    "objectID": "slides/03-posit.html#history-tab-1",
    "href": "slides/03-posit.html#history-tab-1",
    "title": "Posit Cloud ☁️",
    "section": "History Tab",
    "text": "History Tab\n\nThe History tab keeps a record of all previous commands.\n\n\nsave icon: save all history to a file\n\nTo Console: send the selected commands to the console.\n\nTo Source : inserted the selected commands into the current script.\n\n\n\n\nIn the console pane, use ⬆️ to show the previous commands."
  },
  {
    "objectID": "slides/03-posit.html#r-packages",
    "href": "slides/03-posit.html#r-packages",
    "title": "Posit Cloud ☁️",
    "section": "R Packages 📦",
    "text": "R Packages 📦\n\nWhen we start a R session, only built-in packages like base, stats, graphics, etc are available.\nInstalling packages is an easy way to get access to others data and functions.\n\n\n and more!"
  },
  {
    "objectID": "slides/03-posit.html#installing-r-packages",
    "href": "slides/03-posit.html#installing-r-packages",
    "title": "Posit Cloud ☁️",
    "section": "Installing R Packages 📦",
    "text": "Installing R Packages 📦\n\n\n\nTo install a package, for example, the ggplot2 package, we use the command\n\n\ninstall.packages(\"ggplot2\")\n\n\nIn the right-bottom pane, Packages > Install"
  },
  {
    "objectID": "slides/03-posit.html#loading-r-packages",
    "href": "slides/03-posit.html#loading-r-packages",
    "title": "Posit Cloud ☁️",
    "section": "Loading R Packages 📦",
    "text": "Loading R Packages 📦\n\n\n\nWhat happened when you run\n\nggplot(mpg, aes(x = displ, \n                y = hwy, \n                colour = class)) + \n    geom_point()\n\n\n\n\nTo use any function or data in ggplot2, we write ggplot2:: followed by the name of the function or data.\n\n\nggplot2::ggplot(ggplot2::mpg, \n                ggplot2::aes(\n                    x = displ, \n                    y = hwy, \n                    colour = class)\n                ) + \n    ggplot2::geom_point()\n\n\n\n\n\nWe can load the package into our R session using library().\n\nWith library(ggplot2), R knows the function and data are from the ggplot2 package.\n\n\nlibrary(ggplot2)\nggplot(mpg, aes(x = displ, \n                y = hwy, \n                colour = class)) + \n    geom_point()"
  },
  {
    "objectID": "slides/03-posit.html#help",
    "href": "slides/03-posit.html#help",
    "title": "Posit Cloud ☁️",
    "section": "Help",
    "text": "Help\n\nDon’t know how a function works or what a data set is about ❓\n👉 Simply type ? followed by the data name or function name like\n\n\n?mean\n?mpg\n\n\n\nWhat does the function mean() do? What is the size of mpg?\n\n\n\n\n\n\n\n\n\n\n\nA document will show up in the Help tab, teaching you how to use the function or explaining the data set."
  },
  {
    "objectID": "slides/03-posit.html#section-4",
    "href": "slides/03-posit.html#section-4",
    "title": "Posit Cloud ☁️",
    "section": "",
    "text": "01-Running R Script \n\nLoad R package ggplot2 into your Posit Cloud.\n\n\n## install the package if you haven't!\n________(ggplot2)\n\n\nCreate a R script named lab01-run-script.R in your 3570-project.\nCopy and paste the code below into the script, and save it.\n\n\nbar <- ggplot(data = diamonds) + \n    geom_bar(mapping = aes(x = cut, fill = cut), \n             show.legend = FALSE, width = 1) + \n    theme(aspect.ratio = 1) +\n    labs(x = NULL, y = NULL)\nbar + coord_flip()\nbar + coord_polar()\n\n\nSource the script. A pretty plot showing up?!  \n\n\n\n\n\n\nmath3570-s24.github.io/website"
  },
  {
    "objectID": "slides/06-syntax.html#run-code-in-console",
    "href": "slides/06-syntax.html#run-code-in-console",
    "title": "Basic R/Python \n",
    "section": "Run Code in Console",
    "text": "Run Code in Console\n\n\n\n\nreticulate::repl_python() to Python.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nquit or exit to change Console back to R."
  },
  {
    "objectID": "slides/06-syntax.html#arithmetic-and-logical-operators",
    "href": "slides/06-syntax.html#arithmetic-and-logical-operators",
    "title": "Basic R/Python \n",
    "section": "Arithmetic and Logical Operators",
    "text": "Arithmetic and Logical Operators\n\n\n\n\n\n\n\n\n\n\n\n2 + 3 / (5 * 4) ^ 2\n\n[1] 2.01\n\n5 == 5.00\n\n[1] TRUE\n\n# 5 and 5L are of the same value too\n# 5 is of type double; 5L is integer\n5 == 5L\n\n[1] TRUE\n\ntypeof(5L)\n\n[1] \"integer\"\n\n!TRUE == FALSE\n\n[1] TRUE\n\n\n\n\n\n\n\n\n\n\n\n\n\n2 + 3 / (5 * 4) ** 2\n\n2.0075\n\n5 == 5.00\n\nTrue\n\n5 == int(5)\n\nTrue\n\ntype(int(5))\n\n<class 'int'>\n\nnot True == False\n\nTrue\n\n\n\n\n\nrepl_python()"
  },
  {
    "objectID": "slides/06-syntax.html#arithmetic-and-logical-operators-1",
    "href": "slides/06-syntax.html#arithmetic-and-logical-operators-1",
    "title": "Basic R/Python \n",
    "section": "Arithmetic and Logical Operators",
    "text": "Arithmetic and Logical Operators\n\n\n\n\n\n\n\n\n\n\nType coercion: When doing AND/OR comparisons, all nonzero values are treated as TRUE and 0 as FALSE.\n\n-5 | 0\n\n[1] TRUE\n\n1 & 1\n\n[1] TRUE\n\n2 | 0\n\n[1] TRUE\n\n\n\n\n\n\n\n\n\n\n\n\nbool() converts nonzero numbers to True and zero to False\n\n-5 | 0\n\n-5\n\n1 & 1\n\n1\n\nbool(2) | bool(0)\n\nTrue"
  },
  {
    "objectID": "slides/06-syntax.html#math-functions",
    "href": "slides/06-syntax.html#math-functions",
    "title": "Basic R/Python \n",
    "section": "Math Functions",
    "text": "Math Functions\n\n\n\n\n\n\n\n\n\n\nMath functions in R are built-in.\n\nsqrt(144)\n\n[1] 12\n\nexp(1)\n\n[1] 2.72\n\nsin(pi/2)\n\n[1] 1\n\nlog(32, base = 2)\n\n[1] 5\n\nabs(-7)\n\n[1] 7\n\n\n\n# R comment\n\n\n\n\n\n\n\n\n\n\n\nNeed to import math library in Python.\n\nimport math\nmath.sqrt(144)\n\n12.0\n\nmath.exp(1)\n\n2.718281828459045\n\nmath.sin(math.pi/2)\n\n1.0\n\nmath.log(32, 2)\n\n5.0\n\nabs(-7)\n\n7\n\n\n\n# python comment"
  },
  {
    "objectID": "slides/06-syntax.html#variables-and-assignment",
    "href": "slides/06-syntax.html#variables-and-assignment",
    "title": "Basic R/Python \n",
    "section": "Variables and Assignment",
    "text": "Variables and Assignment\n\n\n\n\n\n\n\n\n\n\nUse <- to do assignment. Why\n\n## we create an object, value 5, \n## and call it x, which is a variable\nx <- 5\nx\n\n[1] 5\n\n(x <- x + 6)\n\n[1] 11\n\nx == 5\n\n[1] FALSE\n\nlog(x)\n\n[1] 2.4\n\n\n\n\n\n\n\n\n\n\n\n\nUse = to do assignment.\n\nx = 5\nx\n\n5\n\nx = x + 6\nx\n\n11\n\nx == 5\n\nFalse\n\nmath.log(x)\n\n2.3978952727983707"
  },
  {
    "objectID": "slides/06-syntax.html#object-types",
    "href": "slides/06-syntax.html#object-types",
    "title": "Basic R/Python \n",
    "section": "Object Types",
    "text": "Object Types\n\n\n\n\n\n\n\n\n\n\ncharacter, double, integer and logical.\n\ntypeof(5)\n\n[1] \"double\"\n\n\n\ntypeof(5L)\n\n[1] \"integer\"\n\n\n\ntypeof(\"I_love_data_science!\")\n\n[1] \"character\"\n\n\n\ntypeof(1 > 3)\n\n[1] \"logical\"\n\n\n\nis.double(5L)\n\n[1] FALSE\n\n\n\n\n\n\n\n\n\n\n\n\nstr, float, int and bool.\n\ntype(5.0)\n\n<class 'float'>\n\ntype(5)\n\n<class 'int'>\n\ntype(\"I_love_data_science!\")\n\n<class 'str'>\n\ntype(1 > 3)\n\n<class 'bool'>\n\ntype(5) is float\n\nFalse\n\n\n\n\n\npython long (long integers, they can also be represented in octal and hexadecimal)"
  },
  {
    "objectID": "slides/06-syntax.html#section-2",
    "href": "slides/06-syntax.html#section-2",
    "title": "Basic R/Python \n",
    "section": "",
    "text": "Variable defined previously is a scalar value, or in fact a (atomic) vector of length one."
  },
  {
    "objectID": "slides/06-syntax.html#atomic-vector",
    "href": "slides/06-syntax.html#atomic-vector",
    "title": "Basic R/Python \n",
    "section": "(Atomic) Vector",
    "text": "(Atomic) Vector\n\nTo create a vector, use c(), short for concatenate or combine.\n\nAll elements of a vector must be of the same type.\n\n\n\n\n(dbl_vec <- c(1, 2.5, 4.5)) \n\n[1] 1.0 2.5 4.5\n\n(int_vec <- c(1L, 6L, 10L))\n\n[1]  1  6 10\n\n## TRUE and FALSE can be written as T and F\n(log_vec <- c(TRUE, FALSE, F))  \n\n[1]  TRUE FALSE FALSE\n\n(chr_vec <- c(\"pretty\", \"girl\"))\n\n[1] \"pretty\" \"girl\"  \n\n\n\n\n## check how many elements in a vector\nlength(dbl_vec) \n\n[1] 3\n\n## check a compact description of \n## any R data structure\nstr(dbl_vec) \n\n num [1:3] 1 2.5 4.5"
  },
  {
    "objectID": "slides/06-syntax.html#sequence-of-numbers",
    "href": "slides/06-syntax.html#sequence-of-numbers",
    "title": "Basic R/Python \n",
    "section": "Sequence of Numbers",
    "text": "Sequence of Numbers\n\nUse : to create a sequence of integers.\nUse seq() to create a sequence of numbers of type double with more options. \n\n\n\n(vec <- 1:5) \n\n[1] 1 2 3 4 5\n\ntypeof(vec)\n\n[1] \"integer\"\n\n# a sequence of numbers from 1 to 10 with increment 2\n(seq_vec <- seq(from = 1, to = 10, by = 2))\n\n[1] 1 3 5 7 9\n\ntypeof(seq_vec)\n\n[1] \"double\""
  },
  {
    "objectID": "slides/06-syntax.html#operations-on-vectors",
    "href": "slides/06-syntax.html#operations-on-vectors",
    "title": "Basic R/Python \n",
    "section": "Operations on Vectors",
    "text": "Operations on Vectors\n\nWe can do any operations on vectors as we do on a scalar variable (vector of length 1).\n\n\n\n\n# Create two vectors\nv1 <- c(3, 8)\nv2 <- c(4, 100) \n\n## All operations happen element-wisely\n# Vector addition\nv1 + v2\n\n[1]   7 108\n\n# Vector subtraction\nv1 - v2\n\n[1]  -1 -92\n\n\n\n\n# Vector multiplication\nv1 * v2\n\n[1]  12 800\n\n# Vector division\nv1 / v2\n\n[1] 0.75 0.08\n\nsqrt(v2)\n\n[1]  2 10"
  },
  {
    "objectID": "slides/06-syntax.html#recycling-of-vectors",
    "href": "slides/06-syntax.html#recycling-of-vectors",
    "title": "Basic R/Python \n",
    "section": "Recycling of Vectors",
    "text": "Recycling of Vectors\n\nIf we apply arithmetic operations to two vectors of unequal length, the elements of the shorter vector will be recycled to complete the operations.  \n\n\n\nv1 <- c(3, 8, 4, 5)\n# The following 2 operations are the same\nv1 * 2\n\n[1]  6 16  8 10\n\nv1 * c(2, 2, 2, 2)\n\n[1]  6 16  8 10\n\nv3 <- c(4, 11)\nv1 + v3  ## v3 becomes c(4, 11, 4, 11) when doing the operation\n\n[1]  7 19  8 16"
  },
  {
    "objectID": "slides/06-syntax.html#subsetting-vectors",
    "href": "slides/06-syntax.html#subsetting-vectors",
    "title": "Basic R/Python \n",
    "section": "Subsetting Vectors",
    "text": "Subsetting Vectors\n\nTo extract element(s) in a vector, we use a pair of brackets [] with element indexing.\nThe indexing starts with 1.\n\n\n\n\nv1\n\n[1] 3 8 4 5\n\nv2\n\n[1]   4 100\n\n## The 3rd element\nv1[3] \n\n[1] 4\n\n\n\n\nv1[c(1, 3)]\n\n[1] 3 4\n\nv1[1:2]\n\n[1] 3 8\n\n## extract all except a few elements\n## put a negative sign before the vector of \n## indices\nv1[-c(2, 3)] \n\n[1] 3 5"
  },
  {
    "objectID": "slides/06-syntax.html#factor-1",
    "href": "slides/06-syntax.html#factor-1",
    "title": "Basic R/Python \n",
    "section": "Factor",
    "text": "Factor\n\nA vector of type factor can be ordered in a meaningful way.\n\nCreate a factor by factor(). It is a type of integer, not character. 😲 🙄\n\n\n## Create a factor from a character vector using function factor()\n(fac <- factor(c(\"med\", \"high\", \"low\")))\n\n[1] med  high low \nLevels: high low med\n\ntypeof(fac)  ## The type is integer.\n\n[1] \"integer\"\n\nstr(fac)  ## The integers show the level each element in vector fac belongs to.\n\n Factor w/ 3 levels \"high\",\"low\",\"med\": 3 1 2\n\n\n\n\norder_fac <- factor(c(\"med\", \"high\", \"low\"),\n                    levels = c(\"low\", \"med\", \"high\"))\nstr(order_fac)\n\n Factor w/ 3 levels \"low\",\"med\",\"high\": 2 3 1\n\n\nlevels(fac) ## Each level represents an integer, ordered from the vector alphabetically."
  },
  {
    "objectID": "slides/06-syntax.html#list-generic-vectors",
    "href": "slides/06-syntax.html#list-generic-vectors",
    "title": "Basic R/Python \n",
    "section": "List (Generic Vectors)",
    "text": "List (Generic Vectors)\n\n\nLists are different from (atomic) vectors: Elements can be of any type, including lists.\nConstruct a list by using list().\n\n\n\n\n## a list of 3 elements of different types\nx_lst <- list(idx = 1:3, \n              \"a\", \n              c(TRUE, FALSE))\n\n\n\n$idx\n[1] 1 2 3\n\n[[2]]\n[1] \"a\"\n\n[[3]]\n[1]  TRUE FALSE\n\n\n\n\nstr(x_lst)\n\nList of 3\n $ idx: int [1:3] 1 2 3\n $    : chr \"a\"\n $    : logi [1:2] TRUE FALSE\n\nnames(x_lst)\n\n[1] \"idx\" \"\"    \"\"   \n\nlength(x_lst)\n\n[1] 3"
  },
  {
    "objectID": "slides/06-syntax.html#subsetting-a-list",
    "href": "slides/06-syntax.html#subsetting-a-list",
    "title": "Basic R/Python \n",
    "section": "Subsetting a List",
    "text": "Subsetting a List\n\n\nReturn an  element  of a list\n\n## subset by name (a vector)\nx_lst$idx  \n\n[1] 1 2 3\n\n## subset by indexing (a vector)\nx_lst[[1]]  \n\n[1] 1 2 3\n\ntypeof(x_lst$idx)\n\n[1] \"integer\"\n\n\n\n\nReturn a  sub-list  of a list\n\n## subset by name (still a list)\nx_lst[\"idx\"]  \n\n$idx\n[1] 1 2 3\n\n## subset by indexing (still a list)\nx_lst[1]  \n\n$idx\n[1] 1 2 3\n\ntypeof(x_lst[\"idx\"])\n\n[1] \"list\"\n\n\n\n\n\n\nThis is where we should pay more attention to. When we subset a list, it may return an element of the list, or it returns a sub-list of the list.\nLet’s see how it happens.\nThis is our x_lst. We can subset a list by name or by indexing.\nSuppose we want the first element of the list, we can get it by its name using x_lst$idx.\nWe can also obtain it by using indexing like x_lst[[1]] because we want the first element.\nNotice that the way we subset a list returns an integer vector, the real first element of the list, not a list.\nLet’s see another case on the right.\nWe can also subset by name using single pair of brackets, and put the name inside the brackets.\nOr we can subset by indexing, again using single pair of brackets.\nAnd you see what happened? The way we subset a list here returns a sub-list, not the element itself.\nSo please be careful when subsetting a list.\nIf you want a vector, use these ways. If you want to keep it as a list, use these ways."
  },
  {
    "objectID": "slides/06-syntax.html#section-3",
    "href": "slides/06-syntax.html#section-3",
    "title": "Basic R/Python \n",
    "section": "",
    "text": "pepper packet pepper shaker"
  },
  {
    "objectID": "slides/06-syntax.html#section-4",
    "href": "slides/06-syntax.html#section-4",
    "title": "Basic R/Python \n",
    "section": "",
    "text": "If list x is a train carrying objects, then x[[5]] is the object in car 5; x[4:6] is a train of cars 4-6.\n— @RLangTip, https://twitter.com/RLangTip/status/268375867468681216"
  },
  {
    "objectID": "slides/06-syntax.html#matrix-1",
    "href": "slides/06-syntax.html#matrix-1",
    "title": "Basic R/Python \n",
    "section": "Matrix",
    "text": "Matrix\n\nA matrix is a two-dimensional analog of a vector with attribute dim.\nUse command matrix() to create a matrix.\n\n\n## Create a 3 by 2 matrix called mat\n(mat <- matrix(data = 1:6, nrow = 3, ncol = 2)) \n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\ndim(mat); nrow(mat); ncol(mat)\n\n[1] 3 2\n\n\n[1] 3\n\n\n[1] 2\n\n\n\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n\n\n$dim\n[1] 3 2"
  },
  {
    "objectID": "slides/06-syntax.html#row-and-column-names",
    "href": "slides/06-syntax.html#row-and-column-names",
    "title": "Basic R/Python \n",
    "section": "Row and Column Names",
    "text": "Row and Column Names\n\n\n\nmat\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n## assign row names and column names\nrownames(mat) <- c(\"A\", \"B\", \"C\")\ncolnames(mat) <- c(\"a\", \"b\")\nmat\n\n  a b\nA 1 4\nB 2 5\nC 3 6\n\n\n\n\nrownames(mat)\n\n[1] \"A\" \"B\" \"C\"\n\ncolnames(mat)\n\n[1] \"a\" \"b\"\n\nattributes(mat)\n\n$dim\n[1] 3 2\n\n$dimnames\n$dimnames[[1]]\n[1] \"A\" \"B\" \"C\"\n\n$dimnames[[2]]\n[1] \"a\" \"b\""
  },
  {
    "objectID": "slides/06-syntax.html#subsetting-a-matrix",
    "href": "slides/06-syntax.html#subsetting-a-matrix",
    "title": "Basic R/Python \n",
    "section": "Subsetting a Matrix",
    "text": "Subsetting a Matrix\n\nUse the same indexing approach as vectors on rows and columns.\nUse comma , to separate row and column index.\n\nmat[2, 2] extracts the element of the second row and second column.\n\n\n\n\nmat\n\n  a b\nA 1 4\nB 2 5\nC 3 6\n\n## all rows and 2nd column\n## leave row index blank\n## specify 2 in coln index\nmat[, 2]\n\nA B C \n4 5 6 \n\n\n\n\n## 2nd row and all columns\nmat[2, ] \n\na b \n2 5 \n\n## The 1st and 3rd rows and the 1st column\nmat[c(1, 3), 1] \n\nA C \n1 3"
  },
  {
    "objectID": "slides/06-syntax.html#binding-matrices",
    "href": "slides/06-syntax.html#binding-matrices",
    "title": "Basic R/Python \n",
    "section": "Binding Matrices",
    "text": "Binding Matrices\n\ncbind() (binding matrices by adding columns)\nrbind() (binding matrices by adding rows)\nWhen matrices are combined by columns (rows), they should have the same number of rows (columns).\n\n\n\n\nmat\n\n  a b\nA 1 4\nB 2 5\nC 3 6\n\nmat_c <- matrix(data = c(7,0,0,8,2,6), \n                nrow = 3, ncol = 2)\n## should have the same number of rows\ncbind(mat, mat_c)  \n\n  a b    \nA 1 4 7 8\nB 2 5 0 2\nC 3 6 0 6\n\n\n\n\nmat_r <- matrix(data = 1:4, \n                nrow = 2, \n                ncol = 2)\n## should have the same number of columns\nrbind(mat, mat_r)  \n\n  a b\nA 1 4\nB 2 5\nC 3 6\n  1 3\n  2 4"
  },
  {
    "objectID": "slides/06-syntax.html#data-frame-the-most-common-way-of-storing-datasets",
    "href": "slides/06-syntax.html#data-frame-the-most-common-way-of-storing-datasets",
    "title": "Basic R/Python \n",
    "section": "Data Frame: The Most Common Way of Storing Datasets",
    "text": "Data Frame: The Most Common Way of Storing Datasets\n\nA data frame is of type list of equal-length vectors, having a 2-dimensional structure.\nMore general than matrix: Different columns can have different types.\nUse data.frame() that takes named vectors as input “element”.\n\n\n\n\n## data frame w/ an dbl column named age\n## and char column named gender.\n(df <- data.frame(age = c(19, 21, 40), \n                  gen = c(\"m\",\"f\", \"m\")))\n\n  age gen\n1  19   m\n2  21   f\n3  40   m\n\n## a data frame has a list structure\nstr(df)  \n\n'data.frame':   3 obs. of  2 variables:\n $ age: num  19 21 40\n $ gen: chr  \"m\" \"f\" \"m\"\n\n\n\n\n## must set column names\n## or they are ugly and non-recognizable\ndata.frame(c(19,21,40), c(\"m\",\"f\",\"m\")) \n\n  c.19..21..40. c..m....f....m..\n1            19                m\n2            21                f\n3            40                m"
  },
  {
    "objectID": "slides/06-syntax.html#properties-of-data-frames",
    "href": "slides/06-syntax.html#properties-of-data-frames",
    "title": "Basic R/Python \n",
    "section": "Properties of Data Frames",
    "text": "Properties of Data Frames\nData frame has properties of matrix and list.\n\n\n\nnames(df)  ## df as a list\n\n[1] \"age\" \"gen\"\n\ncolnames(df)  ## df as a matrix\n\n[1] \"age\" \"gen\"\n\nlength(df) ## df as a list\n\n[1] 2\n\nncol(df) ## df as a matrix\n\n[1] 2\n\ndim(df) ## df as a matrix\n\n[1] 3 2\n\n\n\n\n## rbind() and cbind() can be used on df\ndf_r <- data.frame(age = 10, \n                   gen = \"f\")\nrbind(df, df_r)\n\n  age gen\n1  19   m\n2  21   f\n3  40   m\n4  10   f\n\ndf_c <- \n    data.frame(col = c(\"red\",\"blue\",\"gray\"))\n(df_new <- cbind(df, df_c))\n\n  age gen  col\n1  19   m  red\n2  21   f blue\n3  40   m gray"
  },
  {
    "objectID": "slides/06-syntax.html#subsetting-a-data-frame",
    "href": "slides/06-syntax.html#subsetting-a-data-frame",
    "title": "Basic R/Python \n",
    "section": "Subsetting a Data Frame",
    "text": "Subsetting a Data Frame\nCan use either list or matrix subsetting methods.\n\n\n\ndf_new\n\n  age gen  col\n1  19   m  red\n2  21   f blue\n3  40   m gray\n\n## Subset rows\ndf_new[c(1, 3), ]\n\n  age gen  col\n1  19   m  red\n3  40   m gray\n\n## select the row where age == 21\ndf_new[df_new$age == 21, ]\n\n  age gen  col\n2  21   f blue\n\n\n\n\n## Subset columns\n## like a list\ndf_new$age\n\n[1] 19 21 40\n\ndf_new[c(\"age\", \"gen\")]\n\n  age gen\n1  19   m\n2  21   f\n3  40   m\n\n## like a matrix\ndf_new[, c(\"age\", \"gen\")]\n\n  age gen\n1  19   m\n2  21   f\n3  40   m\n\n\n\n\n\n\n  age gen  col\n1  19   m  red\n3  40   m gray\n\n\n'data.frame':   3 obs. of  1 variable:\n $ age: num  19 21 40\n\n\n num [1:3] 19 21 40"
  },
  {
    "objectID": "slides/06-syntax.html#section-5",
    "href": "slides/06-syntax.html#section-5",
    "title": "Basic R/Python \n",
    "section": "",
    "text": "05-R Data Type Summary \n\nIn lab.qmd Lab 5,\n\nCreate R objects vector v1, factor f2, list l3, matrix m4 and data frame d5.\nCheck typeof() and class() of those objects, and create a list having the output below.\n\n\n\n\n\nv1 <- __________\nf2 <- __________\nl3 <- __________\nm4 <- __________\nd5 <- __________\nv <- c(type = typeof(v1), class = class(v1))\nf <- c(type = __________, class = _________)\nl <- c(type = __________, class = _________)\nm <- c(type = __________, class = _________)\nd <- c(type = __________, class = _________)\n____(vec    =   v,\n     ______ = ___,\n     ______ = ___,\n     ______ = ___,\n     ______ = ___)\n\n\n\n\n$vec\n     type     class \n \"double\" \"numeric\" \n\n$fac\n     type     class \n\"integer\"  \"factor\" \n\n$lst\n  type  class \n\"list\" \"list\" \n\n$mat\n     type    class1    class2 \n\"integer\"  \"matrix\"   \"array\" \n\n$df\n        type        class \n      \"list\" \"data.frame\""
  },
  {
    "objectID": "slides/06-syntax.html#python-lists",
    "href": "slides/06-syntax.html#python-lists",
    "title": "Basic R/Python \n",
    "section": "Python Lists",
    "text": "Python Lists\n\nPython has numbers and strings, but no built-in vector structure.\nTo create a sequence type of structure, we can use a list that can save several elements in an single object.\nTo create a list in Python, we use [].\n\n\n\n\nlst_num = [0, 2, 4] \nlst_num\n\n[0, 2, 4]\n\ntype(lst_num)\n\n<class 'list'>\n\nlen(lst_num)\n\n3\n\n\n\nList elements can have different types!\n\nlst = ['data', 'math', 34, True]\nlst\n\n['data', 'math', 34, True]"
  },
  {
    "objectID": "slides/06-syntax.html#subsetting-lists",
    "href": "slides/06-syntax.html#subsetting-lists",
    "title": "Basic R/Python \n",
    "section": "Subsetting Lists",
    "text": "Subsetting Lists\n\n\n\nIndexing in Python always starts at 0!\n\n0: the 1st element\n\n\nlst\n\n['data', 'math', 34, True]\n\nlst[0]\n\n'data'\n\ntype(lst[0]) ## not a list\n\n<class 'str'>\n\n\n\n\n-1: the last element\n\n\nlst[-2]\n\n34\n\n\n\n\n\n[a:b]: the (a+1)-th to b-th elements\n\n\nlst[1:4]\n\n['math', 34, True]\n\ntype(lst[1:4]) ## a list\n\n<class 'list'>\n\n\n\n\n[a:]: elements from the (a+1)-th to the last\n\n\nlst[2:]\n\n[34, True]\n\n\n\n\n\nWhat does lst[0:1] return? Is it a list?\n\n\nWhat does lst[0:1] return? Is it a list?"
  },
  {
    "objectID": "slides/06-syntax.html#lists-are-mutable",
    "href": "slides/06-syntax.html#lists-are-mutable",
    "title": "Basic R/Python \n",
    "section": "Lists are Mutable",
    "text": "Lists are Mutable\nLists are changed in place!\n\n\n\nlst[1]\n\n'math'\n\nlst[1] = \"stats\"\nlst\n\n['data', 'stats', 34, True]\n\n\n\n\nlst[2:] = [False, 77]\nlst\n\n['data', 'stats', False, 77]\n\n\n\n\nIf we change any element value in a list, the list itself will be changed as well."
  },
  {
    "objectID": "slides/06-syntax.html#list-operations-and-methods-list.method",
    "href": "slides/06-syntax.html#list-operations-and-methods-list.method",
    "title": "Basic R/Python \n",
    "section": "List Operations and Methods list.method()\n",
    "text": "List Operations and Methods list.method()\n\n\n\n\n## Concatenation\nlst_num + lst\n\n[0, 2, 4, 'data', 'stats', False, 77]\n\n\n\n## Repetition\nlst_num * 3 \n\n[0, 2, 4, 0, 2, 4, 0, 2, 4]\n\n\n\n## Membership\n34 in lst\n\nFalse\n\n\n\n\n## Appends \"cat\" to lst\nlst.append(\"cat\")\nlst\n\n['data', 'stats', False, 77, 'cat']\n\n\n\n## Removes and returns last object from list\nlst.pop()\n\n'cat'\n\nlst\n\n['data', 'stats', False, 77]\n\n\n\n## Removes object from list\nlst.remove(\"stats\")\nlst\n\n['data', False, 77]\n\n\n\n## Reverses objects of list in place\nlst.reverse()\nlst\n\n[77, False, 'data']\n\n\n\n\nThis is a common syntax in Python. We start with a Pyhton object of some type, then type dot followed by any method specifically for this particular data type or structure for operations. list.pop(index)"
  },
  {
    "objectID": "slides/06-syntax.html#tuples",
    "href": "slides/06-syntax.html#tuples",
    "title": "Basic R/Python \n",
    "section": "Tuples",
    "text": "Tuples\n\nTuples work exactly like lists except they are immutable, i.e., they can’t be changed in place.\nTo create a tuple, we use ().\n\n\n\n\ntup = ('data', 'math', 34, True)\ntup\n\n('data', 'math', 34, True)\n\ntype(tup)\n\n<class 'tuple'>\n\nlen(tup)\n\n4\n\n\n\n\ntup[2:]\n\n(34, True)\n\ntup[-2]\n\n34\n\n\n\ntup[1] = \"stats\"  ## does not work!\n# TypeError: 'tuple' object does not support item assignment\n\n\ntup\n\n('data', 'math', 34, True)\n\n\n\n\n‘tuple’ object does not support item assignment"
  },
  {
    "objectID": "slides/06-syntax.html#tuples-functions-and-methods",
    "href": "slides/06-syntax.html#tuples-functions-and-methods",
    "title": "Basic R/Python \n",
    "section": "Tuples Functions and Methods",
    "text": "Tuples Functions and Methods\n\n# Converts a list into tuple\ntuple(lst_num)\n\n(0, 2, 4)\n\n\n\n# number of occurance of \"data\"\ntup.count(\"data\")\n\n1\n\n\n\n# first index of \"data\"\ntup.index(\"data\")\n\n0\n\n\n\n\n\n\n\n\nNote\n\n\n\nLists have more methods than tuples because lists are more flexible."
  },
  {
    "objectID": "slides/06-syntax.html#dictionaries",
    "href": "slides/06-syntax.html#dictionaries",
    "title": "Basic R/Python \n",
    "section": "Dictionaries",
    "text": "Dictionaries\n\nA dictionary consists of key-value pairs.\nA dictionary is mutable, i.e., the values can be changed in place and more key-value pairs can be added.\nTo create a dictionary, we use {\"key name\": value}.\nThe value can be accessed by the key in the dictionary.\n\n\ndic = {'Name': 'Ivy', 'Age': 7, 'Class': 'First'}\n\n\ndic['Age']\n\n7\n\n\n\ndic['age']  ## does not work\n\n\ndic['Age'] = 9\ndic['Class'] = 'Third'\ndic\n\n{'Name': 'Ivy', 'Age': 9, 'Class': 'Third'}"
  },
  {
    "objectID": "slides/06-syntax.html#properties-of-dictionaries",
    "href": "slides/06-syntax.html#properties-of-dictionaries",
    "title": "Basic R/Python \n",
    "section": "Properties of Dictionaries",
    "text": "Properties of Dictionaries\n\nPython will use the last assignment!\n\n\ndic1 = {'Name': 'Ivy', 'Age': 7, 'Name': 'Liya'}\ndic1['Name']\n\n'Liya'\n\n\n\n\nKeys are unique and immutable.\nA key can be a tuple, but CANNOT be a list.\n\n\n## The first key is a tuple!\ndic2 = {('First', 'Last'): 'Ivy Lee', 'Age': 7}\ndic2[('First', 'Last')]\n\n'Ivy Lee'\n\n\n\n## does not work\ndic2 = {['First', 'Last']: 'Ivy Lee', 'Age': 7}\ndic2[['First', 'Last']]"
  },
  {
    "objectID": "slides/06-syntax.html#disctionary-methods",
    "href": "slides/06-syntax.html#disctionary-methods",
    "title": "Basic R/Python \n",
    "section": "Disctionary Methods",
    "text": "Disctionary Methods\n\n\n{'Name': 'Ivy', 'Age': 9, 'Class': 'Third'}\n\n\n\n## Returns list of dictionary dict's keys\ndic.keys()\n\ndict_keys(['Name', 'Age', 'Class'])\n\n\n\n\n\n## Returns list of dictionary dict's values\ndic.values()\n\ndict_values(['Ivy', 9, 'Third'])\n\n\n\n\n\n\n## Returns a list of dict's (key, value) tuple pairs\ndic.items()\n\ndict_items([('Name', 'Ivy'), ('Age', 9), ('Class', 'Third')])\n\n\n\n\n\n\n## Adds dictionary dic2's key-values pairs in to dic\ndic2 = {'Gender': 'female'}\ndic.update(dic2)\ndic\n\n{'Name': 'Ivy', 'Age': 9, 'Class': 'Third', 'Gender': 'female'}\n\n\n\n## Removes all elements of dictionary dict\ndic.clear()\ndic\n\n{}"
  },
  {
    "objectID": "slides/06-syntax.html#section-6",
    "href": "slides/06-syntax.html#section-6",
    "title": "Basic R/Python \n",
    "section": "",
    "text": "06-Python Data Structure \nIn lab.qmd Lab 6,\n\nCreate a Python list and dictionary similar to the R list below.\n\n\nx_lst <- list(idx = 1:3, \n              \"a\", \n              c(TRUE, FALSE))\n\nRemember to create Python code chunk\n\n```{Python}\n#| echo: true\n#| eval: false\n\n```\n\nAny issue of this Python chunk?\nCommit and Push your work once you are done."
  },
  {
    "objectID": "slides/06-syntax.html#python-data-structures-for-data-science",
    "href": "slides/06-syntax.html#python-data-structures-for-data-science",
    "title": "Basic R/Python \n",
    "section": "Python Data Structures for Data Science",
    "text": "Python Data Structures for Data Science\n\nPython built-in data structures are not specifically for data science.\nTo use more data science friendly functions and structures, such as array or data frame, Python relies on packages NumPy and pandas."
  },
  {
    "objectID": "slides/06-syntax.html#installing-numpy-and-pandas",
    "href": "slides/06-syntax.html#installing-numpy-and-pandas",
    "title": "Basic R/Python \n",
    "section": "Installing NumPy and pandas*",
    "text": "Installing NumPy and pandas*\nIn your lab-yourusername project, run\n\nlibrary(reticulate)\nvirtualenv_create(\"myenv\")\n\n\nGo to Tools > Global Options > Python > Select > Virtual Environments"
  },
  {
    "objectID": "slides/06-syntax.html#installing-numpy-and-pandas-1",
    "href": "slides/06-syntax.html#installing-numpy-and-pandas-1",
    "title": "Basic R/Python \n",
    "section": "Installing NumPy and pandas*",
    "text": "Installing NumPy and pandas*\nYou may need to restart R session. Do it, and in the new R session, run\n\nlibrary(reticulate)\npy_install(c(\"numpy\", \"pandas\", \"matplotlib\"))\n\n\nRun the following Python code, and make sure everything goes well.\n\nimport numpy as np\nimport pandas as pd\nv1 = np.array([3, 8])\nv1\ndf = pd.DataFrame({\"col\": ['red', 'blue', 'green']})\ndf"
  },
  {
    "objectID": "slides/06-syntax.html#central-tendency-mean-and-median",
    "href": "slides/06-syntax.html#central-tendency-mean-and-median",
    "title": "Basic R/Python \n",
    "section": "Central Tendency: Mean and Median",
    "text": "Central Tendency: Mean and Median\n\n\n\n\n\n\n\n\n\n\n\ndata <- c(3,12,56,9,230,22)\nmean(data)\n\n[1] 55.3\n\nmedian(data)  \n\n[1] 17\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata = np.array([3,12,56,9,230,22])\ntype(data)\n\n<class 'numpy.ndarray'>\n\nnp.mean(data)\n\n55.333333333333336\n\nnp.median(data)\n\n17.0\n\n\n\n\n\n\nThe (arithmetic) mean or average is adding up all of the values, then dividing by the total number of them.\nMean balances data Values.\nThe median is the middle value when data values are arranged from the lowest to highest.\nMean is sensitive to extreme values (outliers)."
  },
  {
    "objectID": "slides/06-syntax.html#variation",
    "href": "slides/06-syntax.html#variation",
    "title": "Basic R/Python \n",
    "section": "Variation",
    "text": "Variation\n\n\n\n\n\n\n\n\n\n\n\nquantile(data, c(0.25, 0.5, 0.75)) \n\n  25%   50%   75% \n 9.75 17.00 47.50 \n\n\n\nvar(data)\n\n[1] 7677\n\nsd(data)\n\n[1] 87.6\n\n\n\nsummary(data)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    3.0     9.8    17.0    55.3    47.5   230.0 \n\n\n\n\n\n\n\n\n\n\n\n\n\nnp.quantile(data,  [0.25, 0.5, 0.75])\n\narray([ 9.75, 17.  , 47.5 ])\n\n\n\nnp.var(data, ddof = 1)\n\n7676.666666666666\n\nnp.std(data, ddof = 1)\n\n87.61658899242008\n\n\n\ndf = pd.Series(data)\ndf.describe()\n\ncount      6.000000\nmean      55.333333\nstd       87.616589\nmin        3.000000\n25%        9.750000\n50%       17.000000\n75%       47.500000\nmax      230.000000\ndtype: float64\n\n\n\n\n\n\n\np-th percentile: a data value such that at most \\(p\\%\\) of the data values are below it and at most \\((1−p)\\%\\) of the values are above it.\n\nFirst Quartile (Q1): the 25-th percentile\n\nSecond Quartile (Q2): the 50-th percentile (Median)\n\nThird Quartile (Q3): the 75-th percentile\n\nInterquartile Range (IQR): Q3 - Q1 IQR(data)\n\n\nq75, q25 = np.percentile(data, [75 ,25])\nq75 - q25\n\n37.75\n\n\nDelta Degrees of Freedom."
  },
  {
    "objectID": "slides/06-syntax.html#r-plot",
    "href": "slides/06-syntax.html#r-plot",
    "title": "Basic R/Python \n",
    "section": "R plot()\n",
    "text": "R plot()\n\n\n\n\nmtcars[1:15, 1:4]\n\n                    mpg cyl disp  hp\nMazda RX4          21.0   6  160 110\nMazda RX4 Wag      21.0   6  160 110\nDatsun 710         22.8   4  108  93\nHornet 4 Drive     21.4   6  258 110\nHornet Sportabout  18.7   8  360 175\nValiant            18.1   6  225 105\nDuster 360         14.3   8  360 245\nMerc 240D          24.4   4  147  62\nMerc 230           22.8   4  141  95\nMerc 280           19.2   6  168 123\nMerc 280C          17.8   6  168 123\nMerc 450SE         16.4   8  276 180\nMerc 450SL         17.3   8  276 180\nMerc 450SLC        15.2   8  276 180\nCadillac Fleetwood 10.4   8  472 205\n\n\n\n\nplot(x = mtcars$mpg, y = mtcars$hp, \n     xlab  = \"Miles per gallon\", \n     ylab = \"Horsepower\", \n     main = \"Scatter plot\", \n     col = \"red\", \n     pch = 5, las = 1)"
  },
  {
    "objectID": "slides/06-syntax.html#argument-pch",
    "href": "slides/06-syntax.html#argument-pch",
    "title": "Basic R/Python \n",
    "section": "Argument pch",
    "text": "Argument pch\n\n\nThe defualt is pch = 1"
  },
  {
    "objectID": "slides/06-syntax.html#python-matplotlib.pyplot",
    "href": "slides/06-syntax.html#python-matplotlib.pyplot",
    "title": "Basic R/Python \n",
    "section": "Python matplotlib.pyplot\n",
    "text": "Python matplotlib.pyplot\n\n\n\n\nCodemtcars = pd.read_csv('./data/mtcars.csv')\nmtcars.iloc[0:15,0:4]\n\n     mpg  cyl   disp   hp\n0   21.0    6  160.0  110\n1   21.0    6  160.0  110\n2   22.8    4  108.0   93\n3   21.4    6  258.0  110\n4   18.7    8  360.0  175\n5   18.1    6  225.0  105\n6   14.3    8  360.0  245\n7   24.4    4  146.7   62\n8   22.8    4  140.8   95\n9   19.2    6  167.6  123\n10  17.8    6  167.6  123\n11  16.4    8  275.8  180\n12  17.3    8  275.8  180\n13  15.2    8  275.8  180\n14  10.4    8  472.0  205\n\n\n\n\nimport matplotlib.pyplot as plt\nplt.scatter(x = mtcars.mpg, \n            y = mtcars.hp, \n            color = \"r\")\nplt.xlabel(\"Miles per gallon\")\nplt.ylabel(\"Horsepower\")\nplt.title(\"Scatter plot\")"
  },
  {
    "objectID": "slides/06-syntax.html#python-subplots",
    "href": "slides/06-syntax.html#python-subplots",
    "title": "Basic R/Python \n",
    "section": "Python Subplots",
    "text": "Python Subplots\n\n\n\n\n\n\nNote\n\n\n\nThe command plt.scatter() is used for creating one single plot. If multiple subplots are wanted in one single call, one can use the format\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\nax1.scatter(x, y)\nax2.plot(x, y)\n\n\n\n\n\n\nCheck Creating multiple subplots using plt.subplots for more details."
  },
  {
    "objectID": "slides/06-syntax.html#r-subplots",
    "href": "slides/06-syntax.html#r-subplots",
    "title": "Basic R/Python \n",
    "section": "R Subplots",
    "text": "R Subplots\n\npar(mfrow = c(1, 2))\nplot(x = mtcars$mpg, y = mtcars$hp, xlab = \"mpg\")\nplot(x = mtcars$mpg, y = mtcars$weight, xlab = \"mpg\")"
  },
  {
    "objectID": "slides/06-syntax.html#r-boxplot",
    "href": "slides/06-syntax.html#r-boxplot",
    "title": "Basic R/Python \n",
    "section": "R boxplot()\n",
    "text": "R boxplot()\n\n\n\n\nboxplot(mpg ~ cyl, \n        data = mtcars, \n        col = c(\"blue\", \"green\", \"red\"), \n        las = 1, \n        horizontal = TRUE,\n        xlab = \"Miles per gallon\", \n        ylab = \"Number of cylinders\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing relationships between numerical and categorical data, we can check side-by-side boxplots.\nFor example, if we want to check the data variation of (miles per gallon) for each number of cylinder, we can use boxplot() function.\nAnd inside the function, we use the formula mpg ~ cyl, and specify the dataset.\nThat is basically it. The rest of arguments are decorating your plot. They are optional."
  },
  {
    "objectID": "slides/06-syntax.html#python-boxplot",
    "href": "slides/06-syntax.html#python-boxplot",
    "title": "Basic R/Python \n",
    "section": "Python boxplot()\n",
    "text": "Python boxplot()\n\n\nCodecyl_index = np.sort(np.unique(np.array(mtcars.cyl)))\ncyl_shape = cyl_index.shape[0]\ncyl_list = []\nfor i in range (0, cyl_shape):\n    cyl_list.append(np.array(mtcars[mtcars.cyl == cyl_index[i]].mpg))\n\n\n\nimport matplotlib.pyplot as plt\nplt.boxplot(cyl_list, vert=False, labels=[4, 6, 8])\nplt.xlabel(\"Miles per gallon\")\nplt.ylabel(\"Number of cylinders\")"
  },
  {
    "objectID": "slides/06-syntax.html#r-hist",
    "href": "slides/06-syntax.html#r-hist",
    "title": "Basic R/Python \n",
    "section": "R hist()\n",
    "text": "R hist()\n\n\n\nhist() decides the class intervals/with based on breaks. If not provided, R chooses one.\n\n\n\n\nhist(mtcars$wt, \n     breaks = 20, \n     col = \"#003366\", \n     border = \"#FFCC00\", \n     xlab = \"weights\", \n     main = \"Histogram of weights\",\n     las = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nBesides color names, you can also use hex number to specify colors. Pretty handy."
  },
  {
    "objectID": "slides/06-syntax.html#python-hist",
    "href": "slides/06-syntax.html#python-hist",
    "title": "Basic R/Python \n",
    "section": "Python hist()\n",
    "text": "Python hist()\n\n\nplt.hist(mtcars.wt, \n         bins = 19, \n         color=\"#003366\",\n         edgecolor=\"#FFCC00\")\nplt.xlabel(\"weights\")\nplt.title(\"Histogram of weights\")"
  },
  {
    "objectID": "slides/06-syntax.html#r-barplot",
    "href": "slides/06-syntax.html#r-barplot",
    "title": "Basic R/Python \n",
    "section": "R barplot()\n",
    "text": "R barplot()\n\n\n(counts <- table(mtcars$gear)) \n\n\n 3  4  5 \n15 12  5 \n\n\n\nmy_bar <- barplot(counts, \n                  main = \"Car Distribution\", \n                  xlab = \"Number of Gears\", \n                  las = 1)\ntext(x = my_bar, y = counts - 0.8, \n     labels = counts, \n     cex = 0.8)"
  },
  {
    "objectID": "slides/06-syntax.html#python-barplot",
    "href": "slides/06-syntax.html#python-barplot",
    "title": "Basic R/Python \n",
    "section": "Python barplot()\n",
    "text": "Python barplot()\n\n\ncount_py = mtcars.value_counts('gear')\ncount_py\n\ngear\n3    15\n4    12\n5     5\nName: count, dtype: int64\n\n\n\nplt.bar(count_py.index, count_py)\nplt.xlabel(\"Number of Gears\")\nplt.title(\"Car Distribution\")"
  },
  {
    "objectID": "slides/06-syntax.html#r-pie",
    "href": "slides/06-syntax.html#r-pie",
    "title": "Basic R/Python \n",
    "section": "R pie()\n",
    "text": "R pie()\n\n\n(percent <- round(counts / sum(counts) * 100, 2))\n\n\n   3    4    5 \n46.9 37.5 15.6 \n\n(labels <- paste0(3:5, \" gears: \", percent, \"%\"))\n\n[1] \"3 gears: 46.88%\" \"4 gears: 37.5%\"  \"5 gears: 15.62%\"\n\n\n\npie(x = counts, labels = labels,\n    main = \"Pie Chart\", \n    col = 2:4, \n    radius = 1)\n\n\n\nPie charts are used for categorical variables, especially when we want to know percentage of each category.\nThe first argument is the frequency table, and you can add labels to each category."
  },
  {
    "objectID": "slides/06-syntax.html#python-pie",
    "href": "slides/06-syntax.html#python-pie",
    "title": "Basic R/Python \n",
    "section": "Python pie()\n",
    "text": "Python pie()\n\n\npercent = round(count_py / sum(count_py) * 100, 2)\ntexts = [str(percent.index[k]) + \" gear \" + str(percent.array[k]) + \"%\" for k in range(0,3)]\n\n\nplt.pie(count_py, labels = texts, colors = ['r', 'g', 'b'])\nplt.title(\"Pie Charts\")"
  },
  {
    "objectID": "slides/06-syntax.html#r-2d-imaging-image",
    "href": "slides/06-syntax.html#r-2d-imaging-image",
    "title": "Basic R/Python \n",
    "section": "R 2D Imaging: image()\n",
    "text": "R 2D Imaging: image()\n\n\nThe image() function displays the values in a matrix using color.\n\n\n\n\nmatrix(1:30, 6, 5)\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    7   13   19   25\n[2,]    2    8   14   20   26\n[3,]    3    9   15   21   27\n[4,]    4   10   16   22   28\n[5,]    5   11   17   23   29\n[6,]    6   12   18   24   30\n\nimage(matrix(1:30, 6, 5))\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn Python,\n\nplt.imshow(mat_img, cmap='Oranges')"
  },
  {
    "objectID": "slides/06-syntax.html#r-fieldsimage.plot",
    "href": "slides/06-syntax.html#r-fieldsimage.plot",
    "title": "Basic R/Python \n",
    "section": "R fields::image.plot()\n",
    "text": "R fields::image.plot()\n\n\n\n\nlibrary(fields)\nstr(volcano)\n\n num [1:87, 1:61] 100 101 102 103 104 105 105 106 107 108 ...\n\nimage.plot(volcano)"
  },
  {
    "objectID": "slides/06-syntax.html#r-2d-imaging-example-volcano",
    "href": "slides/06-syntax.html#r-2d-imaging-example-volcano",
    "title": "Basic R/Python \n",
    "section": "R 2D Imaging Example: Volcano",
    "text": "R 2D Imaging Example: Volcano"
  },
  {
    "objectID": "slides/06-syntax.html#r-3d-scatter-plot-scatterplot3d",
    "href": "slides/06-syntax.html#r-3d-scatter-plot-scatterplot3d",
    "title": "Basic R/Python \n",
    "section": "R 3D scatter plot: scatterplot3d()\n",
    "text": "R 3D scatter plot: scatterplot3d()\n\n\n\n\nlibrary(\"scatterplot3d\")\nscatterplot3d(x = mtcars$wt, \n              y = mtcars$disp, \n              z = mtcars$mpg, \n              main = \"3D Scatter Plot\", \n              xlab = \"Weights\", \n              ylab = \"Displacement\",\n              zlab = \"Miles per gallon\", \n              pch = 16, \n              color = \"steelblue\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn Python,\n\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')"
  },
  {
    "objectID": "slides/06-syntax.html#r-perspective-plot-persp",
    "href": "slides/06-syntax.html#r-perspective-plot-persp",
    "title": "Basic R/Python \n",
    "section": "R Perspective Plot: persp()\n",
    "text": "R Perspective Plot: persp()\n\n\n\n\n# Exaggerate the relief\nz <- 2 * volcano      \n# 10 meter spacing (S to N)\nx <- 10 * (1:nrow(z))   \n# 10 meter spacing (E to W)\ny <- 10 * (1:ncol(z))   \npar(bg = \"slategray\")\npersp(x, y, z, \n      theta = 135, \n      phi = 30, \n      col = \"green3\", \n      scale = FALSE,\n      ltheta = -120, \n      shade = 0.75, \n      border = NA, \n      box = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn Python,\n\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\nax.plot_surface(X, Y, Z)\n\n\nYou can also generate a so-called perspective plot using the function persp().\nI don’t use the function often. You can try it with different argument values, and see how it changes."
  },
  {
    "objectID": "slides/06-syntax.html#section-7",
    "href": "slides/06-syntax.html#section-7",
    "title": "Basic R/Python \n",
    "section": "",
    "text": "07-Plotting (Bonus question!)  \nIn lab.qmd ## Lab 7,\n\nFor the mtcars data, use R or Python to\n\nmake a scatter plot of miles per gallon vs. weight. Decorate your plot using arguments, col, pch, xlab, etc.\ncreate a histogram of 1/4 mile time. Make it beautiful!\n\n\nCommit and Push your work once you are done.\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nmtcars = pd.read_csv('./data/mtcars.csv')\n\n\n\nFind your mate and work in pairs. \nTwo volunteer pairs teach us how to make beautiful plots next Tuesday (Feb 13)!\nThe presenters will be awarded a hex sticker! 😎"
  },
  {
    "objectID": "slides/06-syntax.html#resources",
    "href": "slides/06-syntax.html#resources",
    "title": "Basic R/Python \n",
    "section": "Resources",
    "text": "Resources\n\nThe R Graph Gallery\nmatplotlib\n\nWe will talk about data visualization in detail soon!\nhttps://stackoverflow.com/questions/43482191/matplotlib-axes-plot-vs-pyplot-plot\n\n\n\nmath3570-s24.github.io/website"
  },
  {
    "objectID": "slides/14-tidyr.html#grammar-of-data-tidying",
    "href": "slides/14-tidyr.html#grammar-of-data-tidying",
    "title": "Tidying Data 🧹",
    "section": "Grammar of Data Tidying",
    "text": "Grammar of Data Tidying\n\n\nHave data organised in an unideal way for our analysis\n\nWant to re-organise the data to carry on with our analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe goal of tidyr is to help you tidy your data via\n\n\npivoting for going between wider and longer data\n\nseparating and uniting character columns\nclarifying how NAs should be treated\nnesting and unnesting columns\n\n\n\n\nSuppose we have a data set that is untidy, and organised in an unideal way for our analysis.\nWhat tidyr is doing is not like dplyr, just filter rows or select columns, because it is kinda meaningless, because the data after filtering or selecting from the untidy data are still untidy.\nSo our goal is to re-organise the data so that it is clean enough for being filtered or selected and for later data analysis.\ntidyr helps you tidy your data via\n\n\npivoting for going between wider and longer data\n\nseparating and uniting character columns\nclarifying how NAs should be treated. for example, do we wanna keep them or drop them, or fill in with some values.\nnesting and unnesting columns\n\n\nWe will go through the first three. You can check the tidyr page to learn more about its functionality."
  },
  {
    "objectID": "slides/14-tidyr.html#pivoting",
    "href": "slides/14-tidyr.html#pivoting",
    "title": "Tidying Data 🧹",
    "section": "Pivoting",
    "text": "Pivoting\n\nTo tidy your data,\n\nfirst, figure out what the (column) variables and (row) observations are\nsecond is to resolve one of two common problems:\n\nOne (column) variable might be spread across multiple columns.\nOne (row) observation might be scattered across multiple rows.\n\n\n\n\n\n\n\ncustomers <- read_csv(\"./data/sales/customers.csv\")\n\n\n\nwider (\\(2 \\times 4\\))\nmore columns\n\ncustomers\n\n# A tibble: 2 × 4\n  customer_id item_1 item_2       item_3\n        <dbl> <chr>  <chr>        <chr> \n1           1 bread  milk         banana\n2           2 milk   toilet paper <NA>  \n\n\n\nlonger (\\(6 \\times 3\\))\nmore rows by pivot_longer()\n\n\n# A tibble: 6 × 3\n  customer_id item_no item        \n        <dbl> <chr>   <chr>       \n1           1 item_1  bread       \n2           1 item_2  milk        \n3           1 item_3  banana      \n4           2 item_1  milk        \n5           2 item_2  toilet paper\n6           2 item_3  <NA>        \n\n\n\n\n\nIf our data is untidy, to tidy our data, first, we get to figure out what the variables (column) and observations (row) are. Remember the definition of tidy data. each row is for one and only one observation, and each column is for one and only one variable.\nOnce we know observations and variables, we need to resolve one of two common problems:\n\nOne variable might be spread across multiple columns.\nOne observation might be scattered across multiple rows.\n\n\neither problem is a reason why our data is not tidy."
  },
  {
    "objectID": "slides/14-tidyr.html#pivot_longer-and-pivot_wider",
    "href": "slides/14-tidyr.html#pivot_longer-and-pivot_wider",
    "title": "Tidying Data 🧹",
    "section": "\npivot_longer() and pivot_wider()\n",
    "text": "pivot_longer() and pivot_wider()\n\n\nTo fix these problems, we’ll need pivot_longer() and pivot_wider()\n\nStarts with a data set,\n\n\npivot_longer() add more rows and decreases the number of columns.\n\npivot_wider() add more columns and decreases the number of rows.\n\n\n\n\nTo fix these problems, we’ll need the functions pivot_longer() and pivot_wider()\n\npivot_longer() starts with a data set and add more rows to it. Make the data set longer\npivot_wider() starts with a data set and add more columns to it. Make the data set wider"
  },
  {
    "objectID": "slides/14-tidyr.html#pivot_longer-and-pivot_wider-1",
    "href": "slides/14-tidyr.html#pivot_longer-and-pivot_wider-1",
    "title": "Tidying Data 🧹",
    "section": "\npivot_longer() and pivot_wider()\n",
    "text": "pivot_longer() and pivot_wider()\n\n\n\nOne variable spreads across multiple columns\n\n\n\n\n\n\n\n\n\nOne subject is scattered across multiple rows\n\n\n\n\n\n\n\n\n\n\n\nTo change the left data set to the right data set, we use pivot_longer(), because the transformed data set has more rows and will typically has less columns as well.\nTo change the right data set to the left data set, we use pivot_wider(), because the transformed data set has more columns and will typically has less rows as well."
  },
  {
    "objectID": "slides/14-tidyr.html#pivot_longer-and-pivot_wider-2",
    "href": "slides/14-tidyr.html#pivot_longer-and-pivot_wider-2",
    "title": "Tidying Data 🧹",
    "section": "\npivot_longer() and pivot_wider()\n",
    "text": "pivot_longer() and pivot_wider()\n\n\n\nthe gif here illustrate the idea of pivot_longer() and pivot_wider() function."
  },
  {
    "objectID": "slides/14-tidyr.html#data-salescustomers.csv",
    "href": "slides/14-tidyr.html#data-salescustomers.csv",
    "title": "Tidying Data 🧹",
    "section": "Data: sales/customers.csv",
    "text": "Data: sales/customers.csv\n\ncustomers <- read_csv(\"data/sales/customers.csv\")\n\n\n\nwider (\\(2 \\times 4\\))\nmore columns\n\ncustomers\n\n# A tibble: 2 × 4\n  customer_id item_1 item_2       item_3\n        <dbl> <chr>  <chr>        <chr> \n1           1 bread  milk         banana\n2           2 milk   toilet paper <NA>  \n\n\n\nlonger (\\(6 \\times 3\\))\nmore rows by pivot_longer()\n\n\n# A tibble: 6 × 3\n  customer_id item_no item        \n        <dbl> <chr>   <chr>       \n1           1 item_1  bread       \n2           1 item_2  milk        \n3           1 item_3  banana      \n4           2 item_1  milk        \n5           2 item_2  toilet paper\n6           2 item_3  <NA>        \n\n\n\n\n\nAnd the original customers data is in this wider format, where I have one row per customer. We have two customers and so two rows in the data frame.\nand then we have individual columns for the items they bought. The first customer bought 3 items, bread, milk, banana and the second customer bought two items, milk and toilet paper, so the the item_3 column value is NA.\nBut this data format may not be what we want.\nIn fact, what we want to do for this particular example is to have it in a format where one row per customer per item, so I can do further analysis.\nso we wanna go from wider to longer format, because the format we want has more rows and less columns."
  },
  {
    "objectID": "slides/14-tidyr.html#pivot_longer",
    "href": "slides/14-tidyr.html#pivot_longer",
    "title": "Tidying Data 🧹",
    "section": "pivot_longer()",
    "text": "pivot_longer()\n\n\n\n\ndata: data frame\n\n\n\npivot_longer(\n    data,\n    cols, \n    names_to = \"name\", \n    values_to = \"value\"\n    )\n\n\n\n\nSo we are going to use a function pivot_longer().\nLike other functions in tidyverse, the first argument is again a data frame."
  },
  {
    "objectID": "slides/14-tidyr.html#pivot_longer-1",
    "href": "slides/14-tidyr.html#pivot_longer-1",
    "title": "Tidying Data 🧹",
    "section": "pivot_longer()",
    "text": "pivot_longer()\n\n\n\ndata: data frame\ncols: columns to pivot into longer format\n\n\n\npivot_longer(\n    data,\n    cols, \n    names_to = \"name\", \n    values_to = \"value\"\n    )\n\n\n\n\nthen we give it the columns we want to pivot into the longer format"
  },
  {
    "objectID": "slides/14-tidyr.html#pivot_longer-2",
    "href": "slides/14-tidyr.html#pivot_longer-2",
    "title": "Tidying Data 🧹",
    "section": "pivot_longer()",
    "text": "pivot_longer()\n\n\n\ndata: data frame\ncols: columns to pivot into longer format\nnames_to: name of the column where column names of pivoted variables go (character string)\n\n\n\npivot_longer(\n    data,\n    cols, \n    names_to = \"name\", \n    values_to = \"value\"\n    )\n\n\n\n\nand then we give the argument [names_to] the name of the column (in the transformed longer data) where column names of pivoted variables (in the original wider data) go, which is a character string."
  },
  {
    "objectID": "slides/14-tidyr.html#pivot_longer-3",
    "href": "slides/14-tidyr.html#pivot_longer-3",
    "title": "Tidying Data 🧹",
    "section": "pivot_longer()",
    "text": "pivot_longer()\n\n\n\ndata: data frame\ncols: columns to pivot into longer format\nnames_to: name of the column where column names of pivoted variables go (character string)\nvalues_to: name of the column where data values in pivoted variables go (character string)\n\n\n\npivot_longer(\n    data,\n    cols, \n    names_to = \"name\", \n    values_to = \"value\"\n    )\n\n\n\n\nand finally we give the argument [values_to] the name of the column (in the transformed longer data) where data values of pivoted variables (in the original wider data) go, which is also a character string.\nSo basically we are constructing two new variables in the new longer data set, and we are going to give their name as a character string, so they can be placed in the headers for those columns."
  },
  {
    "objectID": "slides/14-tidyr.html#customers-rightarrow-purchases",
    "href": "slides/14-tidyr.html#customers-rightarrow-purchases",
    "title": "Tidying Data 🧹",
    "section": "customers \\(\\rightarrow\\) purchases",
    "text": "customers \\(\\rightarrow\\) purchases\n\n\n\ncustomers\n\n# A tibble: 2 × 4\n  customer_id item_1 item_2       item_3\n        <dbl> <chr>  <chr>        <chr> \n1           1 bread  milk         banana\n2           2 milk   toilet paper <NA>  \n\n\n\npurchases <- customers |> \n  pivot_longer(\n    # variables item_1 to item_3 \n    # to be pivoted into longer format \n    cols = item_1:item_3,\n    \n    # col name of the names \n    # item_1 item_2 and item_3 \n    names_to = \"item_no\",\n    \n    # col name of the values in the cols\n    # item_1 item_2 and item_3 \n    values_to = \"item\"\n    )\n\n\n\npurchases\n\n# A tibble: 6 × 3\n  customer_id item_no item        \n        <dbl> <chr>   <chr>       \n1           1 item_1  bread       \n2           1 item_2  milk        \n3           1 item_3  banana      \n4           2 item_1  milk        \n5           2 item_2  toilet paper\n6           2 item_3  <NA>        \n\n\nIn customers data,\n\nNames item_1, item_2, item_3 are values of variable item_no in purchases\nValues bread, milk, etc are values of variable item in purchases\n\n\n\n\nLet’s see how the function actually works in the customers example.\nSuppose we start with the customers data set which is in a wider format.\nAnd we wanna pivot it into a longer format.\nthe columns I wanna pivot are the ones that have information about the items that customers bought, which are item1, item 2, and item3.\nSo I write item1:item3 (we can also use any helper function to select columns)\nOK Now, the column names item_1 item_2, item_3 should be in a new column in the transformed longer data set whose column name is called item_no.\nANd the values in those three columns (the cells in the original data) should go into another new column in the in the transformed longer data set whose column name is called item that basically shows what the customers had purchased.\nSo again the column names of the original wider data go into a new column called item_no\nand the values of the column names of the original wider data go into a new column called item.\nAnd I end up with a longer data set called purchases with 6 rows and 3 columns"
  },
  {
    "objectID": "slides/14-tidyr.html#why-pivot",
    "href": "slides/14-tidyr.html#why-pivot",
    "title": "Tidying Data 🧹",
    "section": "Why Pivot?",
    "text": "Why Pivot?\n\nThe next step of your analysis needs it.\nThe new purchases data set and the prices data can now be joined together with the common key variable item. \n\n\n\n\n\nprices <- read_csv(\"./data/sales/prices.csv\")\nprices\n\n# A tibble: 5 × 2\n  item         price\n  <chr>        <dbl>\n1 avocado       0.5 \n2 banana        0.15\n3 bread         1   \n4 milk          0.8 \n5 toilet paper  3   \n\n\n\n\npurchases |> \n    left_join(prices) #<<\n\n# A tibble: 6 × 4\n  customer_id item_no item         price\n        <dbl> <chr>   <chr>        <dbl>\n1           1 item_1  bread         1   \n2           1 item_2  milk          0.8 \n3           1 item_3  banana        0.15\n4           2 item_1  milk          0.8 \n5           2 item_2  toilet paper  3   \n6           2 item_3  <NA>         NA   \n\n\n\n\n\nWhy do we want to pivot our data?\nMost likely, because the next step of your analysis needs it.\nSuppose we have another data set called prices as shown on the left that contains price of grocery items\nAfter pivoting our data, the new purchases data set and the prices data can now be joined together with the common key variable item.\nWith the joined data set, we can further calculate total revenue for example."
  },
  {
    "objectID": "slides/14-tidyr.html#purchases-rightarrow-customers",
    "href": "slides/14-tidyr.html#purchases-rightarrow-customers",
    "title": "Tidying Data 🧹",
    "section": "purchases \\(\\rightarrow\\) customers",
    "text": "purchases \\(\\rightarrow\\) customers\n\n\n\n\ndata: data frame\n\nnames_from: which column variable in the long format contains the what should be column names in the wide format\n\nvalues_from: which column variable in the long format contains the what should be (cell) values in the new columns in the wide format\n\n\n\npurchases\n\n# A tibble: 6 × 3\n  customer_id item_no item        \n        <dbl> <chr>   <chr>       \n1           1 item_1  bread       \n2           1 item_2  milk        \n3           1 item_3  banana      \n4           2 item_1  milk        \n5           2 item_2  toilet paper\n6           2 item_3  <NA>        \n\npurchases |> \n    pivot_wider(              \n        names_from = item_no, \n        values_from = item    \n    ) \n\n# A tibble: 2 × 4\n  customer_id item_1 item_2       item_3\n        <dbl> <chr>  <chr>        <chr> \n1           1 bread  milk         banana\n2           2 milk   toilet paper <NA>  \n\n\n\n\n\nAll right. In fact we can also go back from a longer data to a wider data, which is the original data set we start with.\n\ndata: data frame\n\nnames_from: which column variable in the long format contains what should be column names in the wide format\n\nvalues_from: which column variable in the long format contains what should be (cell) values in the new columns in the wide format\nNotice that item_no and item are not quoted because they are variables that exist in our data frame purchases.\nWe don’t construct new variables as we do when creating a data set in a longer format.\nOK. That’s pivoting, changing data set from wider to longer, or longer to wider format."
  },
  {
    "objectID": "slides/14-tidyr.html#section-2",
    "href": "slides/14-tidyr.html#section-2",
    "title": "Tidying Data 🧹",
    "section": "",
    "text": "Source: FiveThirtyEight"
  },
  {
    "objectID": "slides/14-tidyr.html#section-3",
    "href": "slides/14-tidyr.html#section-3",
    "title": "Tidying Data 🧹",
    "section": "",
    "text": "17-tidyr (Present your work!) \nIn lab.qmd ## Lab 17 section,\n\n\nImport trump.csv. Call it trump_data as below on the left.\nUse pivot_longer() to transform trump_data into the data set trump_longer on the right.\n\n\n\n\n\ntrump_data\n\n# A tibble: 2,702 × 4\n  subgroup date       approval disapproval\n  <chr>    <date>        <dbl>       <dbl>\n1 Voters   2020-10-04     44.7        52.2\n2 Adults   2020-10-04     43.2        52.6\n3 Adults   2020-10-03     43.2        52.6\n4 Voters   2020-10-03     45.0        51.7\n5 Adults   2020-10-02     43.3        52.4\n6 Voters   2020-10-02     44.5        52.1\n# ℹ 2,696 more rows\n\n\n\n\ntrump_longer <- ______________\n    pivot_longer(\n        cols = ____________,\n        names_to = _______________,\n        values_to = _______________\n    ) \n\n\n\n# A tibble: 5,404 × 4\n  subgroup date       rating_type rating_value\n  <chr>    <date>     <chr>              <dbl>\n1 Voters   2020-10-04 approval            44.7\n2 Voters   2020-10-04 disapproval         52.2\n3 Adults   2020-10-04 approval            43.2\n4 Adults   2020-10-04 disapproval         52.6\n5 Adults   2020-10-03 approval            43.2\n6 Adults   2020-10-03 disapproval         52.6\n# ℹ 5,398 more rows"
  },
  {
    "objectID": "slides/14-tidyr.html#section-4",
    "href": "slides/14-tidyr.html#section-4",
    "title": "Tidying Data 🧹",
    "section": "",
    "text": "BONUS 💳: Use trump_longer to generate a plot like the one below."
  },
  {
    "objectID": "slides/14-tidyr.html#pd.wide_to_long",
    "href": "slides/14-tidyr.html#pd.wide_to_long",
    "title": "Tidying Data 🧹",
    "section": "pd.wide_to_long()",
    "text": "pd.wide_to_long()\n\nimport numpy as np\nimport pandas as pd\n\n\ncustomers = pd.read_csv('./data/sales/customers.csv')\npurchases = pd.wide_to_long(df = customers,\n                            stubnames = ['item'], \n                            i = 'customer_id', \n                            j = 'item_no', sep = '_')\npurchases\n\n                             item\ncustomer_id item_no              \n1           1               bread\n2           1                milk\n1           2                milk\n2           2        toilet paper\n1           3              banana\n2           3                 NaN"
  },
  {
    "objectID": "slides/14-tidyr.html#pd.pivot",
    "href": "slides/14-tidyr.html#pd.pivot",
    "title": "Tidying Data 🧹",
    "section": "pd.pivot()",
    "text": "pd.pivot()\n\npurchases = purchases.reset_index()\npurchases\n\n   customer_id  item_no          item\n0            1        1         bread\n1            2        1          milk\n2            1        2          milk\n3            2        2  toilet paper\n4            1        3        banana\n5            2        3           NaN\n\npurchases.pivot(index = \"customer_id\", columns = \"item_no\", values = \"item\")\n\nitem_no          1             2       3\ncustomer_id                             \n1            bread          milk  banana\n2             milk  toilet paper     NaN\n\n\n\n\n\nmath3570-s24.github.io/website"
  },
  {
    "objectID": "exercise/lab11-ggplot2.html",
    "href": "exercise/lab11-ggplot2.html",
    "title": "Lab 11: ggplot2",
    "section": "",
    "text": "Note\n\n\n\n\n\n\nIn lab.qmd ## Lab 11 section,\n\nUse readr::read_csv() to import the data penguins.csv into your R workspace.\nGenerate the following ggplot:\n\n\n\n\n\n\n\n\n\n\n\npenguins <- read_csv(_________________)\n________ |> \n  ggplot(mapping = ____(x = ______________,\n                        y = ______________,\n                        colour = ________)) +\n  geom______() +\n  ____(title = ____________________,\n       _________ = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = _____________, y = _______________,\n       _______ = \"Species\",\n       _______ = \"Source: Palmer Station LTER / palmerpenguins package\")"
  },
  {
    "objectID": "exercise/lab13-visualization.html",
    "href": "exercise/lab13-visualization.html",
    "title": "Lab 13: Visualization",
    "section": "",
    "text": "Note\n\n\n\n\n\n\nIn lab.qmd ## Lab 13 section,\n\nImport the data penguins.csv.\nGenerate the following\n\n\n\n\n\n\n\n\n\n\n\n\nWarning: Removed 2 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n# library(tidyverse)\npenguins <- read_csv(__________________)\n________ |> ggplot(_______________________) +  ## mapping layer  \n    ___________________ +  ## geometry layer\n    _____________________________  ## label layer\n\n  \n\n________ |> ggplot(______________________________) +  ## mapping layer  \n    _______________ +  ## geometry layer\n    _______________ +  ## label layer\n    ______________________________  +   ## facet layer\n    ______________________________      ## theme layer (set legend.position = \"none\")"
  },
  {
    "objectID": "exercise/lab18-prob.html",
    "href": "exercise/lab18-prob.html",
    "title": "Lab 18: Probability",
    "section": "",
    "text": "Note\n\n\n\n\n\n\nIn lab.qmd ## Lab 18 section,\n\nPlot the probability function \\(P(X = x)\\) of \\(X \\sim \\text{binomial}(n = 5, \\pi = 0.3)\\).\n\nTo use ggplot,\n\nCreate a data frame saving all possible values of \\(x\\) and their corresponding probability using dbinom(x, size = ___, prob = ___).\n\n\n\n# A tibble: 6 × 2\n      x       y\n  <int>   <dbl>\n1     0 0.168  \n2     1 0.360  \n3     2 0.309  \n4     3 0.132  \n5     4 0.0284 \n6     5 0.00243\n\n\n\nAdd geom_col()\n\n\n\n\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "exercise/lab12-facet.html",
    "href": "exercise/lab12-facet.html",
    "title": "Lab 12: Faceting",
    "section": "",
    "text": "Note\n\n\n\n\n\n\nIn lab.qmd ## Lab 12 section,\n\nggplot(data = _______, \n       mapping = aes(x = ______, y = ______, ______ = drv, shape = _____)) +\n    geom______(______ = 3, ______ = 0.8) + \n    facet_grid(______ ~ _______) +\n    guides(______ = \"none\")"
  },
  {
    "objectID": "exercise/lab05-rtype.html",
    "href": "exercise/lab05-rtype.html",
    "title": "Lab 05: R Data Type Summary",
    "section": "",
    "text": "Note\n\n\n\n\n\n\nIn lab.qmd Lab 5,\n\nCreate R objects vector v1, factor f2, list l3, matrix m4 and data frame d5.\nCheck typeof() and class() of those objects, and create a list having the output below.\n\n\nv1 <- __________\nf2 <- __________\nl3 <- __________\nm4 <- __________\nd5 <- __________\nv <- c(type = typeof(v1), class = class(v1))\nf <- c(type = __________, class = _________)\nl <- c(type = __________, class = _________)\nm <- c(type = __________, class = _________)\nd <- c(type = __________, class = _________)\n____(vec    =   v,\n     ______ = ___,\n     ______ = ___,\n     ______ = ___,\n     ______ = ___)\n\n\n\n$vec\n     type     class \n \"double\" \"numeric\" \n\n$fac\n     type     class \n\"integer\"  \"factor\" \n\n$lst\n  type  class \n\"list\" \"list\" \n\n$mat\n     type    class1    class2 \n\"integer\"  \"matrix\"   \"array\" \n\n$df\n        type        class \n      \"list\" \"data.frame\""
  },
  {
    "objectID": "exercise/lab06-py.html",
    "href": "exercise/lab06-py.html",
    "title": "Lab 06: Python Data Structure",
    "section": "",
    "text": "Note\n\n\n\n\n\n\nIn lab.qmd Lab 6,\n\nCreate a Python list and dictionary similar to the R list below.\n\n\nx_lst <- list(idx = 1:3, \n              \"a\", \n              c(TRUE, FALSE))\n\nRemember to create Python code chunk\n\n```{Python}\n#| echo: true\n#| eval: false\n\n```\n\nCommit and Push your work once you are done."
  },
  {
    "objectID": "exercise/lab22-knn.html",
    "href": "exercise/lab22-knn.html",
    "title": "Lab 22: K Nearest Neighbors",
    "section": "",
    "text": "Note\n\n\n\n\n\n\nIn lab.qmd ## Lab 22 section,\n\nuse HEIGHT, WAIST and BMI to predict GENDER using KNN with \\(K = 3\\).\nGenerate the (test) confusion matrix.\nCalculate (test) accuracy rate.\nDoes using more predictors predict better?\n\n\nlibrary(tidymodels)\n## load data\nbodydata <- read_csv(\"./data/body.csv\")\nbody <- bodydata |> \n    select(GENDER, HEIGHT, WAIST, BMI) |> \n    mutate(GENDER = as.factor(GENDER))\n\n## training and test data\nset.seed(2024)\ndf_split <- rsample::initial_split(data = body, prop = 0.8)\ndf_trn <- rsample::training(df_split)\ndf_tst <- rsample::testing(df_split)\n\n\nR Code\n\n## KNN training\nknn_recipe <- recipes::recipe(GENDER ~ HEIGHT + WAIST + BMI, data = df_trn) |> \n    step_normalize(all_predictors())\n\nknn_mdl <- parsnip::nearest_neighbor(neighbors = 3) |> \n    set_mode(\"classification\") |>\n    set_engine(\"kknn\")\n\nknn_out <- workflows::workflow() |> \n    add_recipe(knn_recipe) |> \n    add_model(knn_mdl) |> \n    fit(data = df_trn)\n\n## KNN prediction\nknn_pred <- pull(predict(knn_out, df_tst))\ntable(knn_pred, df_tst$GENDER)\nmean(knn_pred == df_tst$GENDER)\n\n\n\nPython Code\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\n## load data\nbody = pd.read_csv('./data/body.csv')\n\nX = body[['HEIGHT', 'WAIST', 'BMI']]\ny = body['GENDER']\nX_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.2, random_state=2024)\n\n## KNN training\nneigh = KNeighborsClassifier(n_neighbors = 3)\nX_trn = np.array(X_trn)\nX_tst = np.array(X_tst)\nneigh.fit(X_trn, y_trn)\n\n## KNN prediction\ny_pred = neigh.predict(X_tst)\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_tst, y_pred)"
  },
  {
    "objectID": "exercise/lab24-kmeans.html",
    "href": "exercise/lab24-kmeans.html",
    "title": "Lab 24: K-Means Clustering",
    "section": "",
    "text": "Note\n\n\n\n\n\n\nIn lab.qmd ## Lab 24 section,\n\nInstall R package palmerpenguins at https://allisonhorst.github.io/palmerpenguins/\nPerform K-Means to with \\(K = 3\\) to cluster penguins based on bill_length_mm and flipper_length_mm of data peng.\n\n\nlibrary(palmerpenguins)\npeng <- penguins[complete.cases(penguins), ] |> \n    select(flipper_length_mm, bill_length_mm)\n\n\n\n\n\n\n\n\n\n\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "exercise/lab01-rscript.html",
    "href": "exercise/lab01-rscript.html",
    "title": "Lab-01: Running R Script",
    "section": "",
    "text": "Note\n\n\n\n\n\n\n\nLoad R package ggplot2 into your Posit Cloud.\n\n\n## install the package if you haven't!\n________(ggplot2)\n\n\nCreate a R script named lab01-run-script.R in your 3570-project.\nCopy and paste the code below into the script, and save it.\n\n\nbar <- ggplot(data = diamonds) + \n  geom_bar(mapping = aes(x = cut, fill = cut), \n           show.legend = FALSE, width = 1) + \n  theme(aspect.ratio = 1) +\n  labs(x = NULL, y = NULL)\nbar + coord_flip()\nbar + coord_polar()\n\n\nSource the script. A pretty plot showing up?!"
  },
  {
    "objectID": "exercise/lab16-join.html",
    "href": "exercise/lab16-join.html",
    "title": "Lab 16: Joining tables",
    "section": "",
    "text": "Note\n\n\n\n\n\n\nIn lab.qmd ## Lab 16 section\n\nImport the data at https://www.jaredlander.com/data/DiamondColors.csv. Call it diamond_color.\n\n\ndiamond_color <- readr::read_csv(\"the url\")\n\n\nUse left_join() to combine the data set diamonds in ggplot2 and diamond_color by the key variable color.\n\n\n\nSelect the variables carat, color, Description, Details.\n\n\n## Variable \"color\" in diamonds but \"Color\" in diamond_color\n\njoined_df <- diamonds |>  \n    _______(_______, by = c('color' = 'Color')) |>  ## join\n    _______(_________________________________________)  ## select\n\n\nCreate a bar chart of the variable color.\n\n\n\n# A tibble: 53,940 × 4\n   carat color Description    Details                    \n   <dbl> <chr> <chr>          <chr>                      \n 1  0.23 E     Colorless      Minute traces of color     \n 2  0.21 E     Colorless      Minute traces of color     \n 3  0.23 E     Colorless      Minute traces of color     \n 4  0.29 I     Near Colorless Slightly detectable color  \n 5  0.31 J     Near Colorless Slightly detectable color  \n 6  0.24 J     Near Colorless Slightly detectable color  \n 7  0.24 I     Near Colorless Slightly detectable color  \n 8  0.26 H     Near Colorless Color is dificult to detect\n 9  0.22 E     Colorless      Minute traces of color     \n10  0.23 H     Near Colorless Color is dificult to detect\n# ℹ 53,930 more rows"
  },
  {
    "objectID": "exercise/lab14-interactive.html",
    "href": "exercise/lab14-interactive.html",
    "title": "Lab 14: plotly (Presentation)",
    "section": "",
    "text": "Note\n\n\n\n\n\n\nIn lab.qmd ## Lab 14 section,\n\nLoad tidyverse and plotly and the loans.csv data.\nGenerate a plot using plotly. An example is shown below. Welcome to create a more fancy one!\n\n\n\n\n\n\nloans <- read_csv(\"../slides/data/loans.csv\")\np <- plot_ly(loans, x = ~interest_rate, alpha = 0.5)\np |> add_boxplot(y = ~grade, color = ~grade)\n\n\n\n\n\n:::"
  },
  {
    "objectID": "exercise/lab20-slr.html",
    "href": "exercise/lab20-slr.html",
    "title": "Lab 20: Simple Linear Regression",
    "section": "",
    "text": "Note\n\n\n\n\n\n\nIn lab.qmd ## Lab 20 section,\n\nUse the mpg data to fit a simple linear regression where \\(y\\) is hwy and \\(x\\) is cty.\nProduce the plot below. (add the layer geom_smooth(method = \"lm\", se = FALSE))"
  },
  {
    "objectID": "exercise/lab09-numpy.html",
    "href": "exercise/lab09-numpy.html",
    "title": "Lab 09: NumPy and pandas",
    "section": "",
    "text": "Note\n\n\n\n\n\n\nIn lab.qmd ## Lab 9 section, create a Python pandas.DataFrame equivalent to the R tibble\n\ntibble(x = 1:5, y = 5:1, z = LETTERS[1:5])\n\n# A tibble: 5 × 3\n      x     y z    \n  <int> <int> <chr>\n1     1     5 A    \n2     2     4 B    \n3     3     3 C    \n4     4     2 D    \n5     5     1 E    \n\n\n\nimport numpy as np\nimport pandas as pd\nimport string\nlist(string.ascii_uppercase)\ndic = {'__': ____________, \n       '__': reversed(____________),\n       '__': list(string.ascii_uppercase)[______]}\npd._____________(dic)"
  },
  {
    "objectID": "exercise/lab21-logistic.html",
    "href": "exercise/lab21-logistic.html",
    "title": "Lab 21: Logistic Regression",
    "section": "",
    "text": "Note\n\n\n\n\n\n\nIn lab.qmd ## Lab 21 section,\n\nUse our fitted logistic regression model to predict whether you are male or female! Change 175 to your height (cm).\nUse the converter to get your height in cm!\n\n\n# Fit the logistic regression\n\npredict(logis_out$fit, newdata = data.frame(HEIGHT = 175), \n        type = \"response\")"
  },
  {
    "objectID": "exercise/lab08-tibble.html",
    "href": "exercise/lab08-tibble.html",
    "title": "Lab 08: Tibbles and Pipes",
    "section": "",
    "text": "Note\n\n\n\n\n\n\nIn lab.qmd ## Lab 8 section,\n\nCompare and contrast the following operations on a data.frame and equivalent tibble. What are the differences?\n\n\ndf <- data.frame(abc = 1:2, \n                 xyz = c(\"a\", \"b\"))\n# list method\ndf$x\ndf[[2]]\ndf[\"xyz\"]\ndf[c(\"abc\", \"xyz\")]\n# matrix method\ndf[, 2]\ndf[, \"xyz\"]\ndf[, c(\"abc\", \"xyz\")]\n\n\ntib <- tibble(abc = 1:2, \n              xyz = c(\"a\", \"b\"))\n# list method\ntib$x\ntib[[2]]\ntib[\"xyz\"]\ntib[c(\"abc\", \"xyz\")]\n# matrix method\ntib[, 2]\ntib[, \"xyz\"]\ntib[, c(\"abc\", \"xyz\")]\n\n\nUse |> to first select last 12 rows of iris data set using tail(), then provides summary statistics on its columns using summary()."
  },
  {
    "objectID": "exercise/lab23-pca.html",
    "href": "exercise/lab23-pca.html",
    "title": "Lab 23: Principal Component Analysis",
    "section": "",
    "text": "Note\n\n\n\n\n\n\nIn lab.qmd ## Lab 23 section,\n\nUse slice() to print the first six rows of iris data.\nPerform PCA on Sepal.Length, Sepal.Width, Petal.Length, and Petal.Width.\nGenerate biplot, and explain it.\n\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa"
  },
  {
    "objectID": "exercise/lab03-markdown.html",
    "href": "exercise/lab03-markdown.html",
    "title": "Lab 03: Markdown",
    "section": "",
    "text": "Note\n\n\n\n\n\n\n\nBack to your lab.qmd. In ## Lab 3: Markdown section, add a self-introduction paragraph containing a header, bold and italic text.\nAdd another paragraph that contains\n\nlisted items\na hyperlink\na blockquote\nmath expression\n\nOnce done, commit with message “03-Markdown” and push your updated work to GitHub."
  },
  {
    "objectID": "exercise/lab04-chunk.html",
    "href": "exercise/lab04-chunk.html",
    "title": "Lab 04: Code Chunk",
    "section": "",
    "text": "Note\n\n\n\n\n\n\n\nIn lab.qmd ## Lab 4: Code Chunk, use code chunks to\n\ninclude an image with knitr::include_graphics(\"URL or file path\") https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/ggplot2.png\ninclude a plot plot(mtcars$disp, mtcars$mpg)\nShow dataset mtcars as a table using knitr::kable()\ndo some inline code calculation like `r ncol(mtcars)`, `r log(100, base = 10) + sqrt(4)`.\n\nAdd option fig-height: 4, fig-width: 6 and fig-align: right to your plot. What are the changes?\nHow do we set global chunk options to hide and run code in every chunk?\nOnce done, commit with message “04-Code Chunk” and push your work to GitHub."
  },
  {
    "objectID": "exercise/lab19-ci.html",
    "href": "exercise/lab19-ci.html",
    "title": "Lab 19: Confidence Interval",
    "section": "",
    "text": "Note\n\n\n\n\n\n\nIn lab.qmd ## Lab 19 section,\n\nRun the code I give you for simulating 100 \\(95\\%\\) CIs. Change the random generator seed to another number you like.\n\n\nset.seed(a number you like) Birthday? Lucky number?\n\n\nHow many CIs do not cover the true mean \\(\\mu\\)?\n\n\n\n\n\n\n\n\n\n\n```"
  },
  {
    "objectID": "exercise/lab17-tidyr.html",
    "href": "exercise/lab17-tidyr.html",
    "title": "Lab 17: tidyr",
    "section": "",
    "text": "Note\n\n\n\n\n\n\nIn lab.qmd ## Lab 17 section,\n\n\nImport trump.csv. Call it trump_data as below on the left.\nUse pivot_longer() to transform trump_data into the data set trump_longer on the right.\n\n\n\n\n\ntrump_data\n\n# A tibble: 2,702 × 4\n   subgroup date       approval disapproval\n   <chr>    <date>        <dbl>       <dbl>\n 1 Voters   2020-10-04     44.7        52.2\n 2 Adults   2020-10-04     43.2        52.6\n 3 Adults   2020-10-03     43.2        52.6\n 4 Voters   2020-10-03     45.0        51.7\n 5 Adults   2020-10-02     43.3        52.4\n 6 Voters   2020-10-02     44.5        52.1\n 7 Voters   2020-10-01     44.1        52.8\n 8 Adults   2020-10-01     42.7        53.3\n 9 Adults   2020-09-30     42.2        53.7\n10 Voters   2020-09-30     44.2        52.7\n# ℹ 2,692 more rows\n\n\n\ntrump_longer <- ______________\n    pivot_longer(\n        cols = ____________,\n        names_to = _______________,\n        values_to = _______________\n    ) \n\n\n\n# A tibble: 5,404 × 4\n   subgroup date       rating_type rating_value\n   <chr>    <date>     <chr>              <dbl>\n 1 Voters   2020-10-04 approval            44.7\n 2 Voters   2020-10-04 disapproval         52.2\n 3 Adults   2020-10-04 approval            43.2\n 4 Adults   2020-10-04 disapproval         52.6\n 5 Adults   2020-10-03 approval            43.2\n 6 Adults   2020-10-03 disapproval         52.6\n 7 Voters   2020-10-03 approval            45.0\n 8 Voters   2020-10-03 disapproval         51.7\n 9 Adults   2020-10-02 approval            43.3\n10 Adults   2020-10-02 disapproval         52.4\n# ℹ 5,394 more rows\n\n\nBONUS 💵: Use trump_longer to generate a plot like the one below."
  },
  {
    "objectID": "exercise/lab02-quarto.html",
    "href": "exercise/lab02-quarto.html",
    "title": "Lab 02: Quarto File",
    "section": "",
    "text": "Note\n\n\n\n\n\n\n\nGo to your GitHub repo lab-yourusername. Clone it to your Posit Cloud as a project in 2024-Spring-Math-3570 workspace.\nOpen the file lab.qmd.\nChange author in YAML.\nClick on  or Ctrl/Cmd + Shift + K to produce a HTML document.\nHow can we show the current date every time we compile the file? [Hint:] Check your hw00. Compile your document and make sure the date shows up.\nHow do we produce a pdf? Describe it in ## Lab 2: Quarto\nOnce done, commit with message “02-Quarto File” and push your version to GitHub."
  },
  {
    "objectID": "exercise/lab10-import.html",
    "href": "exercise/lab10-import.html",
    "title": "Lab 10: Import Data",
    "section": "",
    "text": "Note\n\n\n\n\n\n\n\nIf you haven’t, install and load the tidyverse package.\n\nIn lab.qmd ## Lab 10 section,        \n\nImport ssa_male_prob.csv and ssa_female_prob.Rds in the data folder using read_csv() and call them ssa_male and ssa_female, respectively.\n\n\nssa_male <- readr::read____(____________)\nssa_female <- readr::read____(____________)\n\n\nPlot Age (x-axis) vs. LifeExp (y-axis) for Female. The type should be “line”, and the line color is red. Add x-label, y-label and title to your plot.\n\n\nplot(x = _____, y = _____, type = ______, col = ______,\n     xlab = ______, ylab = _______, main = ____________)\n\n\nUse lines() to add a line of Age (x-axis) vs. LifeExp (y-axis) for Male to the plot. The color is blue.\n\n\nlines(x = _____, y = _____, col = ______)"
  },
  {
    "objectID": "exercise/lab00-git.html#connect-posit-cloud-and-github-step-1",
    "href": "exercise/lab00-git.html#connect-posit-cloud-and-github-step-1",
    "title": "Lab Exercise: Git/GitHub",
    "section": "Connect Posit Cloud and GitHub: Step 1",
    "text": "Connect Posit Cloud and GitHub: Step 1\n\nPosit Cloud cannot recognize your GitHub account unless you connect them each other.\nIn Posit Cloud, click on your name on the top-right corner to open the right menu.\nClick on Authentication."
  },
  {
    "objectID": "exercise/lab00-git.html#connect-posit-cloud-and-github-step-2",
    "href": "exercise/lab00-git.html#connect-posit-cloud-and-github-step-2",
    "title": "Lab Exercise: Git/GitHub",
    "section": "Connect Posit Cloud and GitHub: Step 2",
    "text": "Connect Posit Cloud and GitHub: Step 2\n\nIn the Authentication window, check the box for Enabled.\n\n\n\n\n\n\n\n\n\n\nWhen check Enabled, will jump to GitHub page shown in the next."
  },
  {
    "objectID": "exercise/lab00-git.html#connect-posit-cloud-and-github-step-3",
    "href": "exercise/lab00-git.html#connect-posit-cloud-and-github-step-3",
    "title": "Lab Exercise: Git/GitHub",
    "section": "Connect Posit Cloud and GitHub: Step 3",
    "text": "Connect Posit Cloud and GitHub: Step 3\n\nFor your GitHub page, click on the green box that says “Authorize rstudio”."
  },
  {
    "objectID": "exercise/lab00-git.html#connect-posit-cloud-and-github-step-4",
    "href": "exercise/lab00-git.html#connect-posit-cloud-and-github-step-4",
    "title": "Lab Exercise: Git/GitHub",
    "section": "Connect Posit Cloud and GitHub: Step 4",
    "text": "Connect Posit Cloud and GitHub: Step 4\n\nBack to the Authentication of Posit Cloud, check Private repo access also enabled.\nMake sure math3570-s24 shows up under Organization access.\nClick on Request\nClick on the green box “Authorize rstudio”.\n\n\n\n\n\n\n\n\n\n\nWhen check Private repo access also enabled, will jump to GitHub page as shown."
  },
  {
    "objectID": "exercise/lab00-git.html#connect-posit-cloud-and-github-step-5",
    "href": "exercise/lab00-git.html#connect-posit-cloud-and-github-step-5",
    "title": "Lab Exercise: Git/GitHub",
    "section": "Connect Posit Cloud and GitHub: Step 5",
    "text": "Connect Posit Cloud and GitHub: Step 5\n\nOnce you’re done, both of these boxes should be checked."
  },
  {
    "objectID": "exercise/lab00-git.html#connect-posit-cloud-and-github-step-5-1",
    "href": "exercise/lab00-git.html#connect-posit-cloud-and-github-step-5-1",
    "title": "Lab Exercise: Git/GitHub",
    "section": "Connect Posit Cloud and GitHub: Step 5-1",
    "text": "Connect Posit Cloud and GitHub: Step 5-1"
  },
  {
    "objectID": "exercise/lab00-git.html#connect-posit-cloud-and-github-step-6",
    "href": "exercise/lab00-git.html#connect-posit-cloud-and-github-step-6",
    "title": "Lab Exercise: Git/GitHub",
    "section": "Connect Posit Cloud and GitHub: Step 6",
    "text": "Connect Posit Cloud and GitHub: Step 6\n\nConfirm that you’ve linked up your GitHub and Posit Cloud accounts GitHub settings > Applications. Should see Posit Cloud listed as an authorized app under Authorized OAuth Apps.\n\n\n\n\n\n\n\n\n\n\nIf you see RStudio is under the Authorized Apps, congratulations! Your RStudio and GitHub are now linked together!"
  },
  {
    "objectID": "exercise/lab00-git.html#github-to-posit-cloud-step-1",
    "href": "exercise/lab00-git.html#github-to-posit-cloud-step-1",
    "title": "Lab Exercise: Git/GitHub",
    "section": "GitHub to Posit Cloud: Step 1",
    "text": "GitHub to Posit Cloud: Step 1\n\nEach of your assignments will begin with the following steps.\nGo to the repo named hw00-yourusername I created for you."
  },
  {
    "objectID": "exercise/lab00-git.html#github-to-posit-cloud-step-2",
    "href": "exercise/lab00-git.html#github-to-posit-cloud-step-2",
    "title": "Lab Exercise: Git/GitHub",
    "section": "GitHub to Posit Cloud: Step 2",
    "text": "GitHub to Posit Cloud: Step 2\n\nOn GitHub,\n\nclick on the green Code button, select HTTPS.\nclick on the clipboard icon on the right to copy the repo URL, such as https://github.com/math3570-s24/hw00-chenghanyu.git"
  },
  {
    "objectID": "exercise/lab00-git.html#github-to-posit-cloud-step-3",
    "href": "exercise/lab00-git.html#github-to-posit-cloud-step-3",
    "title": "Lab Exercise: Git/GitHub",
    "section": "GitHub to Posit Cloud: Step 3",
    "text": "GitHub to Posit Cloud: Step 3\n\nGo to Posit Cloud and into the course workspace 2024-spring-math-3570.\nCreate a New Project from Git Repo.\n\n\n\n\n\n\n\n\n\n\nYou will need to click on the down arrow next to the New Project button to see this option."
  },
  {
    "objectID": "exercise/lab00-git.html#github-to-posit-cloud-step-4",
    "href": "exercise/lab00-git.html#github-to-posit-cloud-step-4",
    "title": "Lab Exercise: Git/GitHub",
    "section": "GitHub to Posit Cloud: Step 4",
    "text": "GitHub to Posit Cloud: Step 4\n\nCopy and paste the URL of your assignment repo into the dialog box.\nHit OK, and you’re good to go!"
  },
  {
    "objectID": "exercise/lab00-git.html#github-to-posit-cloud-step-5",
    "href": "exercise/lab00-git.html#github-to-posit-cloud-step-5",
    "title": "Lab Exercise: Git/GitHub",
    "section": "GitHub to Posit Cloud: Step 5",
    "text": "GitHub to Posit Cloud: Step 5\n\nClick hw00-yourusername to do your assignment in Posit Cloud!\n\n\n\n\n\n\n\n\n\n\n\nDone! We learned the entire process of cloning a repo on GitHub to Posit Cloud as a project.\nNext, we’ll see how to keep your revision record (commit) and send (push) the latest revised version of your work from Posit Cloud to GitHub!"
  },
  {
    "objectID": "exercise/lab00-git.html#personal-access-token-pat-step-1",
    "href": "exercise/lab00-git.html#personal-access-token-pat-step-1",
    "title": "Lab Exercise: Git/GitHub",
    "section": "Personal Access Token (PAT): Step 1",
    "text": "Personal Access Token (PAT): Step 1\n\nGitHub has removed the support for Password Authentication for Git operations.\nBefore we can send our work in Posit Cloud to GitHub, we need Personal Access Token (PAT)\nSettings > Developer settings\n\n\n\n\n\n\n\n\n\n\nGitHub has removed the support for Password Authentication for Git operations for more safety from 08/13/2021."
  },
  {
    "objectID": "exercise/lab00-git.html#personal-access-token-pat-step-2",
    "href": "exercise/lab00-git.html#personal-access-token-pat-step-2",
    "title": "Lab Exercise: Git/GitHub",
    "section": "Personal Access Token (PAT): Step 2",
    "text": "Personal Access Token (PAT): Step 2"
  },
  {
    "objectID": "exercise/lab00-git.html#personal-access-token-pat-step-3",
    "href": "exercise/lab00-git.html#personal-access-token-pat-step-3",
    "title": "Lab Exercise: Git/GitHub",
    "section": "Personal Access Token (PAT): Step 3",
    "text": "Personal Access Token (PAT): Step 3"
  },
  {
    "objectID": "exercise/lab00-git.html#personal-access-token-pat-step-4",
    "href": "exercise/lab00-git.html#personal-access-token-pat-step-4",
    "title": "Lab Exercise: Git/GitHub",
    "section": "Personal Access Token (PAT): Step 4",
    "text": "Personal Access Token (PAT): Step 4"
  },
  {
    "objectID": "exercise/lab00-git.html#personal-access-token-pat-step-5",
    "href": "exercise/lab00-git.html#personal-access-token-pat-step-5",
    "title": "Lab Exercise: Git/GitHub",
    "section": "Personal Access Token (PAT): Step 5",
    "text": "Personal Access Token (PAT): Step 5\n\nCopy and paste your PAT to a secrete and safe space!!"
  },
  {
    "objectID": "exercise/lab00-git.html#posit-cloud-to-github-step-1---edit-your-file",
    "href": "exercise/lab00-git.html#posit-cloud-to-github-step-1---edit-your-file",
    "title": "Lab Exercise: Git/GitHub",
    "section": "Posit Cloud to GitHub: Step 1 - Edit your file",
    "text": "Posit Cloud to GitHub: Step 1 - Edit your file\n\nOpen a Quarto (qmd) file in your project, in YAML change the author name to your name.\nClick Render to generate your beautiful document. (If you are asked to install any packages, please do!)"
  },
  {
    "objectID": "exercise/lab00-git.html#posit-cloud-to-github-step-2---commit-changes",
    "href": "exercise/lab00-git.html#posit-cloud-to-github-step-2---commit-changes",
    "title": "Lab Exercise: Git/GitHub",
    "section": "Posit Cloud to GitHub: Step 2 - Commit changes",
    "text": "Posit Cloud to GitHub: Step 2 - Commit changes\n\nGo to the Git tab in your RStudio.\nClick on Diff. This shows you the difference between the last committed state of the document and its current state that includes your changes.\nCheck Staged box to add files to be committed.\nWrite “Update author’s name” in the Commit message box and hit Commit."
  },
  {
    "objectID": "exercise/lab00-git.html#posit-cloud-to-github-step-3---push-changes",
    "href": "exercise/lab00-git.html#posit-cloud-to-github-step-3---push-changes",
    "title": "Lab Exercise: Git/GitHub",
    "section": "Posit Cloud to GitHub: Step 3 - Push changes",
    "text": "Posit Cloud to GitHub: Step 3 - Push changes\n\nWe’ve made an update and committed this change locally.\nIt’s time to push the changes to your repo on GitHub, so that others (Dr. Yu) can see your changes.\nClick on Push.\nIn the prompted dialogue box, enter your GitHub user name, and your password (PAT)."
  },
  {
    "objectID": "exercise/lab00-git.html#posit-cloud-to-github-step-3---updated-repo",
    "href": "exercise/lab00-git.html#posit-cloud-to-github-step-3---updated-repo",
    "title": "Lab Exercise: Git/GitHub",
    "section": "Posit Cloud to GitHub: Step 3 - Updated Repo",
    "text": "Posit Cloud to GitHub: Step 3 - Updated Repo\n\nBack to your GitHub repo and refresh it.\nThe online repo is now synced with your local project in Posit Cloud."
  },
  {
    "objectID": "exercise/lab00-posit.html#install-posit-cloud",
    "href": "exercise/lab00-posit.html#install-posit-cloud",
    "title": "Lab Exercise: Posit Cloud",
    "section": "Install Posit Cloud",
    "text": "Install Posit Cloud\n\nStep 1: In the Posit website https://posit.co/, choose Products > Posit Cloud as shown below.\n\n\n\n\n\n\n\n\n\n\n\nStep 2: Click GET STARTED.\nStep 3: Free or Student > Sign Up. Please sign up with GitHub if you have one or use your Marquette email address.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew project"
  },
  {
    "objectID": "exercise/lab00-posit.html#welcome-to-3570-data-science",
    "href": "exercise/lab00-posit.html#welcome-to-3570-data-science",
    "title": "Lab Exercise: Posit Cloud",
    "section": "Welcome to 3570 Data Science!",
    "text": "Welcome to 3570 Data Science!\n\nI’m sending you a link via email for joining the course workspace 2024-spring-math-3570. Please join.\n\n\n\n\n\n\n\n\n\n\n\nIn the bar, click workspace 2024-spring-math-3570.\nClick New Project > New RStudio Project to get into the IDE.\nIn Untitled Project, name your project as 3570-project.\nIn the Console pane, write R code: a string \"Hello WoRld!\" or math 2 + 4.\nTools > Global Options > Appearance to select your favorite editor theme."
  },
  {
    "objectID": "exercise/lab15-dplyr.html",
    "href": "exercise/lab15-dplyr.html",
    "title": "Lab 15: dplyr",
    "section": "",
    "text": "Note\n\n\n\n\n\n\nIn lab.qmd ## Lab 15 section, import the murders.csv data and\n\nAdd (mutate) the variable rate = total / population * 100000 to murders data (as I did).\nFilter states that are in region Northeast or West and their murder rate is less than 1.\nSelect variables state, region, rate.\n\n\nPrint the output table after you do 1. to 3., and save it as object my_states.\nGroup my_states by region. Then summarize data by creating variables avg and stdev that compute the mean and standard deviation of rate.\nArrange the summarized table by avg.\n\n\n_______ <- _______ |> \n    mutate(_______) |> \n    filter(_______) |> \n    select(_______)\n\n_______ |>  \n    group_by(______) |> \n    summarize(______) |> \n    arrange(_______)\n\n\n\n          state    region      rate\n1        Hawaii      West 0.5145920\n2         Idaho      West 0.7655102\n3         Maine Northeast 0.8280881\n4 New Hampshire Northeast 0.3798036\n5        Oregon      West 0.9396843\n6          Utah      West 0.7959810\n7       Vermont Northeast 0.3196211\n8       Wyoming      West 0.8871131\n\n\n# A tibble: 2 × 3\n  region      avg std_dev\n  <fct>     <dbl>   <dbl>\n1 West      0.781   0.164\n2 Northeast 0.509   0.278"
  },
  {
    "objectID": "exercise/lab07-plot.html",
    "href": "exercise/lab07-plot.html",
    "title": "Lab 07: Plotting (Presentation)",
    "section": "",
    "text": "Note\n\n\n\n\nFind your mate and work in pairs. \nTwo volunteer pairs teach us how to make beautiful plots next Tuesday (Feb 13)!\nThe presenters will be awarded a hex sticker! 😎\n\n\n\nIn lab.qmd ## Lab 7,\n\nFor the mtcars data, use R or Python to\n\nmake a scatter plot of miles per gallon vs. weight. Decorate your plot using arguments, col, pch, xlab, etc.\ncreate a histogram of 1/4 mile time. Make it beautiful!\n\nCommit and Push your work once you are done."
  },
  {
    "objectID": "project-description.html",
    "href": "project-description.html",
    "title": "MATH/COSC 3570 Project Description",
    "section": "",
    "text": "Team up! You will be working as a group of 4 (one team with 5). One of you, please email me\n\nyour group member list\nyour team name (Last year we have team names “ggsquard”, “Data Alpha”, “Analytics and Beyond”, “YU”, “Metadata”)\n\nby Friday, 4/5 11:59 PM.\nProposal. Please send me a one-page PDF describing what you are going to do for your project (no word limit) with your project title by Friday, 4/26 11:59 PM.\nMeeting. Schedule a group meeting with Dr. Yu discussing your project. Please book a time slot in the Excel form.\nPresentation. You will be presenting your project on Monday, 5/6 10:30 AM - 12:30 PM.\nMaterials. Please share your entire work (slides, code, data, etc) by Monday, 5/6 11:59 PM."
  },
  {
    "objectID": "project-description.html#team-up",
    "href": "project-description.html#team-up",
    "title": "MATH/COSC 3570 Project Description",
    "section": "Team up!",
    "text": "Team up!\n\nEach one of you loses 3 points of your project grade if you don’t meet the requirement or miss the deadline.\nYou will be randomly assigned to a group if you do not belong to any group before the deadline."
  },
  {
    "objectID": "project-description.html#proposal",
    "href": "project-description.html#proposal",
    "title": "MATH/COSC 3570 Project Description",
    "section": "Proposal",
    "text": "Proposal\n\nEach one of you loses 3 points of your project grade if you don’t meet the requirement or miss the deadline.\nAlthough it is risky, you can change your project topic after you submit your proposal if you decide to do something else."
  },
  {
    "objectID": "project-description.html#meeting",
    "href": "project-description.html#meeting",
    "title": "MATH/COSC 3570 Project Description",
    "section": "Meeting",
    "text": "Meeting\n\nYou loses 3 points of your project grade if you don’t meet the requirement or don’t meet with Dr. Yu at least once.\nPlease choose a meeting time in the Excel form.\nYou must let Dr. Yu know in advance if you need to change your meeting time.\nYou can change your meeting time once.\nEvery team member needs to show up in the meeting.\nPrepare briefly talk about your project."
  },
  {
    "objectID": "project-description.html#presentation",
    "href": "project-description.html#presentation",
    "title": "MATH/COSC 3570 Project Description",
    "section": "Presentation",
    "text": "Presentation\n\nEvery student has to participate (in-person) in the final presentation in order to pass the course.\nEach group presentation should be between 14 and 15 minute long, followed by 1 to 2 minute Q&A. If your presentation is too short or too long, every one of you loses 3 points of your project grade.\nEvery group member has to present some part of the group work. The one who does not present receives no point.\n\n\n\nQuestions are encouraged during Q&A. Everyone is welcome to ask any questions about the projects. It helps everyone evaluate every group’s project and presentation performance. See Section 4 for grading policy.\nEach group is required to ask as least one question.\n\nThe \\(k\\)-th group should ask at least one question to the \\((k-1)\\)-th group in Q&A, \\(k = 2, \\dots, 7\\).\nThe 1st group will ask the last (7th) group questions about their project.\n\nIf you, as a group, don’t ask a question when you should, every one of you loses 3 points of your project grade."
  },
  {
    "objectID": "project-description.html#materials",
    "href": "project-description.html#materials",
    "title": "MATH/COSC 3570 Project Description",
    "section": "Materials",
    "text": "Materials\n\nEach one of you loses 3 points of your project grade if you don’t meet the requirement or miss the deadline.\nYou need to share your entire work, including slides, code, and data if applicable.\nYour code should be able to reproduce all the numerical results, outputs, tables, and figures shown in the slides, including the source of the raw data (where you find and load the data) if the project is about data analysis."
  },
  {
    "objectID": "project-description.html#data-analysis",
    "href": "project-description.html#data-analysis",
    "title": "MATH/COSC 3570 Project Description",
    "section": "Data Analysis",
    "text": "Data Analysis\nFor your data analysis project,\n\nYou need to show that you are good at asking meaningful questions and answering them with results of data analysis.\nYour presentation should include data visualization. Your graphics should be informative that help you\n\nexplore relationships between variables in your data\ndecide which statistical model is used, so that your research questions can be properly answered.\n\n\n\n\nYou should discuss how and why statistical methods/machine learning algorithms are chosen for analyzing your data set.\n\nThe methods we learn in class may not be appropriate for your data and answering your research questions. If this happens, critique your own methods and provide suggestions for improving your analysis. Any issues of your data, and appropriateness of the statistical analysis should be discussed.\n\n\n\n\n\nYou can choose a data set that is publicly available or you may collect your own data using a survey or by conducting an experiment. The dataset you choose cannot be any datasets used in class, including homework assignments and lab exercises.\nBelow are a list of data repositories you can start with, but you are encouraged to explore more and find your favorite one, for example COVID-19 data if you are interested.\n\nTidyTuesday\nKaggle\nAwesome Public Datasets\nHarvard Dataverse\nUCI Machine Learning Repository\nFiveThirtyEight"
  },
  {
    "objectID": "project-description.html#rpython-packages",
    "href": "project-description.html#rpython-packages",
    "title": "MATH/COSC 3570 Project Description",
    "section": "R/Python Packages",
    "text": "R/Python Packages\nFor your R/Python package project,\n\nyou need to\n\nshow how and why the package greatly helps us do data science.\nexplain how to use the functions in the packages by providing data science examples with some real data set. Please don’t use the toy examples in the package documentation.\n\nIf the functions of the package return any results or outputs, please explain them, teaching your audience how to appropriately read the outputs.\nIf the functions return graphics, explain why the visualizations are informative and useful for understanding data and analysis results.\nYou can choose a package that helps us do what we cannot do with the packages and tools learned in class.\n\nFor example, we only learn and packages to help us import data files into R, and we don’t know how to extract data from a website. The  package helps us scrape data from web pages.\n\nIf you choose a package that provides the same functionality as the packages we learned, please show the packages you choose are better.\n\nFor example, its code is shorter, it is run faster, its output is more clear, its plot is prettier, etc. For example, (https://rdatatable.gitlab.io/data.table/) package provides a high-performance version of base R’s with syntax and feature enhancements for ease of use, convenience and programming speed.\n\nBelow are a list of popular R packages that you can start with.\n\nQuick list of useful R packages\nFavorite R Packages by Roel Verbelen\nGreat R packages for data import, wrangling, and visualization\nggplot2 extensions\n\n\nBelow are a list of popular Python packages that you can start with.\n\n20 Must-Have Python Libraries for Data Science for 2024\nTop 38 Python Libraries for Data Science, Data Visualization & Machine Learning"
  },
  {
    "objectID": "project-description.html#group-performance-evaluation",
    "href": "project-description.html#group-performance-evaluation",
    "title": "MATH/COSC 3570 Project Description",
    "section": "Group Performance Evaluation",
    "text": "Group Performance Evaluation\n\nYou will need to evaluate all group projects except the one you work on.\nYou evaluate group performance based on the rubric attached. Four evaluation criteria are considered:\n\nProject Content and Organization (8 pts)\nSlides Quality (4 pts)\nOral Presentation Skill and Delivery (4 pts)\nInteractions and Q&A (4 pts)\n\nThe total points of a project presentation is 20 points.\nEvaluation sheets will be provided on the presentation day.\nHow do you get the full points for each category? Check the requirements below. Note that for Content and Organization, data analysis and package projects have different requirements.\nContent and Organization (Data Analysis)\n\nBeautiful visualization helps find out relationship of variables and specification of models\nAll questions are answered accurately by the models\nDiscuss how and why the models are chosen\nApply sophisticated models and detailed analysis\nAll ideas are presented in logical order\n\nContent and Organization (Packages)\n\nShow how and why the package greatly helps us do data science\nExplain how to use the functions in the package by providing concrete real data science applications and examples with data sets\nTeach audience with understandable examples of how to appropriately read the outputs and/or why the visualizations are informative and useful for understanding data and analysis results\nShow the package is better in some sense, its code is shorter, it is run faster, its output is more clear, its plot is prettier, etc\nAll ideas are presented in logical order\n\nSlides Quality\n\nSlides show code and output beautifully\nSlides clearly aid the speaker in telling a coherent story\nAll tables and graphics are informative and related to the topic and make it easier to understand\nAttractive design, layout, and neatness.\n\nOral Presentation Skill\n\nGood volume and energy\nProper pace and diction\nAvoidance of distracting gestures\n\nInteractions and Q&A\n\nGood eye contact with audience\nExcellent listening skills\nAnswers audience questions with authority and accuracy\n\nAfter you evaluate 6 group project presentations, you rank them from 1st to 6th based on their earned points.\nNo two groups receive the same ranking. If you give two or more groups some points, you still need to give them a different ranking, deciding which teams deserve a higher ranking according to your preference."
  },
  {
    "objectID": "project-description.html#individual-performance-evaluation",
    "href": "project-description.html#individual-performance-evaluation",
    "title": "MATH/COSC 3570 Project Description",
    "section": "Individual Performance Evaluation",
    "text": "Individual Performance Evaluation\n\n\nYou choose one single person who you think contributes the most to your group project.\nYou cannot vote for yourself, and you can only vote for one of your teammates.\nIf you don’t vote, you can’t be the best contributor even if you obtain the most votes. The person with the second highest votes wins the best contribution reward.\nIf there is no one single person who gets the most votes, every team member remains the same grade. For example, if your group finish in third place, and there is no best contributor, all members receive 91 (See Table 1)."
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Click here to download the syllabus."
  },
  {
    "objectID": "course-syllabus.html#time-and-location",
    "href": "course-syllabus.html#time-and-location",
    "title": "Syllabus",
    "section": "Time and location",
    "text": "Time and location\n\n\n\n\nDay\nTime\nLocation\n\n\n\n\nLectures\nTu & Th\n2:00 - 3:15 PM\nOlin Engineering 198\n\n\nLab\nNone\nNone\nNone"
  },
  {
    "objectID": "course-syllabus.html#office-hours",
    "href": "course-syllabus.html#office-hours",
    "title": "Syllabus",
    "section": "Office Hours",
    "text": "Office Hours\n\nMy in-person office hours are TuTh 4:50 - 5:50 PM, and Wed 12 - 1 PM in Cudahy Hall room 353.\nYou are welcome to schedule an online meeting via Microsoft Teams if you need/prefer."
  },
  {
    "objectID": "course-syllabus.html#learning-objectives",
    "href": "course-syllabus.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of the semester, you will be able to…\n\nRepresent and manipulate data in effective ways\nManipulate data using packages/tools and by ad hoc data handling\nUse mathematical, computational and statistical tools to detect patterns and model performance\nUse computational principles and tools to tackle issues addressable by data science\nUse a solid foundation in data science to independently learn new methodologies and technologies in the field of data science"
  },
  {
    "objectID": "course-syllabus.html#prerequisites",
    "href": "course-syllabus.html#prerequisites",
    "title": "Syllabus",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nCOSC 1010 (Intro to Programming) and MATH 4720 (Intro to Statistics), or MATH 2780 (Intro to Regression and Classification).\nProgramming experience is helpful because the course involves doing regression analysis using  programming language.\nThe course will also assume facility with using the internet and a personal computer/laptop. The course involves coding in R and Python using Posit Cloud, a cloud integrated development environment (IDE).\nTalk to me if you are not sure whether or not this is the right course for you."
  },
  {
    "objectID": "course-syllabus.html#e-mail-policy",
    "href": "course-syllabus.html#e-mail-policy",
    "title": "Syllabus",
    "section": "E-mail Policy",
    "text": "E-mail Policy\n\nI will attempt to reply your email quickly, at least within 24 hours.\nExpect a reply on Monday if you send a question during weekends. If you do not receive a response from me within two days, re-send your question/comment in case there was a “mix-up” with email communication (Hope this won’t happen!).\nPlease start your subject line with [math3570] or [cosc3570] followed by a clear description of your question. See an example below.\n\n\n\n\nEmail Subject Line Example\n\n\n\nEmail etiquette is important. Please read this article to learn more about email etiquette.\nI am more than happy to answer your questions about this course or data science/statistics in general. However, with tons of email messages everyday, I may choose NOT to respond to students’ e-mail if\n\nThe student could answer his/her own inquiry by reading the syllabus or information on the course website or D2L.\nThe student is asking for an extra credit opportunity. The answer is “no”.\nThe student is requesting an extension on homework. The answer is “no”.\nThe student is asking for a grade to be raised for no legitimate reason. The answer is “no”.\nThe student is sending an email with no etiquette."
  },
  {
    "objectID": "course-syllabus.html#textbooks",
    "href": "course-syllabus.html#textbooks",
    "title": "Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\n\nNo textbook is required for this course. Course materials are mainly Dr. Yu’s slides. Below are some good references.\n\n(r4ds) R for Data Science by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund.\n(tmwr) Tidy Modeling with R by Max Kuhn and Julia Silge.\n(py4ds) Python for Data Analysis by Wes McKinney.\n(IS) Introduction to Statistics by Cheng-Han Yu. (Good resource for brushing up your basic probability, statistics and simple linear regression knowledge.)"
  },
  {
    "objectID": "course-syllabus.html#grading-policy",
    "href": "course-syllabus.html#grading-policy",
    "title": "Syllabus",
    "section": "Grading Policy",
    "text": "Grading Policy\n\n40% In-class lab exercises and participation.\n30% Homework\n30% Final project competition\nExtra credit opportunities\nEvery student has to participate (in-person) in the final presentation in order to pass the course.\nYou will NOT be allowed any extra credit projects/homework/exam to compensate for a poor grade. Everyone is given the same opportunity to do well in this class. I may use class participation to make grade adjustments at the end of the semester.\nThe final grade is based on the grade-percentage conversion Table 1 on the next page. \\([x, y)\\) means greater than or equal to \\(x\\) and less than \\(y\\). For example, 94.1 is in \\([94, 100]\\) and the grade is A and 93.8 is in \\([90, 94)\\) and the grade is A-.\n\n\n\n\nGrade-Percentage Conversion\n\n\nGrade\nPercentage\n\n\n\n\nA\n[94, 100]\n\n\nA-\n[90, 94)\n\n\nB+\n[87, 90)\n\n\nB\n[83, 87)\n\n\nB-\n[80, 83)\n\n\nC+\n[77, 80)\n\n\nC\n[73, 77)\n\n\nC-\n[70, 73)\n\n\nD+\n[65, 70)\n\n\nD\n[60, 65)\n\n\nF\n[0, 60)\n\n\n\n\n\n\nLab exercises\n\nThere are several in-class lab exercises, which are graded as complete/incomplete and used as evidence of attendance and class participation.\nYou are allowed to have two incomplete lab exercises without penalty. Beyond that, 2% grade percentage will be taken off for each missing/incomplete exercise.\nNo make-up lab exercises for any reason.\n\n\n\nHomework\n\nThe homework assignments are individual. You should submit your own work.\nYou may not directly share or discuss answers/code with anyone other than the instructor. But you are welcome to discuss the problems in general and ask for advice.\nHomework will be assigned through GitHub. You need to clone/pull the homework repo into Posit Cloud and work on the Quarto file in the repo. A step-by-step guide will be discussed in class before homework is assigned.\nYou will have at least one week to complete your assignment.\nNo make-up homework for any reason unless you got COVID or excused absence.\nIf you miss a homework assignment due to COVID-19 symptoms, exposure, diagnosis, quarantine, and/or isolation, or you have an excused absence as defined in Attendance in Academic Regulations, the homework percentage will be added to your final project. If you miss more than one assignment, only one assignment percentage can be added to the final project percentage. You get 0% for the other assignment.\n\n\n\nFinal project competition\n\nYou will be team up to do the final project. Your project can be in either of the following categories:\n\nData analysis using statistical models or machine learning algorithms\nIntroduce a R or Python package not learned in class, including live demo\nIntroduce a data science tool (visualization, computing, etc) not learned in class, including live demo\nWeb development: Shiny website or dashboard, including live demo\n\nDetails about the project will be provided as the course progresses. You must complete the final project and be in class to present it in order to pass this course.\n\n\n\n\n\nThe final project presentation is on Monday, 5/6 10:30 AM - 12:30 PM."
  },
  {
    "objectID": "course-syllabus.html#sharingreusing-code-policy",
    "href": "course-syllabus.html#sharingreusing-code-policy",
    "title": "Syllabus",
    "section": "Sharing/Reusing Code Policy",
    "text": "Sharing/Reusing Code Policy\n\nUnless explicitly stated otherwise, you may make use of any online resources, but you must explicitly cite where you obtained any code you directly use or use as inspiration in your solutions.\nAny recycled code that is discovered and is not explicitly cited will be treated as plagiarism, regardless of source."
  },
  {
    "objectID": "course-syllabus.html#university-and-college-policies",
    "href": "course-syllabus.html#university-and-college-policies",
    "title": "Syllabus",
    "section": "University and college policies",
    "text": "University and college policies\nAs a student in this course, you have agreed to comply with Marquette undergraduate policies and regulations."
  },
  {
    "objectID": "course-syllabus.html#accommodation",
    "href": "course-syllabus.html#accommodation",
    "title": "Syllabus",
    "section": "Accommodation",
    "text": "Accommodation\nIf you need to request accommodations, or modify existing accommodations that address disability-related needs, please contact Disability Service."
  },
  {
    "objectID": "course-syllabus.html#important-dates",
    "href": "course-syllabus.html#important-dates",
    "title": "Syllabus",
    "section": "Important dates",
    "text": "Important dates\n\nJan 24: Last day to add/swap/drop\nMar 10-16: Spring break\nMar 12: Midterm grade submission\nMar 28 - Apr 1: Easter break\nApr 12: Withdrawal deadline\nMay 4: Last day of class\nMay 6: Final project presentation\nMay 14: Final grade submission\n\nClick here for the full Marquette academic calendar."
  }
]